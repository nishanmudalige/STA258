<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 17 ANOVA (Analysis of Variance) | Demo Book</title>
  <meta name="description" content="Chapter 17 ANOVA (Analysis of Variance) | Demo Book" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 17 ANOVA (Analysis of Variance) | Demo Book" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 ANOVA (Analysis of Variance) | Demo Book" />
  
  
  

<meta name="author" content="Nishan Mudalige" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inference-for-simple-linear-regression.html"/>
<link rel="next" href="analysis-of-categorical-data.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>STA258: Statistics with Applied Probability</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#nishan-mudalige-masoud-ataei-nurlana-alili-bryan-xu"><i class="fa fa-check"></i>Nishan Mudalige, Masoud Ataei, Nurlana Alili, Bryan Xu</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html"><i class="fa fa-check"></i><b>1</b> Descriptive Statistics and an Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#descriptive-statistics"><i class="fa fa-check"></i><b>1.2</b> Descriptive Statistics</a></li>
<li class="chapter" data-level="1.3" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#graphical-techniques"><i class="fa fa-check"></i><b>1.3</b> Graphical Techniques</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#histograms"><i class="fa fa-check"></i><b>1.3.1</b> Histograms</a></li>
<li class="chapter" data-level="1.3.2" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#box-plots"><i class="fa fa-check"></i><b>1.3.2</b> Box-Plots</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#introduction-to-r"><i class="fa fa-check"></i><b>1.4</b> Introduction to R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html"><i class="fa fa-check"></i><b>2</b> Sampling Distributions Related to a Normal Population</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#normal-distribution"><i class="fa fa-check"></i><b>2.1</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.2" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#gamma-and-chi-square-distribution"><i class="fa fa-check"></i><b>2.2</b> Gamma and Chi-square Distribution</a></li>
<li class="chapter" data-level="2.3" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#students-t-distribution-and-f-distribution"><i class="fa fa-check"></i><b>2.3</b> Student’s t-Distribution and F-Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-central-limit-theorem.html"><a href="the-central-limit-theorem.html"><i class="fa fa-check"></i><b>3</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="4" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html"><i class="fa fa-check"></i><b>4</b> Normal Approximation to the Binomial Distribution</a>
<ul>
<li class="chapter" data-level="4.1" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#bernoulli-distribution"><i class="fa fa-check"></i><b>4.2</b> Bernoulli Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#sampling-distribution-of-the-sum-and-mgf-derivation"><i class="fa fa-check"></i><b>4.3</b> Sampling Distribution of the Sum and MGF Derivation</a></li>
<li class="chapter" data-level="4.4" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#binomial-distribution"><i class="fa fa-check"></i><b>4.4</b> Binomial Distribution</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#visualizing-the-pmf-of-binomial-distributions"><i class="fa fa-check"></i><b>4.4.1</b> Visualizing the PMF of Binomial Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#sampling-distribution-of-a-sample-proportion-and-the-normal-approximation"><i class="fa fa-check"></i><b>4.5</b> Sampling Distribution of a Sample Proportion and the Normal Approximation</a>
<ul>
<li class="chapter" data-level="" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#conditions-for-using-the-normal-approximation"><i class="fa fa-check"></i>Conditions for Using the Normal Approximation</a></li>
<li class="chapter" data-level="" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#bernoulli-distribution-binomial-with-n-1"><i class="fa fa-check"></i>Bernoulli Distribution (Binomial with <span class="math inline">\(n = 1\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#normal-approximation-to-binomial"><i class="fa fa-check"></i><b>4.6</b> Normal Approximation to Binomial</a></li>
<li class="chapter" data-level="4.7" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#continuity-correction"><i class="fa fa-check"></i><b>4.7</b> Continuity Correction</a>
<ul>
<li class="chapter" data-level="" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#continuity-correction-table"><i class="fa fa-check"></i>Continuity Correction Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html"><i class="fa fa-check"></i><b>5</b> Law of Large Numbers</a>
<ul>
<li class="chapter" data-level="5.1" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#convergence-in-probability"><i class="fa fa-check"></i><b>5.1</b> Convergence in Probability</a></li>
<li class="chapter" data-level="5.2" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#weak-law-of-large-numbers-wlln"><i class="fa fa-check"></i><b>5.2</b> Weak Law of Large Numbers (WLLN)</a>
<ul>
<li class="chapter" data-level="" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#proof-of-the-weak-law-of-large-numbers-wlln"><i class="fa fa-check"></i>Proof of the Weak Law of Large Numbers (WLLN)</a></li>
<li class="chapter" data-level="" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#empirical-probability-insight"><i class="fa fa-check"></i>Empirical Probability Insight</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><i class="fa fa-check"></i><b>6</b> One Sample Confidence Intervals on a Mean When the Population Variance is Known</a>
<ul>
<li class="chapter" data-level="6.1" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#interpretation"><i class="fa fa-check"></i><b>6.2</b> Interpretation</a></li>
<li class="chapter" data-level="6.3" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#confidence-interval-for-mu-known-variance"><i class="fa fa-check"></i><b>6.3</b> Confidence Interval for <span class="math inline">\(\mu\)</span> (Known Variance)</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#confidence-interval-for-the-mean-of-a-normal-population"><i class="fa fa-check"></i>Confidence Interval for the Mean of a Normal Population</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#large-sample-ci-for-mu-normal-data"><i class="fa fa-check"></i>Large Sample CI for <span class="math inline">\(\mu\)</span> (Normal data)</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#one-sample-ci-on-the-population-mean-mu"><i class="fa fa-check"></i>One Sample CI on the Population Mean <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#table-of-common-z-values"><i class="fa fa-check"></i>Table of Common <span class="math inline">\(z\)</span>-values</a></li>
<li class="chapter" data-level="6.4" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#appendix"><i class="fa fa-check"></i><b>6.4</b> APPENDIX</a>
<ul>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#pivotal-quantities"><i class="fa fa-check"></i>Pivotal quantities</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><i class="fa fa-check"></i><b>7</b> One-Sample Confidence Intervals on a Mean When the Population Variance is Unknown</a>
<ul>
<li class="chapter" data-level="7.1" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#cis-for-mu"><i class="fa fa-check"></i><b>7.1</b> CIs for <span class="math inline">\(\mu\)</span></a>
<ul>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#independence-assumption"><i class="fa fa-check"></i>Independence Assumption</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#randomization-condition"><i class="fa fa-check"></i>Randomization Condition</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#standard-error"><i class="fa fa-check"></i>Standard Error</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#a-few-final-comments"><i class="fa fa-check"></i>A few final comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="one-sample-confidence-intervals-on-a-proportion.html"><a href="one-sample-confidence-intervals-on-a-proportion.html"><i class="fa fa-check"></i><b>8</b> One Sample Confidence Intervals On a Proportion</a></li>
<li class="chapter" data-level="9" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html"><i class="fa fa-check"></i><b>9</b> Sample Size Selection using Confidence Intervals</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#empirical-rule"><i class="fa fa-check"></i><b>9.0.1</b> Empirical Rule</a></li>
<li class="chapter" data-level="9.1" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#secSampleSizeCIMean"><i class="fa fa-check"></i><b>9.1</b> Calculating Sample Size for a Confidence Interval on a Mean</a>
<ul>
<li class="chapter" data-level="" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#when-sigma-is-known"><i class="fa fa-check"></i>When <span class="math inline">\(\sigma\)</span> is Known</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#calculating-sample-size-for-a-confidence-interval-on-a-proportion"><i class="fa fa-check"></i><b>9.2</b> Calculating Sample Size for a Confidence Interval on a Proportion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html"><i class="fa fa-check"></i><b>10</b> Two Sample Confidence Interval</a>
<ul>
<li class="chapter" data-level="10.1" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-a-difference-of-mean"><i class="fa fa-check"></i><b>10.1</b> Two Sample Confidence Interval on a Difference of Mean</a></li>
<li class="chapter" data-level="10.2" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-paired-data"><i class="fa fa-check"></i><b>10.2</b> Two Sample Confidence Interval on Paired Data</a></li>
<li class="chapter" data-level="10.3" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-proportions"><i class="fa fa-check"></i><b>10.3</b> Two Sample Confidence Interval on Proportions</a></li>
<li class="chapter" data-level="10.4" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-variances"><i class="fa fa-check"></i><b>10.4</b> Two Sample Confidence Interval on Variances</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html"><i class="fa fa-check"></i><b>11</b> Introduction to Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="11.1" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#test-of-hypothesis-for-one-mean"><i class="fa fa-check"></i><b>11.1</b> Test of Hypothesis for One Mean</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-1-decide-on-a-level-of-significance-alpha"><i class="fa fa-check"></i>Step 1: Decide on a Level of Significance (<span class="math inline">\(\alpha\)</span>)</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-2-state-the-null-hypothesis-h_0-and-the-alternative-hypothesis-h_a"><i class="fa fa-check"></i>Step 2: State the Null Hypothesis (<span class="math inline">\(H_0\)</span>) and the Alternative Hypothesis (<span class="math inline">\(H_a\)</span>)</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-3-calculate-an-appropriate-test-statistic"><i class="fa fa-check"></i>Step 3: Calculate an appropriate test statistic</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-4-calculate-the-p-value"><i class="fa fa-check"></i>Step 4: Calculate the p-value</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-5-compare-p-value-to-level-of-significance-alpha-and-make-a-conclusion"><i class="fa fa-check"></i>Step 5: Compare <em>p</em>-value to level of significance <span class="math inline">\(\alpha\)</span> and make a conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#test-of-hypothesis-for-one-proportion"><i class="fa fa-check"></i><b>11.2</b> Test of Hypothesis for One Proportion</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#introduction-to-hypothesis-testing-significance-test"><i class="fa fa-check"></i>Introduction to Hypothesis Testing (Significance Test)</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#steps-in-conducting-hypothesis-testing"><i class="fa fa-check"></i>Steps in conducting Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#some-additional-examples"><i class="fa fa-check"></i>Some Additional Examples</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#test-of-hypothesis-for-one-variance"><i class="fa fa-check"></i><b>11.3</b> Test of Hypothesis for One Variance</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="one-sample-hypothesis-test-on-a-proportion-and-variance.html"><a href="one-sample-hypothesis-test-on-a-proportion-and-variance.html"><i class="fa fa-check"></i><b>12</b> One Sample Hypothesis Test on a Proportion and Variance</a>
<ul>
<li class="chapter" data-level="12.1" data-path="one-sample-hypothesis-test-on-a-proportion-and-variance.html"><a href="one-sample-hypothesis-test-on-a-proportion-and-variance.html#one-sample-hypothesis-test-on-a-proportion"><i class="fa fa-check"></i><b>12.1</b> One Sample Hypothesis Test on a Proportion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="statistical-power.html"><a href="statistical-power.html"><i class="fa fa-check"></i><b>13</b> Statistical Power</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-power.html"><a href="statistical-power.html#type-i-and-ii-errors"><i class="fa fa-check"></i>Type I and II Errors</a></li>
<li class="chapter" data-level="13.1" data-path="statistical-power.html"><a href="statistical-power.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>13.1</b> Type I and Type II Errors</a></li>
<li class="chapter" data-level="13.2" data-path="statistical-power.html"><a href="statistical-power.html#using-power-to-determine-sample-size"><i class="fa fa-check"></i><b>13.2</b> Using Power to Determine Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html"><i class="fa fa-check"></i><b>14</b> Two Sample Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="14.1" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-means-with-independent-samples"><i class="fa fa-check"></i><b>14.1</b> Comparing Means with Independent Samples</a>
<ul>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#setting-up-hypotheses"><i class="fa fa-check"></i>Setting Up Hypotheses</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#structure-of-a-test-statistic"><i class="fa fa-check"></i>Structure of a Test Statistic</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#hypothesis-test-on-a-difference-of-means-mu_1---mu_2"><i class="fa fa-check"></i>Hypothesis Test on a Difference of Means (<span class="math inline">\(\mu_1 - \mu_2\)</span>)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-means-of-independent-samples-normal-population-assumptions"><i class="fa fa-check"></i>Comparing Means of Independent Samples (Normal Population Assumptions)</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-two-populations-means-independent-sampling-equal-variances-assumed"><i class="fa fa-check"></i><b>14.1.1</b> Comparing Two Populations Means: Independent Sampling (Equal Variances Assumed)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#small-sample-confidence-interval-for-mu_1---mu_2-with-equal-variances"><i class="fa fa-check"></i>Small-Sample Confidence Interval for <span class="math inline">\(\mu_1 - \mu_2\)</span> (with equal variances)</a></li>
<li class="chapter" data-level="14.1.2" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-two-populations-means-independent-sampling-unequal-variances-assumed"><i class="fa fa-check"></i><b>14.1.2</b> Comparing Two Populations Means: Independent Sampling (Unequal Variances Assumed)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#small-sample-confidence-interval-for-mu_1---mu_2-unequal-variances"><i class="fa fa-check"></i>Small-Sample Confidence Interval for <span class="math inline">\(\mu_1 - \mu_2\)</span> (Unequal Variances)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#degrees-of-freedom"><i class="fa fa-check"></i>Degrees of Freedom</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-t-test-unequal-variances"><i class="fa fa-check"></i>Two-Sample t-Test (Unequal Variances)</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#the-fold-rule"><i class="fa fa-check"></i><b>14.2</b> The Fold Rule</a></li>
<li class="chapter" data-level="14.3" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-hypothesis-test-on-paired-data"><i class="fa fa-check"></i><b>14.3</b> Two Sample Hypothesis Test on Paired Data</a></li>
<li class="chapter" data-level="14.4" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-hypothesis-test-on-proportions"><i class="fa fa-check"></i><b>14.4</b> Two Sample Hypothesis Test on Proportions</a></li>
<li class="chapter" data-level="14.5" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-hypothesis-test-on-variances"><i class="fa fa-check"></i><b>14.5</b> Two Sample Hypothesis Test on Variances</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="introduction-to-simple-linear-regression.html"><a href="introduction-to-simple-linear-regression.html"><i class="fa fa-check"></i><b>15</b> Introduction to Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="15.1" data-path="introduction-to-simple-linear-regression.html"><a href="introduction-to-simple-linear-regression.html#measures-of-linear-relationship"><i class="fa fa-check"></i><b>15.1</b> Measures of Linear Relationship</a></li>
<li class="chapter" data-level="15.2" data-path="introduction-to-simple-linear-regression.html"><a href="introduction-to-simple-linear-regression.html#least-squares-method"><i class="fa fa-check"></i><b>15.2</b> Least Squares Method</a></li>
<li class="chapter" data-level="15.3" data-path="introduction-to-simple-linear-regression.html"><a href="introduction-to-simple-linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>15.3</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="15.4" data-path="introduction-to-simple-linear-regression.html"><a href="introduction-to-simple-linear-regression.html#sst-sse-and-ssr"><i class="fa fa-check"></i><b>15.4</b> SST, SSE and SSR</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html"><i class="fa fa-check"></i><b>16</b> Inference for Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#inference-on-regression"><i class="fa fa-check"></i><b>16.1</b> Inference on Regression</a>
<ul>
<li class="chapter" data-level="" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#estimating-variance-in-linear-regression"><i class="fa fa-check"></i>Estimating Variance in Linear Regression</a></li>
<li class="chapter" data-level="" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#the-regression-model"><i class="fa fa-check"></i>The Regression Model</a></li>
<li class="chapter" data-level="" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#interpreting-confidence-intervals-for-beta_1"><i class="fa fa-check"></i>Interpreting Confidence Intervals for <span class="math inline">\(\beta_1\)</span></a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#anova-table-analysis-of-variance"><i class="fa fa-check"></i><b>16.2</b> ANOVA Table (ANalysis Of VAriance)</a></li>
<li class="chapter" data-level="16.3" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#residual-plots"><i class="fa fa-check"></i><b>16.3</b> Residual Plots</a>
<ul>
<li class="chapter" data-level="" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#what-to-look-for-in-a-good-residual-plot"><i class="fa fa-check"></i>What to Look for in a Good Residual Plot</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="anova-analysis-of-variance.html"><a href="anova-analysis-of-variance.html"><i class="fa fa-check"></i><b>17</b> ANOVA (Analysis of Variance)</a></li>
<li class="chapter" data-level="18" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html"><i class="fa fa-check"></i><b>18</b> Analysis of Categorical Data</a>
<ul>
<li class="chapter" data-level="18.1" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#multinomial-response-model"><i class="fa fa-check"></i><b>18.1</b> Multinomial Response Model</a></li>
<li class="chapter" data-level="18.2" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#properties-of-the-multinomial-model"><i class="fa fa-check"></i><b>18.2</b> Properties of the Multinomial Model</a></li>
<li class="chapter" data-level="18.3" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#example"><i class="fa fa-check"></i><b>18.3</b> Example</a></li>
<li class="chapter" data-level="18.4" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-1.-state-hypotheses."><i class="fa fa-check"></i><b>18.4</b> Step 1. State Hypotheses.</a></li>
<li class="chapter" data-level="18.5" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#table-of-expected-counts"><i class="fa fa-check"></i><b>18.5</b> Table of Expected Counts</a></li>
<li class="chapter" data-level="18.6" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-2.-computing-test-statistic"><i class="fa fa-check"></i><b>18.6</b> Step 2. Computing test statistic</a></li>
<li class="chapter" data-level="18.7" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-3.-finding-p-value"><i class="fa fa-check"></i><b>18.7</b> Step 3. Finding P-value</a></li>
<li class="chapter" data-level="18.8" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-4.-conclusion"><i class="fa fa-check"></i><b>18.8</b> Step 4. Conclusion</a></li>
<li class="chapter" data-level="18.9" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#chi-square-distributions"><i class="fa fa-check"></i><b>18.9</b> Chi-Square Distributions</a></li>
<li class="chapter" data-level="18.10" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#example-1"><i class="fa fa-check"></i><b>18.10</b> Example</a></li>
<li class="chapter" data-level="18.11" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-1.-state-hypotheses.-1"><i class="fa fa-check"></i><b>18.11</b> Step 1. State Hypotheses.</a></li>
<li class="chapter" data-level="18.12" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#table-of-expected-counts-1"><i class="fa fa-check"></i><b>18.12</b> Table of Expected Counts</a></li>
<li class="chapter" data-level="18.13" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-2.-computing-test-statistic-1"><i class="fa fa-check"></i><b>18.13</b> Step 2. Computing test statistic</a></li>
<li class="chapter" data-level="18.14" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-3.-finding-p-value-1"><i class="fa fa-check"></i><b>18.14</b> Step 3. Finding P-value</a></li>
<li class="chapter" data-level="18.15" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-4.-conclusion-1"><i class="fa fa-check"></i><b>18.15</b> Step 4. Conclusion</a></li>
<li class="chapter" data-level="18.16" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#exercise"><i class="fa fa-check"></i><b>18.16</b> Exercise</a></li>
<li class="chapter" data-level="18.17" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#table-of-expected-counts-2"><i class="fa fa-check"></i><b>18.17</b> Table of Expected Counts</a></li>
<li class="chapter" data-level="18.18" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-2.-computing-test-statistic-2"><i class="fa fa-check"></i><b>18.18</b> Step 2. Computing test statistic</a></li>
<li class="chapter" data-level="18.19" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-3.-finding-p-value-2"><i class="fa fa-check"></i><b>18.19</b> Step 3. Finding P-value</a></li>
<li class="chapter" data-level="18.20" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#step-4.-conclusion-2"><i class="fa fa-check"></i><b>18.20</b> Step 4. Conclusion</a></li>
<li class="chapter" data-level="18.21" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#the-chi-square-test-for-goodness-of-fit"><i class="fa fa-check"></i><b>18.21</b> The Chi-square Test for Goodness of fit</a></li>
<li class="chapter" data-level="18.22" data-path="analysis-of-categorical-data.html"><a href="analysis-of-categorical-data.html#sample-size-assumption"><i class="fa fa-check"></i><b>18.22</b> Sample Size Assumption</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Demo Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="anova-analysis-of-variance" class="section level1 hasAnchor" number="17">
<h1><span class="header-section-number">Chapter 17</span> ANOVA (Analysis of Variance)<a href="anova-analysis-of-variance.html#anova-analysis-of-variance" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>used to compare multiple nears<br />
Suppose we have <span class="math inline">\(k\)</span> groups <span class="math inline">\((k \geqslant 3)\)</span> and each group is given a
treatment.</p>
<p>The A NOVA model is<br />
<img src="2025_07_03_0e611eebd5aab7c4aee1g-02" alt="image" /><br />
equimient form of ANOLA is</p>
<p><span class="math display">\[Y_{i j}=\mu_{i}+\varepsilon_{i j}, \quad \varepsilon_{i j} \sim N\left(0, \sigma^{2}\right)\]</span></p>
<div class="center">
<table style="width:93%;">
<colgroup>
<col width="18%" />
<col width="16%" />
<col width="16%" />
<col width="22%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">group 1</th>
<th align="center">group 2</th>
<th align="center"><span class="math inline">\(\cdots\)</span></th>
<th align="center">group <span class="math inline">\(k\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"></td>
<td align="center"><span class="math inline">\(x_{11}\)</span></td>
<td align="center"><span class="math inline">\(x_{21}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(x_{k 1}\)</span></td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center"><span class="math inline">\(x_{12}\)</span></td>
<td align="center"><span class="math inline">\(x_{22}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(x_{k 2}\)</span></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="even">
<td rowspan="5" align="center"><table>
<tbody>
<tr class="odd">
<td align="center">sample</td>
</tr>
<tr class="even">
<td align="center">mean</td>
</tr>
<tr class="odd">
<td align="center">sample</td>
</tr>
<tr class="even">
<td align="center">size</td>
</tr>
<tr class="odd">
<td align="center">size</td>
</tr>
</tbody>
</table></td>
<td align="center"><span class="math inline">\(x_{1 n}\)</span></td>
<td align="center"><span class="math inline">\(x_{2 n}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(x_{k n k}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{1}\)</span></td>
<td align="center"><span class="math inline">\(x_{1}\)</span></td>
<td align="center"><span class="math inline">\(\bar{x}_{2}\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\bar{x}_{k}\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(n_{2}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(S_{k}^{2}\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(n_{k}\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
</div>
<!-- total sample size $n=n_{1}+n_{2}+\ldots+n_{k}$ over all sample mean -->
<!-- $\bar{x}=\frac{1}{n} \sum_{i=1}^{k} \sum_{j=1}^{n_{i}} x_{i j}=\frac{1}{n} \sum_{i=1}^{k} n_{i} \bar{x}_{i}$\ -->
<!-- Sources of variation\ -->
<!-- Variation within goods (SSW /SSE)\ -->
<!-- SSE $=\sum_{i=1}^{k}\left(n_{i}-1\right) \cdot s_{i}^{2}$\ -->
<!-- treatment affect\ -->
<!-- variation between groups (SSB/SSTr)\ -->
<!-- SSTro $=\sum_{i=1}^{k} n_{i}\left(\bar{x}_{i}-\bar{x}\right)^{2}$\ -->
<!-- Total variation -->
<!-- $$\text { SS Total }=\sum_{i=1}^{k} \sum_{j=1}^{n_{i}}\left(x_{i j}-\bar{x}\right)^{2}$$ -->
<!-- SStutal $=$ SSTrt + SSE -->
<!-- ::: center -->
<!-- +:------------+:--------------:+:---------------:+:------------------------------------:+:-----------------------------:+ -->
<!-- | ANOVA Table | MS $=\frac{\delta S}{d f}$                                              |                               | -->
<!-- +-------------+----------------+-----------------+--------------------------------------+-------------------------------+ -->
<!-- | Source      |   ------------ |   ----------    |   --------                           | $F$-Stat                      | -->
<!-- |             |      $d f$     |    $S S$ of     |     Mean                             |                               | -->
<!-- |             |    degrees of  |    Squares      |    Squere                            |                               | -->
<!-- |             |     Freedom    |   ----------    |   --------                           |                               | -->
<!-- |             |   ------------ |                 |                                      |                               | -->
<!-- +-------------+----------------+-----------------+--------------------------------------+-------------------------------+ -->
<!-- | Treatment   | $K-1$          | $S S T r t$     | $M S T r=\frac{\delta S T r t}{K-1}$ | $F^{*}=\frac{M S T r}{M S E}$ | -->
<!-- +-------------+----------------+-----------------+--------------------------------------+-------------------------------+ -->
<!-- | Error       | $\cap-K$       | $S S G$         | $M S E=\frac{\delta S E}{n-K}$       | $X$                           | -->
<!-- +-------------+----------------+-----------------+--------------------------------------+-------------------------------+ -->
<!-- | Total       | $\cap-1$       | $S S T o t a l$ | $X$                                  | $X$                           | -->
<!-- +-------------+----------------+-----------------+--------------------------------------+-------------------------------+ -->
<!-- ::: -->
<!-- Hypothesis Test\ -->
<!-- $H_{0}: \mu_{1}=\mu_{2} \ldots .=\mu_{k} \quad \xrightarrow{\text { all }}$ -->
<!-- means are equal)\ -->
<!-- tai. Al least one $\mu_{j}, j=1, \ldots, k$ fat least one mean -->
<!-- different) is different Lat least one treatment has a different effect -->
<!-- Test Stat -->
<!-- $$F^{*}=\frac{M S T r}{M S E}=\frac{S S T r+/ k-1}{S S E \mathscr{L} n-k} \sim F_{(k-1, n-k)}$$ -->
<!-- Reference distribution\ -->
<!-- $F$ distrib with amerater $=k=1$ and denominator $=n=k \quad d f$\ -->
<!-- p-value -->
<!-- Conclusions\ -->
<!-- ![image](2025_07_03_0e611eebd5aab7c4aee1g-05){width="\\textwidth"}\ -->
<!-- usual way -->
<!-- ## The Data Matrix  -->
<!-- The following table shows last year's sales data for a small business. -->
<!-- The sample is put into a matrix format in which each of the three -->
<!-- columns corresponds to one of the three countries in which the company -->
<!-- does business. The numbers in the cell represent the sales (in units of -->
<!-- $\$ 1000$ ) made in that country last year. These data will be used to -->
<!-- develop the theory underlying Analysis of Variance or, for short, ANOVA. -->
<!-- ## The Data Matrix {the-data-matrix-1 .unnumbered} -->
<!-- ::: center -->
<!--              Country A   Country B   Country C -->
<!--   --------- ----------- ----------- ----------- -->
<!--                  6          10          14 -->
<!--                 10           8          13 -->
<!--                  7          12          11 -->
<!--                  9          10          10 -->
<!--    Average       8          10          12 -->
<!-- ::: -->
<!-- Altogether, there were 12 sales last year that totaled $\$ 120$ - so the -->
<!-- average sale was $\$ 10$.\ -->
<!-- The column (country) averages are:\ -->
<!-- Column 1 (Country A) \$8.\ -->
<!-- Column 2 (Country B) \$10.\ -->
<!-- Column 3 (Country C) \$12. -->
<!-- Now we will begin our study of how to make a statistically valid -->
<!-- prediction of the next sales figure. In that regard, there are two -->
<!-- possible situations that can occur. -->
<!-- 1.  The country of the next sale (observation) is not known. -->
<!-- 2.  The country of the next sale is known. -->
<!-- ## Situation 1. {situation-1. .unnumbered} -->
<!-- Without any additional information, the best prediction is the sample -->
<!-- mean $\$ 10$. This prediction is best in the least squares sense - that -->
<!-- is, if $\$ 10$ had been used to predict each of the 12 observations in -->
<!-- the sample, then the total of the squared errors $S S_{\text {total }}$ -->
<!-- would be as small as possible. In our data set, $S S_{\text {total }}$ -->
<!-- equals 60 . That figure can be verified by calculating -->
<!-- $\sum\left(x_{i}-10\right)^{2}$ for each observation $x_{i}$ of the -->
<!-- sample. -->
<!-- Situation 2. One-factor ANOVA Model.\ -->
<!-- If the country of the next sale is known, then two different predictions -->
<!-- are possible for the next sales figure: -->
<!-- - The sample mean $\$ 10$. -->
<!-- - The mean of the sales of the country in which the next sale will -->
<!--   occur. (In this case, $\$ 8$ if the next sale will occur in Country A, -->
<!--   $\$ 10$ in Country B, or $\$ 12$ in Country C.) This prediction -->
<!--   ignores the information present in the sales figures from the other -->
<!--   two countries. -->
<!-- ## The Null Hypothesis for One-Factor ANOVA {the-null-hypothesis-for-one-factor-anova .unnumbered} -->
<!-- We have discussed the prediction possibilities for one-factor ANOVA -->
<!-- models. Now, we will learn how to test the statistical significance of a -->
<!-- one-factor ANOVA model.\ -->
<!-- Let's suppose that we want to predict the next sales figure, and that we -->
<!-- know the country in which this sale will occur. Without any statistical -->
<!-- testing, we can always by default use the sample mean $\$ 10$ to predict -->
<!-- the next sale. The default prediction, the sample mean, doesn't use any -->
<!-- information about the country (column) in which the sale will occur. -->
<!-- ## The Null Hypothesis for One-Factor ANOVA {the-null-hypothesis-for-one-factor-anova-1 .unnumbered} -->
<!-- However, if instead we use the mean of the observations in only one -->
<!-- column (the column that corresponds to the particular country in which -->
<!-- we know the next sale will occur), then we have to test the null -->
<!-- hypothesis -->
<!-- $$H_{0}: \mu_{C O L 1}, \mu_{C O L 2}, \mu_{C O L 3}, \text { are equal }$$ -->
<!-- and reject it in favor of the alternative hypothesis\ -->
<!-- $H_{a}: \mu_{C O L 1}, \mu_{C O L 2}, \mu_{C O L 3}$, are NOT all equal -->
<!-- ## The Null Hypothesis for One-Factor ANOVA {the-null-hypothesis-for-one-factor-anova-2 .unnumbered} -->
<!-- If the null hypothesis is rejected, then we can be statistically -->
<!-- confident that the column means are not all equal, and therefore that -->
<!-- the individual column means (i. e., $\$ 8, \$ 10, \$ 12$ ) can be used -->
<!-- to predict the amount of the next sale. If the next sale was going to -->
<!-- occur in Country A , then the prediction would be $\$ 8$. If the next -->
<!-- sale was going to occur in Country B, then the prediction would be -->
<!-- $\$ 10$. If the next sale was going to occur in Country C , then the -->
<!-- prediction would be $\$ 12$. -->
<!-- ## The One-Factor ANOVA F Test {the-one-factor-anova-f-test .unnumbered} -->
<!-- To test the null hypothesis stated above, we have to calculate an -->
<!-- F-statistic. If $F_{*}>F_{(c-1, n-c), \alpha}$, then reject $H_{0}$, and -->
<!-- use the sample column means to predict future observations. Otherwise, -->
<!-- do not reject $H_{0}$ and use the overall sample mean to predict future -->
<!-- observations. -->
<!-- ## ANOVA Table {anova-table .unnumbered} -->
<!-- To see how this $F_{*}$ is calculated, see the ANOVA Table below. -->
<!-- ::: center -->
<!-- +---------------+-------------------------+---------------------+-----------------------------+-----------------------------------------+ -->
<!-- |   ----------- |   -----------------     |   ----------------- |   -------------             | F Ratio                                 | -->
<!-- |    Source of  |      Degrees of         |        Sum of       |    Mean Sum of              |                                         | -->
<!-- |    Variation  |        Freedom          |        Square       |      Squares                |                                         | -->
<!-- |   ----------- |    $(\mathrm{df})$      |    $(\mathrm{SS})$  |    $($ MSS $)$              |                                         | -->
<!-- |               |   -----------------     |   ----------------- |   -------------             |                                         | -->
<!-- +:=============:+:=======================:+:===================:+:===========================:+:=======================================:+ -->
<!-- | Treatments    | $\mathrm{c}-1$          | SST                 | $\frac{\text { SST }}{c-1}$ | $F=\frac{\text { MST }}{\text { MSE }}$ | -->
<!-- +---------------+-------------------------+---------------------+-----------------------------+-----------------------------------------+ -->
<!-- | Error         | $\mathrm{n}-\mathrm{c}$ | SSE                 | $\frac{\text { SSE }}{n-c}$ |                                         | -->
<!-- +---------------+-------------------------+---------------------+-----------------------------+-----------------------------------------+ -->
<!-- | Total         | $\mathrm{n}-1$          | SSTOTAL             |                             |                                         | -->
<!-- +---------------+-------------------------+---------------------+-----------------------------+-----------------------------------------+ -->
<!-- ::: -->
<!-- ## Calculation of SStotal {calculation-of-sstotal .unnumbered} -->
<!-- If no model is used, then the predictions for each of the 12 -->
<!-- observations will be 10 . If these predictions are used, the squared -->
<!-- error of these 12 predictions is given in the table below. -->
<!-- ::: center -->
<!--    Country A   Country B   Country C -->
<!--   ----------- ----------- ----------- -->
<!--       16           0          16 -->
<!--        0           4           9 -->
<!--        9           4           1 -->
<!--        1           0           0 -->
<!-- ::: -->
<!-- Prediction Errors Squared when NO Factor is used $($ Total $)=60$. -->
<!-- ## Calculation of SSE {calculation-of-sse .unnumbered} -->
<!-- If the column model is used, then the 12 observations would have the -->
<!-- following 12 predictions, where $\$ 8$ is the average for the first -->
<!-- column, $\$ 10$ is the average for the second column, and $\$ 12$ is the -->
<!-- average for the third column. -->
<!-- ::: center -->
<!--    Country A   Country B   Country C -->
<!--   ----------- ----------- ----------- -->
<!--        8          10          12 -->
<!--        8          10          12 -->
<!--        8          10          12 -->
<!--        8          10          12 -->
<!-- ::: -->
<!-- ## Calculation of SSE {calculation-of-sse-1 .unnumbered} -->
<!-- Using the above 12 predictions, the errors squared are shown in the -->
<!-- table below. -->
<!-- ::: center -->
<!--    Country A   Country B   Country C -->
<!--   ----------- ----------- ----------- -->
<!--        4           0           4 -->
<!--        4           4           1 -->
<!--        1           4           1 -->
<!--        1           0           4 -->
<!-- ::: -->
<!-- Errors Squared when the Column Factor is used $($ Total $)=28$. -->
<!-- ## Calculation of SST {calculation-of-sst .unnumbered} -->
<!-- The units explained by the column model are calculated by finding the -->
<!-- square of each prediction change when moving from NO model to the column -->
<!-- model. The following table presents the square of each prediction -->
<!-- change: -->
<!-- ::: center -->
<!--    Country A   Country B   Country C -->
<!--   ----------- ----------- ----------- -->
<!--        4           0           4 -->
<!--        4           0           4 -->
<!--        4           0           4 -->
<!--        4           0           4 -->
<!-- ::: -->
<!-- Table of the Square of the Prediction Change when Moving from NO Model -->
<!-- to the Column Model $($ Total $)=32$. -->
<!-- ## ANOVA Table {anova-table-1 .unnumbered} -->
<!-- The ANOVA Table for the column factor can now be filled in as shown -->
<!-- below: -->
<!-- ::: center -->
<!-- +---------------+---------------------+---------------------+-----------------------+----------------------------+ -->
<!-- |   ----------- |   ----------------- |   ----------------- |   -------------       | F Ratio                    | -->
<!-- |    Source of  |      Degrees of     |        Sum of       |    Mean Sum of        |                            | -->
<!-- |    Variation  |        Freedom      |        Square       |      Squares          |                            | -->
<!-- |   ----------- |    $(\mathrm{df})$  |    $(\mathrm{SS})$  |    $($ MSS $)$        |                            | -->
<!-- |               |   ----------------- |   ----------------- |   -------------       |                            | -->
<!-- +:=============:+:===================:+:===================:+:=====================:+:==========================:+ -->
<!-- | Treatments    | 2                   | 32                  | $\frac{32}{2}=16$     | $\frac{16}{3.1111}=5.1428$ | -->
<!-- +---------------+---------------------+---------------------+-----------------------+----------------------------+ -->
<!-- | Error         | 9                   | 28                  | $\frac{28}{9}=3.1111$ |                            | -->
<!-- +---------------+---------------------+---------------------+-----------------------+----------------------------+ -->
<!-- | Total         | 11                  | 60                  |                       |                            | -->
<!-- +---------------+---------------------+---------------------+-----------------------+----------------------------+ -->
<!-- ::: -->
<!-- So for this one-factor ANOVA model, $F_{*}=5.1428$. -->
<!-- ## Conclusion {conclusion .unnumbered} -->
<!-- If the null hypothesis is true, then the F-statistic should be a value -->
<!-- from the $F_{2,9}$ distribution. Referring to the table that contains -->
<!-- the upper 0.05 cut-off points of F distributions, we see that -->
<!-- $F_{(2,9), 0.05}=4.256$. Since 5.1428 is greater than 4.256 , this tells -->
<!-- us that the F -statistic is in the upper 0.05 of the $F_{2,9}$ -->
<!-- distribution. Therefore we reject the null hypothesis at the 0.05 -->
<!-- significance level, and we conclude that the country means are not all -->
<!-- the same. Thus, the prediction for the next sale in a known country is -->
<!-- the mean of all the previous sales in that country. -->
<!--     ## R Code; -->
<!--     salesA=c(6,10,7,9); -->
<!--     salesB=c (10, 8,12,10); -->
<!--     salesC=c(14,13,11,10); -->
<!--     sales=c(salesA,salesB,salesC); -->
<!--     country=c(rep(1,4),rep(2,4),rep(3,4)); -->
<!--     oneway.test(sales~country,var.equal=TRUE); -->
<!--     #### -->
<!--     #### One-way analysis of means -->
<!--     #### -->
<!--     #### data: sales and country -->
<!--     #### F = 5.1429, num df = 2, denom df = 9, p-value = -->
<!--     #### 0.0324 -->
<!--     ## R Code; -->
<!--     ## Another way: using lm; -->
<!--     salesA=c(6,10,7,9); -->
<!--     salesB=c (10, 8,12,10); -->
<!--     salesC=c(14,13,11,10); -->
<!--     sales=c(salesA,salesB,salesC); -->
<!--     country=c(rep(1,4),rep(2,4),rep(3,4)); -->
<!--     anova(lm(sales~factor(country))); -->
<!--     #### Analysis of Variance Table -->
<!--     #### -->
<!--     #### Response: sales -->
<!--     #### Df Sum Sq Mean Sq F value Pr(>F) -->
<!--     #### factor(country) 2 32 16.0000 5.1429 0.0324 * -->
<!--     #### Residuals 9 28 3.1111 -->
<!--     #### --- -->
<!--     #### Signif. codes: -->
<!--     #### 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- Officially, to use the predictions from an ANOVA model, three -->
<!-- assumptions about the populations from which the sample was taken must -->
<!-- be satisfied: -->
<!-- 1.  Each population has a Normal distribution. -->
<!-- 2.  Each population has the same standard deviation $\sigma$. -->
<!-- 3.  The observations are mutually independent of one another. -->
<!-- ## Formulas {formulas .unnumbered} -->
<!-- Sum of Squares for Treatments (a.k.a. between-treatments variation or -->
<!-- Explained) -->
<!-- $$S S T=\sum_{j=1}^{k} n_{j}\left(\bar{x}_{j}-\overline{\bar{x}}\right)^{2}$$ -->
<!-- Sum of Squares for Error (a.k.a. within-treatments variation or -->
<!-- Unexplained) -->
<!-- $$S S E=\sum_{j=1}^{k} \sum_{i=1}^{n_{j}}\left(x_{i j}-\bar{x}_{j}\right)^{2}=\left(n_{1}-1\right) s_{1}^{2}+\ldots+\left(n_{k}-1\right) s_{k}^{2}$$ -->
<!-- ## Formulas {formulas-1 .unnumbered} -->
<!-- ## Mean Square for Treatments {mean-square-for-treatments .unnumbered} -->
<!-- $$M S T=\frac{S S T}{k-1}$$ -->
<!-- Mean Square for Error -->
<!-- $$M S E=\frac{S S E}{n-k}$$ -->
<!-- ## Formulas {formulas-2 .unnumbered} -->
<!-- ## Test Statistic {test-statistic .unnumbered} -->
<!-- $$F=\frac{M S T}{M S E}$$ -->
<!-- ## Exercise {exercise .unnumbered} -->
<!-- A statistics practitioner calculated the following statistics: -->
<!-- ::: center -->
<!--    Statistic   1    Treatment   2 -->
<!--   ----------- ---- ----------- ---- -->
<!--        n       5        5       3 -->
<!--    $\bar{x}$   10      15       20 -->
<!--     $s^{2}$    50      50       50 -->
<!-- ::: -->
<!-- Complete the ANOVA table. -->
<!-- ## Solution {solution .unnumbered} -->
<!-- $$\begin{aligned} -->
<!-- & \overline{\bar{x}}=\frac{5(10)+5(15)+5(20)}{5+5+5}=15 \\ -->
<!-- & S S T=5(10-15)^{2}+5(15-15)^{2}+5(20-15)^{2}=250 \\ -->
<!-- & S S E=(5-1)(50)+(5-1)(50)+(5-1)(50)=600 -->
<!-- \end{aligned}$$ -->
<!-- ## ANOVA Table {anova-table-2 .unnumbered} -->
<!-- ::: center -->
<!--   Source of Variation   Degrees of Freedom (df)   Sum of Square (SS)   Mean Sum of Squares (MSS)   F Ratio -->
<!--   --------------------- ------------------------- -------------------- --------------------------- ----------------------- -->
<!--   Treatments            2                         250                  $\frac{250}{2}=125$         $\frac{125}{50}=2.50$ -->
<!--   Error                 12                        600                  $\frac{600}{12}=50$          -->
<!--   Total                 14                        850                                               -->
<!-- ::: -->
<!-- ## Exercise {exercise-1 .unnumbered} -->
<!-- A statistics practitioner calculated the following statistics: -->
<!-- ::: center -->
<!--    Statistic   1    Treatment   -->
<!--   ----------- ---- ----------- ---- -->
<!--        n       4        4       3 -->
<!--    $\bar{x}$   20      22       25 -->
<!--     $s^{2}$    10      10       10 -->
<!-- ::: -->
<!-- Complete the ANOVA table. -->
<!-- ## Solution {solution-1 .unnumbered} -->
<!-- $$\begin{aligned} -->
<!-- & \overline{\bar{x}}=\frac{4(20)+4(22)+4(25)}{4+4+4}=22.33 \\ -->
<!-- & S S T=4(20-22.33)^{2}+4(22-22.33)^{2}+4(25-22.33)^{2}=50.67 \\ -->
<!-- & S S E=(4-1)(10)+(4-1)(10)+(4-1)(10)=90 -->
<!-- \end{aligned}$$ -->
<!-- ## ANOVA Table {anova-table-3 .unnumbered} -->
<!-- ::: center -->
<!--   Source of Variation   Degrees of Freedom (df)   Sum of Square (SS)   Mean Sum of Squares (MSS)   F Ratio -->
<!--   --------------------- ------------------------- -------------------- --------------------------- ------------------------- -->
<!--   Treatments            2                         50.67                $\frac{50.67}{2}=25.33$     $\frac{25.33}{10}=2.53$ -->
<!--   Error                 9                         90                   $\frac{90}{9}=10$            -->
<!--   Total                 11                        140.67                                            -->
<!-- ::: -->
<!-- ## Exercise {exercise-2 .unnumbered} -->
<!-- A consumer organization was concerned about the differences between the -->
<!-- advertised sizes of containers and the actual amount of product. In a -->
<!-- preliminary study, six packages of three different brands of margarine -->
<!-- that are supposed to contain 500 ml were measured. The differences from -->
<!-- 500 ml are listed here. Do these data provide sufficient evidence to -->
<!-- conclude that differences exist between the three brands? Use -->
<!-- $\alpha=0.05$. -->
<!-- ::: center -->
<!--    Brand 1   Brand 2   Brand 3 -->
<!--   --------- --------- --------- -->
<!--       1         2         1 -->
<!--       3         2         2 -->
<!--       3         4         4 -->
<!--       0         3         2 -->
<!--       1         0         3 -->
<!--       0         4         4 -->
<!-- ::: -->
<!-- Exercise on 3 bounds -->
<!-- ::: center -->
<!-- +-------------------+---------------+------------+-------------+ -->
<!-- |   --------------- | Brand 1       | Brand 2    | Brand 3     | -->
<!-- |   consider        |               |            |             | -->
<!-- |   $\alpha=0.05$   |               |            |             | -->
<!-- |   --------------- |               |            |             | -->
<!-- +:==================+:==============+:===========+:============+ -->
<!-- |                   | $x_{1 / 2}=1$ | $x_{21}=2$ | $x_{3 /}=1$ | -->
<!-- +-------------------+---------------+------------+-------------+ -->
<!-- |                   | $x_{12}=3$    | $x_{22}=2$ | $x_{32}=2$  | -->
<!-- +-------------------+---------------+------------+-------------+ -->
<!-- |                   | 3             | 4          | 4           | -->
<!-- +-------------------+---------------+------------+-------------+ -->
<!-- |                   | 0             | 3          | 2           | -->
<!-- +-------------------+---------------+------------+-------------+ -->
<!-- |                   | 1             | 0          | 3           | -->
<!-- +-------------------+---------------+------------+-------------+ -->
<!-- |                   | $x_{16} 0$    | $x_{26}$   | $x_{36}=4$  | -->
<!-- +-------------------+---------------+------------+-------------+ -->
<!-- |                   | $n_{1}=6$     | $n_{2}=6$  | $n_{3}=6$   | -->
<!-- +-------------------+---------------+------------+-------------+ -->
<!-- ::: -->
<!-- Overall Sample size: $n=n_{1}+n_{2}+n_{3}=18$\ -->
<!-- Sample means\ -->
<!-- Group 1 : -->
<!-- $\bar{x}_{1}=\frac{1}{n_{1}} \sum_{j=1}^{n_{1}} x_{1 j}=\frac{1}{6}(1+3 \ldots+0)=1,33$\ -->
<!-- Group 2 : -->
<!-- $\bar{x}_{2}=\frac{1}{n_{2}} \sum_{j=1}^{n} x_{2 j}=\frac{1}{6}(2+2 \ldots+4)=2,50$\ -->
<!-- Group 3: -->
<!-- $\bar{x}_{3}=\frac{1}{n_{3}} \sum_{j=1}^{n_{1}} x_{3 j}=\frac{1}{6}(1+2 \ldots+4)=2.67$\ -->
<!-- Overall mean\ -->
<!-- (1)\ -->
<!-- (3) -->
<!-- $$\begin{aligned} -->
<!-- \bar{x} & =\frac{\sum_{i=1}^{k} \sum_{j=1}^{n i} x_{i j}=\frac{1+3+\ldots+0+2+2+\ldots+4+\sqrt{1+2+\ldots+4}}{18}}{n} \\ -->
<!-- & =\frac{1}{n} \sum_{i=1}^{k} n_{i} \bar{x}_{i}=\frac{1}{18}[(86) 01,33)+(6)(2.5)+(6)(2.67) \\ -->
<!-- & =2.17 -->
<!-- \end{aligned}$$ -->
<!-- ::: center -->
<!--   Brand 1        Brand 2      Brand 3 -->
<!--   ------------ ------------ ------------ -->
<!--   $x_{11}=1$    $x_{21}=2$   $x_{31}=1$ -->
<!--   $x_{12}=3$    $x_{22}=2$   $x_{32}=2$ -->
<!--   3                 4            4 -->
<!--   0                 3            2 -->
<!--   1                 0            3 -->
<!--   $x_{16}$          0         $x_{26}$ -->
<!-- ::: -->
<!-- Sample Variances\ -->
<!-- Group 1: SI -->
<!-- ${ }_{1}^{2}=\frac{1}{n_{1}-1} \sum_{j=1}^{n_{1}}\left(x_{i j}-\bar{x}_{1}\right)^{2}$ -->
<!-- $$\begin{aligned} -->
<!-- & =\frac{1}{6-1}\left[(1-1,33)^{2}+\ldots+(0-1,33)^{2}\right] \\ -->
<!-- & =1,87 -->
<!-- \end{aligned}$$ -->
<!-- Grap 2! $S_{2}{ }^{2}=$ 1 -->
<!-- $$=2,30$$ -->
<!-- Group $3: S_{3}^{2}=$ -->
<!-- $$=1,47$$ -->
<!-- $$\begin{array}{lll} -->
<!-- \hline \text { Brand } 1 & \text { Brand } 2 & \text { Brand } 3 \\ -->
<!-- \hline x_{11}=1 & x_{21}=2 & x_{31}=1 \\ -->
<!-- x_{12}=3 & x_{22}=2 & x_{32}=2 \\ -->
<!-- 3 & 4 & 4 \\ -->
<!-- 0 & 3 & 2 \\ -->
<!-- 1 & 0 & 3 \\ -->
<!-- x_{16} & 0 & x_{26} \\ -->
<!-- \hline n_{1}=6 & 4 & x_{36}=4 \\ -->
<!-- \bar{x}_{1}=1,33 & \bar{x}_{2}=6 & n_{3}=6 \\ -->
<!-- S_{1}^{2}=1,80 & S_{2}^{2}=2,30 & \bar{x}_{3}=2,67 \\ -->
<!-- S_{3}^{2}=1,47 -->
<!-- \end{array}$$ -->
<!-- overall near $\bar{x}=2.17, \quad n=18$\ -->
<!-- within -->
<!-- $$\begin{aligned} -->
<!-- S S E & =\sum_{i=1}^{n_{i}}\left(n_{i}-1\right) s_{i}^{2}=(6-1) \cdot(1.87)+(6-1) \cdot(2.30)+(6-1)(1.47) \\ -->
<!-- & =28.20 -->
<!-- \end{aligned}$$ -->
<!-- Between it rt affect) -->
<!-- $$\begin{aligned} -->
<!-- & \delta \delta \operatorname{tot}=\sum_{r=1}^{n} n_{i}\left(\bar{x}_{i}-\bar{x}\right)^{2}=6(1,33-2,17)^{2}+6(2,50-2,17)^{2} \\ -->
<!-- &+6(2,67-2,17)^{2} -->
<!-- \end{aligned}$$ -->
<!-- $$=6,39$$ -->
<!-- ::: center -->
<!-- +:------------+:--------------:+:---------------:+:------------------------------------:+:-----------------------------:+ -->
<!-- | ANOVA Table | MS $=\frac{\delta S}{d f}$                                              |                               | -->
<!-- +-------------+----------------+-----------------+--------------------------------------+-------------------------------+ -->
<!-- | Source      |   ------------ |   ----------    |   --------                           | $F$-Stat                      | -->
<!-- |             |      $d f$     |    $S S$ of     |     Mean                             |                               | -->
<!-- |             |    degrees of  |    Squares      |    Squere                            |                               | -->
<!-- |             |     Freedom    |   ----------    |   --------                           |                               | -->
<!-- |             |   ------------ |                 |                                      |                               | -->
<!-- +-------------+----------------+-----------------+--------------------------------------+-------------------------------+ -->
<!-- | Treatment   | $K-1$          | $S S T r t$     | $M S T r=\frac{\delta S T r t}{K-1}$ | $F^{*}=\frac{M S T r}{M S E}$ | -->
<!-- +-------------+----------------+-----------------+--------------------------------------+-------------------------------+ -->
<!-- | Error       | $\cap-K$       | $S S G$         | $M S E=\frac{\delta S E}{n-K}$       | $X$                           | -->
<!-- +-------------+----------------+-----------------+--------------------------------------+-------------------------------+ -->
<!-- | Total       | $\cap-1$       | $S S T o t a l$ | $X$                                  | $X$                           | -->
<!-- +-------------+----------------+-----------------+--------------------------------------+-------------------------------+ -->
<!-- ::: -->
<!-- Hypothesis Test\ -->
<!-- $H_{0}: \mu_{1}=\mu_{2} \ldots .=\mu_{k} \quad \xrightarrow{\text { all }}$ -->
<!-- means are equal)\ -->
<!-- tHai. At least one $\mu_{j}, j=1, \ldots, k$ fat least one mean -->
<!-- different) is different Lat least one treatment has a different effect\ -->
<!-- ![image](2025_07_03_0e611eebd5aab7c4aee1g-42){width="\\textwidth"} -->
<!-- Hypothesis Test\ -->
<!-- $H_{0}: \mu_{1}=\mu_{2}=\mu_{3} \quad \rightarrow$ all treatments -->
<!-- equally affective\ -->
<!-- tHai: At least one $\mu_{j}, j=1, \ldots, 3$ fat least one mean -->
<!-- different) is different Lat least one treatment has a different effect -->
<!-- Test stat -->
<!-- $$F^{\hbar}=\frac{M S T_{r} t}{M S E}=\frac{S S T_{r} t / k-1}{S S E / n-k}=1.70 \sim F_{(2,15)}$$ -->
<!-- Fat num $=2$ and denom $=15 \mathrm{df}$\ -->
<!-- ![image](2025_07_03_0e611eebd5aab7c4aee1g-43){width="\\textwidth"}\ -->
<!-- 2.70 (table) -->
<!-- $$\begin{array}{r} -->
<!-- \text { p-ralue }>0,100>0,05 \\ -->
<!-- (\alpha) -->
<!-- \end{array}$$ -->
<!-- Insufficient evidence to reject $H_{0}: \mu_{1}=\mu_{2}=\mu_{3}$ The -->
<!-- analysis from AMOUA Suggests all 3 groups have same mean. -->
<!-- Suppose he rejected Ho. Which mean (or means) are different? -->
<!-- $$\mu_{1}, \mu_{2}, \mu_{3}$$ -->
<!-- pairmise comparisons\ -->
<!-- $M_{1}, M_{2}$\ -->
<!-- $\mu_{1}, \mu_{3}$\ -->
<!-- $\binom{k}{2}$\ -->
<!-- $\mu_{2}, \mu_{3}$ -->
<!-- Step 1. State Hypotheses.\ -->
<!-- $\mu_{i}=$ population mean for differences from 500 ml (brand $i$, -->
<!-- where\ -->
<!-- $i=1,2,3$ ).\ -->
<!-- $H_{0}: \mu_{1}=\mu_{2}=\mu_{3}$\ -->
<!-- $H_{a}$ : At least two means differ. -->
<!-- ## Solution {solution-2 .unnumbered} -->
<!-- Step 2. Compute test statistic. -->
<!-- ::: center -->
<!--               Brand 1   Brand 2   Brand 3 -->
<!--   ---------- --------- --------- --------- -->
<!--      Mean      1.33      2.50      2.67 -->
<!--    Variance    1.87      2.30      1.47 -->
<!-- ::: -->
<!-- Grand mean $=\overline{\bar{x}}=2.17$.\ -->
<!-- $S S T=6(1.33-2.17)^{2}+6(2.50-2.17)^{2}+6(2.67-2.17)^{2}=$ -->
<!-- $6.387 \approx 6.39$\ -->
<!-- SSE $=(6-1)(1.87)+(6-1)(2.30)+(6-1)(1.47)=28.20$ -->
<!-- ## Solution {solution-3 .unnumbered} -->
<!-- Grand mean $=\overline{\bar{x}}=2.17$. -->
<!-- $$\begin{aligned} -->
<!-- & S S T=6(1.33-2.17)^{2}+6(2.50-2.17)^{2}+6(2.67-2.17)^{2}= \\ -->
<!-- & 6.387 \approx 6.39 \\ -->
<!-- & S S E=(6-1)(1.87)+(6-1)(2.30)+(6-1)(1.47)=28.20 -->
<!-- \end{aligned}$$ -->
<!-- ## ANOVA Table {anova-table-4 .unnumbered} -->
<!-- ::: center -->
<!--   Source of Variation   Degrees of Freedom (df)   Sum of Square (SS)   Mean Sum of Squares (MSS)   F Ratio -->
<!--   --------------------- ------------------------- -------------------- --------------------------- --------------------------- -->
<!--   Treatments            2                         6.39                 $\frac{6.39}{2}=3.195$      $\frac{3.195}{1.88}=1.70$ -->
<!--   Error                 15                        28.20                $\frac{28.20}{15}=1.88$      -->
<!--   Total                 17                        34.59                                             -->
<!-- ::: -->
<!-- Step 3. Find Rejection Region.\ -->
<!-- We reject the null hypothesis only if -->
<!-- $$F>F_{\alpha, k-1, n-k}$$ -->
<!-- If we let $\alpha=0.05$, the rejection region for this exercise is -->
<!-- $$F>F_{0.05,2,15}=3.682$$ -->
<!-- ## Step 4. Conclusion. {step-4.-conclusion. .unnumbered} -->
<!-- We found the value of the test statistic to be $F=1.70$. Since -->
<!-- $F=1.70<F_{0.05,2,15}=3.682$, we can't reject $H_{0}$. Thus, there is -->
<!-- not evidence to infer that the average differences differ between the -->
<!-- three brands. -->
<!--     ## R Code; -->
<!--     brand1=c(1,3,3,0,1,0); -->
<!--     brand2=c (2,2,4,3,0,4); -->
<!--     brand3=c(1,2,4,2,3,4); -->
<!--     differences=c(brand1,brand2,brand3); -->
<!--     brand=c(rep(1,6),rep(2,6),rep(3,6)); -->
<!--     oneway.test(differences~brand,var.equal=TRUE); -->
<!--     #### -->
<!--     #### One-way analysis of means -->
<!--     #### -->
<!--     #### data: differences and brand -->
<!--     #### F = 1.6864, num df = 2, denom df = 15, p-value -->
<!--     #### = 0.2185 -->
<!-- Margarine example -->
<!-- ::: displayquote -->
<!-- brand = as.factor(brand)\ -->
<!-- anova_model = aov(differences\~brand)\ -->
<!-- anova(anova_model)\ -->
<!-- Analysis of Variance Table\ -->
<!-- Response: differences -->
<!-- ::: -->
<!-- ::: center -->
<!--   ----------- ---- --------- --------- ----------- -->
<!--                 Df    Sum Sq   Mean Sq  $F$ value -->
<!--   brand          2    6.3333    3.1667   1.6864 -->
<!--   Residuals     15   28.1667    1.8778   0.2185 -->
<!--   Resrur                                -->
<!--   ----------- ---- --------- --------- ----------- -->
<!-- ::: -->
<!-- Totals\ -->
<!-- a nut given by $R$ -->
<!-- ## Exercise {exercise-3 .unnumbered} -->
<!-- The friendly folks a the Internal Revenue Service (IRS) in the United -->
<!-- States and Canada Revenue Agency (CRA) are always looking for ways to -->
<!-- improve the wording and format of its tax return forms. Three new forms -->
<!-- have been developed recently. To determine which, if any, are superior -->
<!-- to the current form, 120 individuals were asked to participate in an -->
<!-- experiment. Each of the three new forms and the currently used form were -->
<!-- filled out by 30 different people. The amount of time (in minutes) taken -->
<!-- by each person to complete the task was recorded.\ -->
<!-- What conclusions can be drawn from these data? -->
<!-- ## R Code {r-code .unnumbered} -->
<!--     ##Step 1. Entering data; -->
<!--     ## importing data; -->
<!--     ## url of tax return forms; -->
<!--     forms_url = -->
<!--     "https://mcs.utm.utoronto.ca/"nosedal/data/tax-forms.txt" -->
<!--     forms_data= read.table(forms_url,header=TRUE); -->
<!--     names(forms_data); -->
<!--     forms_data[1:4, ]; -->
<!-- ## R Code {r-code-1 .unnumbered} -->
<!-- ::: center -->
<!-- +-------------------------------------------+ -->
<!-- |                                           | -->
<!-- +:======+:======+:======+:======+:======+:==+ -->
<!-- | \####   | Form1 | Form2 | Form3 | Form4 |   | -->
<!-- +-------+-------+-------+-------+-------+---+ -->
<!-- | \#### 1 | 23    | 88    | 116   | 103   |   | -->
<!-- +-------+-------+-------+-------+-------+---+ -->
<!-- | \#### 2 | 59    | 114   | 123   | 122   |   | -->
<!-- +-------+-------+-------+-------+-------+---+ -->
<!-- | \#### 3 | 68    | 81    | 64    | 105   |   | -->
<!-- +-------+-------+-------+-------+-------+---+ -->
<!-- | \#### 4 | 122   | 41    | 136   | 73    |   | -->
<!-- +-------+-------+-------+-------+-------+---+ -->
<!-- ::: -->
<!-- ## R Code {r-code-2 .unnumbered} -->
<!--     ##Step 2. ANOVA; -->
<!--     time1=forms_data$Form1; -->
<!--     time2=forms_data$Form2; -->
<!--     time3=forms_data$Form3; -->
<!--     time4=forms_data$Form4; -->
<!--     length(forms_data$Form1); -->
<!--     times=c(time1,time2,time3,time4); -->
<!--     forms=c(rep(1,30),rep(2,30),rep(3,30),rep(4,30)); -->
<!--     oneway.test(times~forms,var.equal=TRUE) -->
<!-- ## R Code {r-code-3 .unnumbered} -->
<!--     #### [1] 30 -->
<!--     #### -->
<!--     #### One-way analysis of means -->
<!--     #### -->
<!--     #### data: times and forms -->
<!--     #### F = 2.9358, num df = 3, denom df = 116, p-value -->
<!--     #### = 0.03632 -->
<!-- ## Assumptions of ANOVA {assumptions-of-anova .unnumbered} -->
<!-- Model -->
<!-- ::: center -->
<!--   -------------- -->
<!--   observalurs -->
<!--   of a surv -->
<!--   are assigned -->
<!--   the same -->
<!--   -------------- -->
<!-- ::: -->
<!-- \$\\left{\\begin{array}{l}Y\_{i -->
<!-- j}=\\underbrace{\\mu+T\_{i}}+\\varepsilon\_{i j}\ -->
<!-- Y\_{i j}=\\mu\_{i}+\\varepsilon\_{i j},\\end{array} \\varepsilon\_{i j} -->
<!-- \\sim N\\left(0, \\sigma\^{2}\\right)\\right.\$ your mean -->
<!-- ## Assumptions {assumptions .unnumbered} -->
<!-- I'Obser vutions are independent\ -->
<!-- ![image](2025_07_03_0e611eebd5aab7c4aee1g-59){width="\\textwidth"} -->
<!-- 3\ -->
<!-- Error terms (residuals) are Normal\ -->
<!-- 3\ -->
<!-- All groups have the same population variance -->
<!-- $$\sigma_{1}^{2}=\sigma_{2}^{2}=\ldots=\sigma_{k}^{2}=\sigma^{2}$$ -->
<!-- Estimated with the MSE -->
<!-- $$\begin{aligned} -->
<!-- \sigma_{2} & =\text { MdSE } \\ -->
<!-- & \text { from ArovA } -->
<!-- \end{aligned}$$ -->
<!-- ## Multiple Comparisons {multiple-comparisons .unnumbered} -->
<!-- ## Example {example .unnumbered} -->
<!-- Because of foreign competition, North American automobile manufacturers -->
<!-- have become more concerned with quality. One aspect of quality is the -->
<!-- cost of repairing damage caused by accidents. A manufacturer is -->
<!-- considering several new types of bumpers. To test how well they react to -->
<!-- low-speed collisions, 10 bumpers of each of four different types were -->
<!-- installed on mid-size cars, which were then driven into a wall at 5 -->
<!-- miles per hour. The cost of repairing the damage in each case was -->
<!-- assessed. The data are shown below.\ -->
<!-- a. Is there sufficient evidence at the $5 \%$ significance level to -->
<!-- infer that the bumpers differ in their reactions to low-speed -->
<!-- collisions? b. If differences exist, which bumpers differ? Apply -->
<!-- Fisher's LSD method with the Bonferroni adjustment. -->
<!-- ## Data {data .unnumbered} -->
<!-- ::: center -->
<!--   Bumper 1               Bumper 2   Bumper 3   Bumper 4 -->
<!--   ---------------------- ---------- ---------- ---------- -->
<!--   610                    404        599        272 -->
<!--   354                    663        426        405 -->
<!--   234                    521        429        197 -->
<!--   399                    518        621        363 -->
<!--   278                    499        426        297 -->
<!--   358                    374        414        538 -->
<!--   379                    562        332        181 -->
<!--   548                    505        460        318 -->
<!--   196                    375        494        412 -->
<!--   444                    438        637        499 -->
<!--   $\bar{x}=380$          \-         ᄀ         ר -->
<!--   $S_{1}^{2}=16924.22$   こ         \-         \- -->
<!--   $n_{1}=10$             \-                     -->
<!-- ::: -->
<!--     cost_bumper1 = c(610, 354, 234, 399, 278, 358, 379, 548, 196, 444) -->
<!--     cost_bumper2 = c(404, 663, 521, 518, 499, 374, 562, 505, 375, 438) -->
<!--     cost_bumper3 = c(599, 426, 429, 621, 426, 414, 332, 460, 494, 637) -->
<!--     cost_bumper4 = c(272, 405, 197, 363, 297, 538, 181, 318, 412, 499) -->
<!--     bumper_data = rbind( -->
<!--         data.frame(cost = cost_bumper1, type = "Bumper 1"), -->
<!--         data.frame(cost = cost_bumper2, type = "Bumper 2"), -->
<!--         data.frame(cost = cost_bumper3, type = "Bumper 3"), -->
<!--         data.frame(cost = cost_bumper4, type = "Bumper 4") -->
<!--     ) -->
<!--     ## data in long form at -->
<!--     bumper_data -->
<!--     library(mosaic) -->
<!--     result = do.call(rbind, lapply(list(cost_bumper1, cost_bumper2, -->
<!--         cost_bumper3, cost_bumper4), fav_stats)) -->
<!--     rownames(result) = paste("Bumper", 1:4) -->
<!--     result -->
<!-- ::: center -->
<!--            min   Q1       median   Q3       max   mean    sd          n    missing -->
<!--   -------- ----- -------- -------- -------- ----- ------- ----------- ---- --------- -->
<!--   Bumper   196   297.00   368.5    432.75   610   380.0   130.09313   10   0 -->
<!--   Bumper   374   412.50   502.0    520.25   663   485.9   90.53968    10   0 -->
<!--   Bumper   332   426.00   444.5    572.75   637   483.8   102.10866   10   0 -->
<!--   Bumper   181   278.25   340.5    410.25   538   348.2   118.52688   10   0 -->
<!-- ::: -->
<!--         ## Create anova model -->
<!--         bumper_model = aov(cost ~ type, data = bumper_data) -->
<!--         ## Get ANOVA table -->
<!--         anova(bumper_model) -->
<!--         Analysis of Variance Table -->
<!-- ::: center -->
<!-- ![image](2025_07_03_0e611eebd5aab7c4aee1g-65){width="\\textwidth"} -->
<!-- ::: -->
<!--     Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- Bumper Example\ -->
<!-- $H_{0}: \mu_{1}=\mu_{2}=\mu_{3}=\mu_{4}$\ -->
<!-- Hal At least one $\mu_{i}, i=1, \ldots 4$ is different\ -->
<!-- Test stat -->
<!-- $$\begin{aligned} -->
<!-- & F=4.0563 \sim F_{(3036)} \\ -->
<!-- & p \text {-value }=0.01395<0.05 \\ -->
<!-- & (\alpha) -->
<!-- \end{aligned}$$ -->
<!-- Sufficient Evidence to reject $H_{0}$ and conclude at least one soup of -->
<!-- bumpers has a different meas repair cost.\ -->
<!-- $\rightarrow$ which group / groups cure different?\ -->
<!-- perform pairwise comparisons\ -->
<!-- le. Create pairwise confidence intervals)\ -->
<!-- For this example -->
<!-- $$\begin{aligned} -->
<!-- & 1 \mu_{1}-\mu_{2} \\ -->
<!-- & 2 \mu_{1}-\mu_{3} \\ -->
<!-- & 3 \mu_{1}-\mu_{4} \\ -->
<!-- & 4 \mu_{2}-\mu_{3} \\ -->
<!-- & 5 \mu_{2}-\mu_{4} \\ -->
<!-- & 6 \mu_{3}-\mu_{4} -->
<!-- \end{aligned}$$ -->
<!-- ## Pairwise confidence Intervals {pairwise-confidence-intervals .unnumbered} -->
<!-- For each pair of groups $i$ and $j$, -->
<!-- $i=1, \ldots, k ; \quad j=1, \ldots, k ; \quad i \neq j$ -->
<!-- Fisher's LSD -->
<!-- $$\left.\left(\bar{x}_{i}-\bar{x}_{j}\right) \pm t_{(n}=k, \phi / 2\right) \cdot \sqrt{\text { MSE }\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}$$ -->
<!-- Bonferroni -->
<!-- $$\left.\overline{\left(\bar{x}_{i}-\bar{x}_{j}\right)} \pm t_{(n-k,} \phi / 2 m\right) \cdot \sqrt{\text { MSE }\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}$$ -->
<!-- $m=\binom{k}{2}$ \## pairwise comparisons\ -->
<!-- \[Note: not conducting pairwise hypothesis tests\]\ -->
<!-- Fisher's is easier to calculate, has mere statistical power, homer -->
<!-- Bunferroni maintains experiment wit error rate -->
<!-- Fisher: $\alpha_{e}=1-(1-\alpha)^{m}$\ -->
<!-- Bonferroni: $\alpha_{e}=1-\left(1-\frac{\alpha}{m}\right)^{m}$ -->
<!-- Fisher -->
<!-- ::: center -->
<!--   -------- --------------- ------------------------------------- -->
<!--   $m=1$,   $の=0,05$       $1-(1-\infty)^{1}=0,05$ -->
<!--   $m=2$,   $の=0,05$       $1-(1-\infty)^{2}>0,05$ -->
<!--   $m=3$    $\alpha=0,05$   $1-(1-\infty)^{3}>1-(1-\infty)^{2}$ -->
<!--   -------- --------------- ------------------------------------- -->
<!-- ::: -->
<!-- $n \uparrow$ error rate $\uparrow$ -->
<!-- Bonles roni\ -->
<!-- as $m p \quad d e=\left(1-\frac{\alpha}{m}\right)^{m}$ remains almust -->
<!-- constart -->
<!-- ## Solution a) {solution-a .unnumbered} -->
<!-- The test statistic is $F_{*}=4.06$ and the $P-$ value $=0.0139$. There -->
<!-- is enough statistical evidence to infer that there are differences -->
<!-- between some of the bumpers. The question is now, Which bumpers differ? -->
<!-- ## Fisher's Least Significant Difference Method {fishers-least-significant-difference-method .unnumbered} -->
<!-- ::: center -->
<!-- ![image](2025_07_03_0e611eebd5aab7c4aee1g-70){width="\\textwidth"} -->
<!-- ::: -->
<!-- ## Fisher's Least Significant Difference Method {fishers-least-significant-difference-method-1 .unnumbered} -->
<!-- The confidence interval estimator is -->
<!-- $$\left(\bar{x}_{i}-\bar{x}_{j}\right) \pm t_{\alpha / 2} \sqrt{M S E\left(\frac{1}{n_{i}}+\frac{1}{n_{j}}\right)}$$ -->
<!-- ## Least Significant Difference (definition) {least-significant-difference-definition .unnumbered} -->
<!-- We define the least significant difference LSD as -->
<!-- $$L S D=t_{\alpha / 2} \sqrt{M S E\left(\frac{1}{n_{i}}+\frac{1}{n_{j}}\right)}$$ -->
<!-- A simple way of determining whether differences exist between each pair -->
<!-- of population means is to compare the absolute value of the difference -->
<!-- between their two sample means and LSD. In other words, we will conclude -->
<!-- that $\mu_{i}$ and $\mu_{j}$ differ if -->
<!-- $$\left|\bar{x}_{i}-\bar{x}_{j}\right|>L S D$$ -->
<!-- LSD will be the same for all pairs of means if all $k$ sample sizes are -->
<!-- equal. If some sample sizes differ, LSD must be calculated for each -->
<!-- combination.\ -->
<!-- It can be argued that this method is flawed because it will increase the -->
<!-- probability of committing a Type I error. That is, it is more likely -->
<!-- that the analysis of variance to conclude that a difference exists in -->
<!-- some of the population means when in fact none differ. -->
<!-- The true probability of making at least one Type I error is called the -->
<!-- experimentwise Type I error rate, denoted $\alpha_{E}$. The -->
<!-- experimentwise Type I error rate can be calculated as -->
<!-- $$\alpha_{E}=1-(1-\alpha)^{C}$$ -->
<!-- Here $C$ is the number of pairwise comparisons, which can be calculated -->
<!-- by $C=\frac{k(k-1)}{2}$. It can be shown that -->
<!-- $$\alpha_{E} \leq C \alpha$$ -->
<!-- which means that if we want the probability of making at least one Type -->
<!-- I error to be no more than $\alpha_{E}$, we simply specify -->
<!-- $\alpha=\frac{\alpha_{E}}{C}$. The resulting procedure is called the -->
<!-- Bonferroni adjustment. -->
<!-- ## Solution b) {solution-b .unnumbered} -->
<!-- Let's use our example to illustrate Fisher's LSD method and the -->
<!-- Bonferroni adjustment. The four sample means and standard deviations -->
<!-- are\ -->
<!-- $\bar{y}_{1}=380$ and $s_{1}=130.0931$\ -->
<!-- $\bar{y}_{2}=485.9$ and $s_{2}=90.5396$\ -->
<!-- $\bar{y}_{3}=483.8$ and $s_{3}=102.1086$\ -->
<!-- $\bar{y}_{4}=348.2$ and $s_{4}=118.5268$ -->
<!-- ## Solution b) {solution-b-1 .unnumbered} -->
<!-- The pairwise absolute differences are -->
<!-- $$\begin{aligned} -->
<!-- & \left|\bar{y}_{1}-\bar{y}_{2}\right|=|380-485.9|=105.9 \\ -->
<!-- & \left|\bar{y}_{1}-\bar{y}_{3}\right|=|380-483.8|=103.8 \\ -->
<!-- & \left|\bar{y}_{1}-\bar{y}_{4}\right|=|380-348.2|=31.8 \\ -->
<!-- & \left|\bar{y}_{2}-\bar{y}_{3}\right|=|485.9-483.8|=2.1 \\ -->
<!-- & \left|\bar{y}_{2}-\bar{y}_{4}\right|=|485.9-348.2|=137.7 \\ -->
<!-- & \left|\bar{y}_{3}-\bar{y}_{4}\right|=|483.8-348.2|=135.6 -->
<!-- \end{aligned}$$ -->
<!-- ## Solution b) {solution-b-2 .unnumbered} -->
<!-- We have that $M S E=12,399$ and $\nu=n-k=40-4=36$.\ -->
<!-- If we perform the LSD procedure with the Bonferroni adjustment, the -->
<!-- number of pairwise comparisons is 6 . We set $\alpha=0.05 / 6=0.0083$. -->
<!-- Thus $t_{\alpha / 2, n-k}=t_{0.00415,36}=2.7935$ (using R ) and\ -->
<!-- qt (0.00415,36)\ -->
<!-- \#### \[1\] -2.793555\ -->
<!-- $\operatorname{LSD}=t_{\alpha / 2} \sqrt{\operatorname{MSE}\left(\frac{1}{n_{i}}+\frac{1}{n_{j}}\right)} \approx 2.7935 \sqrt{12399\left(\frac{1}{10}+\frac{1}{10}\right)}=139.109$ -->
<!-- Now no pair of means differ because all the absolute values of the -->
<!-- differences between sample means are less than 139.1095.\ -->
<!-- The drawback to the LSD procedure is that we increase the probability of -->
<!-- at least one Type I error. The Bonferroni adjustment corrects this -->
<!-- problem. -->
<!-- ## Bumper example {bumper-example .unnumbered} -->
<!--     library(mosaic) -->
<!--     result = do.call(rbind, lapply(list(cost_bumper1, cost_bumper2, -->
<!--         cost_bumper3, cost_bumper4), fav_stats)) -->
<!--     rownames(result) = paste("Bumper", 1:4) -->
<!--     result -->
<!-- ::: center -->
<!-- +:--------------+:-------+:----+-------:+-------:+-------:+----:+------:+----------:+---:+--------:+ -->
<!-- |               |        | min | Q1     | median | Q3     | max | mean  | sd             | n       | -->
<!-- |               |        |     |        |        |        |     |       |                | missing | -->
<!-- +---------------+--------+-----+--------+--------+--------+-----+-------+-----------+----+---------+ -->
<!-- | $\rightarrow$ | Bumper | 196 | 297.00 | 368.5  | 432.75 | 610 | 380.0 | 130.09313 | 10 | 0       | -->
<!-- |               | 1      |     |        |        |        |     |       |           |    |         | -->
<!-- +---------------+--------+-----+--------+--------+--------+-----+-------+-----------+----+---------+ -->
<!-- | $\rightarrow$ | Bumper | 374 | 412.50 | 502.0  | 520.25 | 663 | 485.9 | 90.53968  | 10 | 0       | -->
<!-- |               | 2      |     |        |        |        |     |       |           |    |         | -->
<!-- +---------------+--------+-----+--------+--------+--------+-----+-------+-----------+----+---------+ -->
<!-- |               | Bumper | 332 | 426.00 | 444.5  | 572.75 | 637 | 483.8 | 102.10866 | 10 | 0       | -->
<!-- |               | 3      |     |        |        |        |     |       |           |    |         | -->
<!-- +---------------+--------+-----+--------+--------+--------+-----+-------+-----------+----+---------+ -->
<!-- |               | Bumper | 181 | 278.25 | 340.5  | 410.25 | 538 | 348.2 | 118.52688 | 10 | 0       | -->
<!-- |               | 4      |     |        |        |        |     |       |           |    |         | -->
<!-- +---------------+--------+-----+--------+--------+--------+-----+-------+-----------+----+---------+ -->
<!-- ::: -->
<!-- For the bumper example, calculate a $95 \%$ pairwise CI for the -->
<!-- difference between Bumper (1) and Bumper (2) using LSD and Bonferroni -->
<!-- Fisher's LSD $\quad\left(\mu_{1}-\mu_{2}\right)$\ -->
<!-- l-の $=0.98 \quad k=4 \quad$ つ $=40$ $\alpha=0.05$\ -->
<!-- $\left(\bar{x}_{1}-\bar{x}_{2}\right) \pm t_{r n-k, \alpha / 2} \cdot \sqrt{\text { MSE }\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}$ -->
<!-- from ANOUA -->
<!-- $==\underbrace{(380-485.9) \pm t_{(36,0.025)}}_{1} \cdot \sqrt{123999\left(\frac{1}{10}+\frac{1}{10}\right)}$ -->
<!-- $$=2,028 \quad \frac{B}{q q t(1-0,025,36)}$$ -->
<!-- $=-105.9 \pm 100.9942$\ -->
<!-- $=\underset{\text { (-ie) }}{21-206.894},-4,9058) \quad \mu_{1}-\mu_{2} \neq 0$ -->
<!-- Fishers LSD suggests $\mu_{2}>\mu_{1}$ -->
<!-- Bonferroni $\left(\mu_{1}-\mu_{2}\right)$ 曲pairmise compurisons -->
<!-- $$\begin{array}{cc} -->
<!-- \begin{array}{cc} -->
<!-- 1-\alpha=0.95 & n=40 \\ -->
<!-- \alpha=0.05 & k=4 -->
<!-- \end{array} & n=\binom{k}{2}=\binom{4}{2}=6 \\ -->
<!-- \left(\bar{x}_{i}-\bar{x}_{i}\right) \pm t_{(n-k, 9 / 2 m)} & \sqrt{\text { MSE }\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)} \\ -->
<!-- =(380-485.9) & 0.004167 \\ -->
<!-- & \pm t_{(36,} \overbrace{0.05 /(2 \times 6)}) \sqrt{12399\left(\frac{1}{10}+\frac{1}{10}\right)} -->
<!-- \end{array}$$ -->
<!-- $$2,79197$$ -->
<!-- $$\frac{B}{2 q+(1-0,004162,36)}$$ -->
<!-- $$\begin{aligned} -->
<!-- & =-105,9 \pm 139,0338, \quad \mu_{1}-\mu_{2} \\ -->
<!-- & =\left(\begin{array}{ccc} -->
<!-- -244,9335, & 33,13347 -->
<!-- \end{array}\right) \quad \frac{0}{-244} -->
<!-- \end{aligned}$$ -->
<!-- Borferroni informs us it is plansible fer $\mu_{1}=\mu_{2}$\ -->
<!-- library(DescTools)\ -->
<!-- PostHocTest(bumper_model, method = \"lsd\") -->
<!-- Posthoc multiple comparisons of means : Fisher LSD $95 \%$ family-wise -->
<!-- confidence level\ -->
<!-- \$type -->
<!-- ::: center -->
<!--                                   diff     lwr.ci        upr.ci      pval -->
<!--   --------------------------- --- -------- ------------- ----------- -------- -->
<!--   Bumper                      1   105.9    4.905342      206.89466   0.0404 -->
<!--   \-                          1   103.8    2.805342      204.79466   0.0443 -->
<!--   Bumper                      1   -31.8    -132.794658   69.19466    0.5271 -->
<!--   Bumper                      2   -2.1     -103.094658   98.89466    0.9666 -->
<!--   \- Bumper                   2   -137.7   -238.694658   -36.70534   0.0089 -->
<!--   ${ }^{\text {- }}$ Bumper   3   -135.6   -236.594658   -34.60534   0.0099 -->
<!-- ::: -->
<!-- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ -->
<!-- library(DescTools)\ -->
<!-- PostHocTest(bumper_model, method = \"bonferroni\") -->
<!-- Posthoc multiple comparisons of means : Bonferroni $95 \%$ family-wise -->
<!-- confidence level\ -->
<!-- \$type -->
<!-- ::: center -->
<!--                       diff     lwr.ci       upr.ci       pval -->
<!--   ------------------- -------- ------------ ------------ -------- -->
<!--   Bumper 2-Bumper 1   105.9    -33.13411    244.934113   0.2423 -->
<!--   Bumper 3-Bumper 1   103.8    -35.23411    242.834113   0.2657 -->
<!--   Bumper 4-Bumper 1   -31.8    -170.83411   107.234113   1.0000 -->
<!--   Bumper 3-Bumper 2   -2.1     -141.13411   136.934113   1.0000 -->
<!--   Bumper 4-Bumper 2   -137.7   -276.73411   1.334113     0.0535 -->
<!--   Bumper 4-Bumper     -135.6   -274.63411   3.434113     0.0595 -->
<!-- ::: -->
<!-- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- ## Bonferroni {bonferroni .unnumbered} -->
<!-- Control fer\ -->
<!-- exper iment wise error\ -->
<!-- $\alpha_{e}=1-\left(1-\frac{\alpha}{m}\right)^{m}$ -->
<!-- Suitable when control of Type I\ -->
<!-- error is required\ -->
<!-- higher rijk of Type II error\ -->
<!-- lover power\ -->
<!-- $\alpha_{e}=1-(1-\alpha)^{m}$ -->
<!-- Suitable when control of Type I error is not a strict concern -->
<!-- Lower risk of Type II error higher power -->
<!-- ## Example {example-1 .unnumbered} -->
<!-- An apple juice manufacturer has developed a new product - a liquid -->
<!-- concentrate that, when mixed with water, produces 1 liter of apple -->
<!-- juice. The product has several attractive features. First, it is more -->
<!-- convenient that canned apple juice. Second, because the apple juice that -->
<!-- is sold in cans is actually made from concentrate, the quality of the -->
<!-- new product is at least as high as canned apple juice. Third, the cost -->
<!-- of the new product is slightly lower than canned apple juice. The -->
<!-- marketing manager has to decide how to market the new product. She can -->
<!-- create advertising that emphasizes convenience, quality, or price. -->
<!-- ## Example {example-2 .unnumbered} -->
<!-- To facilitate a decision, she conducts an experiment. In three different -->
<!-- cities that are similar in size and demographic makeup, she launches the -->
<!-- product with advertising stressing the convenience of the liquid -->
<!-- concentrate in one city. In the second city, the advertisements -->
<!-- emphasize the quality of the product. Advertising that highlights the -->
<!-- relatively low cost of the liquid concentrate is used in the third city. -->
<!-- The number of packages sold weekly is recorded for the 20 weeks -->
<!-- following the beginning of the campaign. -->
<!-- ## Example {example-3 .unnumbered} -->
<!-- These data are available at: -->
<!--     ad_url = -->
<!--     "https://mcs.utm.utoronto.ca/ ~nosedal/data/ad-strategies.t -->
<!-- The marketing manager wants to know if differences in sales exist -->
<!-- between the three advertising strategies. -->
<!-- ## Example {example-4 .unnumbered} -->
<!-- To illustrate Fisher's LSD method and the Bonferroni adjustment, -->
<!-- consider the dataset described above and assume we tested to determine -->
<!-- whether population means differ using a $5 \%$ significance level. The -->
<!-- three sample means are: $577.55,653.0$, and 608.65 .\ -->
<!-- The pair-wise absolute differences are -->
<!-- $$\begin{aligned} -->
<!-- & \left|\bar{x}_{1}-\bar{x}_{2}\right|=|577.55-653.0|=|-75.45|=75.45 \\ -->
<!-- & \left|\bar{x}_{1}-\bar{x}_{3}\right|=|577.55-608.65|=|-31.10|=31.10 \\ -->
<!-- & \left|\bar{x}_{2}-\bar{x}_{3}\right|=|653.0-608.65|=|44.35|=44.35 -->
<!-- \end{aligned}$$ -->
<!-- ## Example {example-5 .unnumbered} -->
<!-- If we conduct the LSD procedure with $\alpha=0.05$ we find -->
<!-- $\left|t_{\alpha / 2, n-k}\right|=\left|t_{0.025,57}\right|=|-2.0024655|=2.0024655$\ -->
<!-- qt (0.025,57)\ -->
<!-- \#### \[1\] -2. 002465\ -->
<!-- $L S D=t_{\alpha / 2} \sqrt{M S E\left(\frac{1}{n_{i}}+\frac{1}{n_{j}}\right)} \approx 2.002 \sqrt{8894\left(\frac{1}{20}+\frac{1}{20}\right)}=59.71$ -->
<!-- ## Example {example-6 .unnumbered} -->
<!-- We can see that only one pair of sample means differ by more than 59.71. -->
<!-- That is, $\left|\bar{x}_{1}-\bar{x}_{2}\right|=75.45$, and the other two -->
<!-- differences are less than LSD. We conclude that only $\mu_{1}$ and -->
<!-- $\mu_{2}$ differ.\ -->
<!-- If we perform the LSD procedure with the Bonferroni adjustment, the -->
<!-- number of pairwise comparisons is 3 (calculated as -->
<!-- $C=k(k-1) / 2=3(2) / 2$. We set $\alpha=0.05 / 3=0.0167$. Thus, -->
<!-- $t_{\alpha / 2, n-k}=t_{0.0083,57}=-2.4682794$ and -->
<!-- $$t_{\alpha / 2} \sqrt{M S E\left(\frac{1}{n_{i}}+\frac{1}{n_{j}}\right)} \approx 2.467 \sqrt{8894\left(\frac{1}{20}+\frac{1}{20}\right)}=73.54$$ -->
<!-- Again we conclude that only $\mu_{1}$ and $\mu_{2}$ differ. Notice, -->
<!-- however, in the second calculation LSD is larger.\ -->
<!-- The drawback to LSD is that we increase the probability of at least one -->
<!-- Type I error. The Bonferroni adjustment corrects this problem. -->

</div>
            </section>

          </div>
        </div>
      </div>
<a href="inference-for-simple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="analysis-of-categorical-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
