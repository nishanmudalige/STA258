<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Sampling Distributions Related to a Normal Population | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Sampling Distributions Related to a Normal Population | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Sampling Distributions Related to a Normal Population | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="descriptive-statistics-and-an-introduction-to-r.html"/>
<link rel="next" href="the-central-limit-theorem.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html"><i class="fa fa-check"></i><b>1</b> Descriptive Statistics and an Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#descriptive-statistics"><i class="fa fa-check"></i><b>1.2</b> Descriptive Statistics</a></li>
<li class="chapter" data-level="1.3" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#graphical-techniques"><i class="fa fa-check"></i><b>1.3</b> Graphical Techniques</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#histograms"><i class="fa fa-check"></i><b>1.3.1</b> Histograms</a></li>
<li class="chapter" data-level="1.3.2" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#box-plots"><i class="fa fa-check"></i><b>1.3.2</b> Box-Plots</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#introduction-to-r"><i class="fa fa-check"></i><b>1.4</b> Introduction to R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html"><i class="fa fa-check"></i><b>2</b> Sampling Distributions Related to a Normal Population</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#normal-distribution"><i class="fa fa-check"></i><b>2.1</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.2" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#gamma-and-chi-square-distribution"><i class="fa fa-check"></i><b>2.2</b> Gamma and Chi-square Distribution</a></li>
<li class="chapter" data-level="2.3" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#students-t-distribution-and-f-distribution"><i class="fa fa-check"></i><b>2.3</b> Student’s t-Distribution and F-Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-central-limit-theorem.html"><a href="the-central-limit-theorem.html"><i class="fa fa-check"></i><b>3</b> The Central Limit Theorem</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sampling-distributions-related-to-a-normal-population" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Sampling Distributions Related to a Normal Population<a href="sampling-distributions-related-to-a-normal-population.html#sampling-distributions-related-to-a-normal-population" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Previously, we have introduced lots of definitions and given you a rough
idea about what really statistics it and what people do in statistics.
Now, we are going to proceed statistical distributions.</p>
<div id="normal-distribution" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Normal Distribution<a href="sampling-distributions-related-to-a-normal-population.html#normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In probability theory and statistics, normal distribution also called
Gaussian distribution which is discovered by a famous German
mathematician Johann Carl Friedrich Gauss in 1809. It is one of the most
important distribution that used to approximate other types of
probability distribution, such as binomial, hypergeometric, inverse (or
negative) hypergeometric, negative binomial and Poisson distribution.
Generally, it is denote as <span class="math inline">\(N(\mu, \sigma^2)\)</span> with probability density
function as the following:
<span class="math display">\[f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}}.\]</span>
Formally, let’s begin with its definition:</p>
<div class="definition">
<p><span id="def:unlabeled-div-23" class="definition"><strong>Definition 2.1  </strong></span>Suppose a random variable <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then
<span class="math inline">\(E(X) = \mu \text{ and } Var(X) = \sigma^2\)</span>. And
<span class="math inline">\(-\infty &lt; \mu &lt; \infty, \sigma^2 &gt; 0.\)</span> Moreover, <span class="math inline">\(X\)</span> has probability
density function as:
<span class="math display">\[f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}}, \text{ for $-\infty &lt; x &lt; \infty$ (same as above).}\]</span>
The only special case of normal distribution is standard normal
distribution, such that a random variable
<span class="math inline">\(Y \sim N( \mu = E(Y) = 0, \sigma^2 = Var(Y) = 1)\)</span>, then <span class="math inline">\(Y\)</span> has
probability density function as:
<span class="math display">\[f(y) = \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{y^2}{2}}.\]</span></p>
</div>
</div>
<div id="gamma-and-chi-square-distribution" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Gamma and Chi-square Distribution<a href="sampling-distributions-related-to-a-normal-population.html#gamma-and-chi-square-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Chi-square and Gamma distributions are two fundamental probability
distributions widely used in statistical theory and applications. The
Gamma distribution is a continuous distribution characterized by its
shape and scale parameters, making it versatile for modeling waiting
times and various positively skewed data. The Chi-square distribution, a
special case of the Gamma distribution, arises naturally in the context
of hypothesis testing and confidence interval estimation, especially in
tests involving variance and categorical data.<br />
<strong>Gamma Distribution</strong></p>
<div class="definition">
<p><span id="def:unlabeled-div-24" class="definition"><strong>Definition 2.2  </strong></span>Suppose a random variable <span class="math inline">\(X\)</span> is Gamma distributed with <span class="math inline">\(\alpha &gt; 0\)</span>
(shape parameter) and <span class="math inline">\(\beta &gt; 0\)</span> (scale parameter) if and only if the
probability density function of <span class="math inline">\(X\)</span> is
<span class="math display">\[f(x) = \frac{x^{\alpha - 1} e^{\frac{-x}{\beta}}}{\beta^{\alpha} \Gamma(\alpha)}, \text{ for $0 &lt; x &lt; \infty.$}\]</span>
Then,
<span class="math inline">\(E(X) = \alpha \beta \text{, } Var(X) = \alpha \beta^2 \text{ and its moment generating function is } M_{X}(t) = \frac{1}{(1 - \beta t)^{\alpha}},\)</span>
for <span class="math inline">\(t &lt; \frac{1}{\beta}.\)</span></p>
</div>
<p>Now, let’s introduce some properties of Gamma function:</p>
<ul>
<li><p>Gamma function (<strong>not a distribution</strong>):
<span class="math display">\[\Gamma(x) = \int_{0}^{\infty}t^{x-1}e^{-t}\,dt, \text{ for $x &gt; 0$.}\]</span></p></li>
<li><p>Properties</p>
<ul>
<li><p>1. <span class="math inline">\(\Gamma(x) = x \cdot \Gamma(x-1)\)</span>;</p></li>
<li><p>2. For all <span class="math inline">\(n \in \mathbb{N} \text{, } \Gamma(n) = (n - 1)!\)</span>;</p></li>
<li><p>3. <span class="math inline">\(\Gamma(\frac{1}{2}) = \sqrt{\pi}\)</span>.</p></li>
</ul></li>
</ul>
<p><strong>Chi-square Distribution</strong></p>
<p>Here is its formal definition:</p>
<div class="definition">
<p><span id="def:unlabeled-div-25" class="definition"><strong>Definition 2.3  </strong></span>A random variable <span class="math inline">\(X\)</span> has a Chi-squared distribution with <span class="math inline">\(n\)</span> degrees of
freedom <span class="math inline">\((\chi_{n}^{2})\)</span> if and only if <span class="math inline">\(X\)</span> is a random variable with a
Gamma distribution with parameters
<span class="math inline">\(\alpha = \frac{n}{2} \text{ and } \beta = 2.\)</span> Then, the probability
density function of <span class="math inline">\(X\)</span> is given by
<span class="math display">\[f(x) = \frac{1}{2^{\frac{n}{2}} \Gamma(\frac{n}{2})} x^{\frac{k}{2} - 1} e^{\frac{-x}{2}}.\]</span>
Moreover,
<span class="math inline">\(E(X) = n \text{, } Var(X) = 2n \text{ and moment generating function of $X$ is } M_{X}(t) = (1 - 2t)^{\frac{-n}{2}}, \text{ for $t &lt; \frac{1}{2}$}.\)</span></p>
</div>
<p>We claim that Chi-square distribution is a special case of Gamma
distribution with <span class="math inline">\(\alpha = \frac{n}{2} \text{ and } \beta = 2\)</span>. Now,
let’s prove it by using moment generating function.<br />
The proof is quite straightforward as the following shows:</p>
<div class="proof">
<p><span id="unlabeled-div-26" class="proof"><em>Proof</em>. </span><em>Proof.</em> Suppose
<span class="math inline">\(X \sim Gamma( \alpha = \frac{n}{2} \text{, } \beta = 2).\)</span><br />
Then the following moment generating function holds for <span class="math inline">\(X\)</span>:
<span class="math display">\[M_{X}(t) = (1- 2t)^{\frac{-n}{2}}, \text{ for $t &lt; \frac{1}{2}$}.\]</span>
Compare the moment generating function of <span class="math inline">\(X\)</span> under Gamma distribution
with Chi-square distribution, we can conclude that
<span class="math inline">\(X \sim \chi_{n}^{2}\)</span>. ◻</p>
</div>
<p><strong>Obtaining Chi-square Distribution by Normal Distribution</strong></p>
<p>Previously, we showed how to use Gamma distribution to get Chi-square
distribution by moment generating function method. Now, let’s do
something interestingly, to use normal distribution to get Chi-square
distribution. We will begin with a theorem, then prove it.</p>
<div class="thm">
<p>Suppose a random variable <span class="math inline">\(Z\)</span> is standard normally distributed, such
that <span class="math inline">\(Z \sim N(0 \text{, }1).\)</span> Then, <span class="math inline">\(Z^2\)</span> is Chi-square distributed
with <span class="math inline">\(1\)</span> degree of freedom, so that <span class="math inline">\(Z^2 \sim \chi_{1}^{2}\)</span>.</p>
</div>
<p>The proof of Theorem <span class="math inline">\(2.1\)</span> isn’t that trivial to see. We still need
moment generating function, but in a different way. Before we get into
the proper proof, let’s grab everything we need:</p>
<ul>
<li><p>1. Recall STA256 about how to get moment generating function for a
given continuous random variable that:
<span class="math display">\[M_{Z}(t) = \int_{-\infty}^{\infty} e^{tx}f_{X}(x)\,dx.\]</span></p></li>
<li><p>2. We also need Gaussian integral: $$</p>
<span class="math display">\[\begin{aligned}
                \int_{-\infty}^{\infty} e^{-x^2}\,dx &amp;= \sqrt{\pi}; \label{eq:gaussian1} \\
                \int_{-\infty}^{\infty} e^{-kx^2}\,dx &amp;= \sqrt{\frac{\pi}{k}}, \text{ for $k &gt; 0$}; \label{eq:gaussian2} \\
                \int_{-\infty}^{\infty} e^{kx^2}\,dx &amp;= \sqrt{\frac{\pi}{-k}}, \text{ for $k &lt; 0$}. \label{eq:gaussian3}

\end{aligned}\]</span>
<p>$$</p></li>
</ul>
<div class="proof">
<p><span id="unlabeled-div-27" class="proof"><em>Proof</em>. </span><em>Proof.</em> Suppose that <span class="math inline">\(Z \sim N(0, 1)\)</span>, then
<span class="math inline">\(f_{Z}(z) = \frac{1}{\sqrt{2\pi}} \cdot e^{\frac{-z^2}{2}}\)</span>.<br />
The moment generating function (MGF) of <span class="math inline">\(Z^2\)</span> is: <span class="math display">\[\begin{aligned}
M_{Z^2}(t) &amp;= \mathbb{E}\left(e^{tZ^2}\right) \\
&amp;= \int_{-\infty}^{\infty} e^{tz^2} f_Z(z) \, dz \\
&amp;= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{tz^2} e^{-\frac{z^2}{2}} \, dz \\
&amp;= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\left(\frac{1}{2}-t\right)z^2} \, dz.
\end{aligned}\]</span></p>
<p>Apply substitution with <span class="math inline">\(u = z\sqrt{\frac{1}{2}-t}\)</span>,
<span class="math inline">\(dz = \frac{du}{\sqrt{\frac{1}{2}-t}}\)</span>: <span class="math display">\[\begin{aligned}
M_{Z^2}(t) &amp;= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-u^2} \cdot \frac{1}{\sqrt{\frac{1}{2}-t}} \, du \\
&amp;= \frac{1}{\sqrt{2\pi}} \cdot \frac{1}{\sqrt{\frac{1}{2}-t}} \int_{-\infty}^{\infty} e^{-u^2} \, du \\
&amp;= \frac{1}{\sqrt{2\pi}} \cdot \frac{1}{\sqrt{\frac{1}{2}-t}} \cdot \sqrt{\pi} \\
&amp;= \frac{1}{\sqrt{1-2t}}.
\end{aligned}\]</span></p>
<p>This is the MGF of a chi-squared distribution with 1 degree of freedom,
<span class="math inline">\(Z^2 \sim \chi_1^2\)</span>. ◻</p>
</div>
<p>Now, we can do another proof by using Theorem <span class="math inline">\(2.1\)</span>.</p>
<div class="thm">
<p>Suppose <span class="math inline">\(Z_1, Z_2, ..., Z_n \overset{\text{i.i.d.}}{\sim} N(0,1)\)</span>, then
the sum of <span class="math inline">\(n\)</span> independent <span class="math inline">\(Z^2\)</span> is going to be Chi-square distributed
with <span class="math inline">\(n\)</span> degrees of freedom, as the following:
<span class="math display">\[\sum_{i=1}^{n}Z_{i}^{2} \sim \chi_{n}^{2}.\]</span></p>
</div>
<p>We need Theorem <span class="math inline">\(2.1\)</span> to prove this, but it going to be easier.</p>
<div class="proof">
<p><span id="unlabeled-div-28" class="proof"><em>Proof</em>. </span><em>Proof.</em> Suppose <span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span>, then its probability density
function is: <span class="math display">\[f_Z(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}.\]</span></p>
<p>Let <span class="math inline">\(\delta = \sum_{i=1}^{n} Z_i^2\)</span>, where
<span class="math inline">\(Z_1, \ldots, Z_n \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,1)\)</span>. The
moment generating function (MGF) of <span class="math inline">\(\delta\)</span> is: <span class="math display">\[\begin{aligned}
M_{\delta}(t) &amp;= \mathbb{E}\left[e^{t\delta}\right] \\
&amp;= \mathbb{E}\left[e^{t(Z_1^2 + \cdots + Z_n^2)}\right] \\
&amp;= \mathbb{E}\left[\prod_{i=1}^n e^{tZ_i^2}\right].
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(Z_1, \ldots, Z_n\)</span> are independent and identically distributed:
<span class="math display">\[\begin{aligned}
M_{\delta}(t) &amp;= \prod_{i=1}^n \mathbb{E}\left[e^{tZ_i^2}\right] \\
&amp;= \prod_{i=1}^n M_{Z_i^2}(t).
\end{aligned}\]</span></p>
<p>From Theorem 2.1, we know <span class="math inline">\(Z_i^2 \sim \chi_1^2\)</span> with MGF
<span class="math inline">\((1-2t)^{-1/2}\)</span>, therefore: <span class="math display">\[\begin{aligned}
M_{\delta}(t) &amp;= \prod_{i=1}^n (1-2t)^{-1/2} \\
&amp;= (1-2t)^{-n/2}.
\end{aligned}\]</span></p>
<p>This is exactly the MGF of a chi-squared distribution with <span class="math inline">\(n\)</span> degrees
of freedom, proving that <span class="math inline">\(\delta \sim \chi_n^2\)</span> as required. ◻</p>
</div>
<p>Here is the last theorem for Chi-square and normal distribution, but we
won’t show you the proof due to its complexity. For people who are
interested in that, please see STA260 lecture notes or power point slide
to figure out.</p>
<div class="thm">
<p>Let <span class="math inline">\(n\)</span> be sample size, <span class="math inline">\(s^2\)</span> be sample variance and <span class="math inline">\(\sigma^2\)</span> be
population variance, then <span class="math inline">\(\frac{(n-1)s^2}{\sigma^2}\)</span> is Chi-square
distributed with <span class="math inline">\(n - 1\)</span> degrees of freedom. As the following:
<span class="math display">\[\frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^{2}.\]</span></p>
</div>
</div>
<div id="students-t-distribution-and-f-distribution" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Student’s t-Distribution and F-Distribution<a href="sampling-distributions-related-to-a-normal-population.html#students-t-distribution-and-f-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The t-distribution and F-distribution are essential tools in inferential
statistics, particularly in the context of hypothesis testing and
variance analysis. The t-distribution, which resembles the normal
distribution but with heavier tails, is primarily used when estimating
population means in situations where the sample size is small and the
population standard deviation is unknown. On the other hand, the
F-distribution is used to compare variances between two populations and
plays a central role in analysis of variance (ANOVA) and regression
analysis.<br />
<strong>Student’s t-Distribution</strong></p>
<div class="definition">
<p><span id="def:unlabeled-div-29" class="definition"><strong>Definition 2.4  </strong></span>Suppose <span class="math inline">\(X\)</span> is t-distributed with <span class="math inline">\(n\)</span> degrees of freedom, then the
probability density function of <span class="math inline">\(X\)</span> is given by:
<span class="math display">\[f_{X}(x) = \frac{\Gamma(\frac{n+1}{2})}{\sqrt{\pi n} \Gamma(\frac{n}{2})} (1+\frac{x^2}{n})^{\frac{-n+1}{2}}.\]</span>
Alternatively, define a new variable <span class="math inline">\(T\)</span> is the following:
<span class="math display">\[T = \frac{W}{\sqrt{\frac{V}{r}}}, \text{ for $W \sim N(0, 1)  \text{ and } V \sim \chi_{r}^{2}$.}\]</span>
Or suppose
<span class="math inline">\(X_1, ..., X_n \overset{\text{i.i.d.}}{\sim} N(\mu, \text{ } \sigma^2)\)</span>,
then <span class="math inline">\(\bar{X} \sim N(\mu, \text{ } \frac{\sigma^2}{n})\)</span>. Thus,
<span class="math display">\[T = \frac{ \bar{x} - \mu}{(\frac{s}{\sqrt{n}})}.\]</span></p>
</div>
<p>Same as normal distribution, student’s t-distribution is also symmetric.
Also, as the degrees of freedom of t-distribution getting larger, the
curve of student’s t-distribution getting closer to standard normal
distribution.<br />
<strong>F-Distribution</strong></p>
<div class="definition">
<p><span id="def:unlabeled-div-30" class="definition"><strong>Definition 2.5  </strong></span>We define a new variable <span class="math inline">\(F\)</span> as the following shows:
<span class="math display">\[F = \frac{ (\frac{W_1}{v_1}) }{ (\frac{W_2}{v_2})} \sim F_{v_1, \text{ } v_2}; \text{ for $W_1 \sim \chi_{v_1}^{2}$ and $W_2 \sim \chi_{v_2}^{2}$; also both $W_1$ and $W_2$ are independent.}\]</span>
Alternatively, we select two samples (with same population variance)
with size <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span>, and also sample variance <span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span>
respectively. Then, F-distribution is:
<span class="math display">\[F = \frac{ [\frac{ (\frac{(n-1)}{\sigma^2}) s_x^2}{n-1}] }{ [\frac{ (\frac{(m-1)}{\sigma^2}) s_y^2}{m-1}] } \sim F_{n-1, \text{ } m-1 }.\]</span></p>
</div>
<p>Both student’s t-distribution and F-distribution are highly used in
inferential statistics, until confidence interval, testing hypothesis
and ANOVA analysis, these two distributions will come to play a lot. At
this point, just guarantee that you know how to obtain those
distribution from random given information is sufficient.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="descriptive-statistics-and-an-introduction-to-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-central-limit-theorem.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
