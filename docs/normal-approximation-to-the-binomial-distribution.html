<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Normal Approximation to the Binomial Distribution | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Normal Approximation to the Binomial Distribution | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Normal Approximation to the Binomial Distribution | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-central-limit-theorem.html"/>
<link rel="next" href="law-of-large-numbers.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>STA258: Statistics with Applied Probability</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#nishan-mudalige-nurlana-alili-bryan-xu"><i class="fa fa-check"></i>Nishan Mudalige, Nurlana Alili, Bryan Xu</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html"><i class="fa fa-check"></i><b>1</b> Descriptive Statistics and an Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#descriptive-statistics"><i class="fa fa-check"></i><b>1.2</b> Descriptive Statistics</a></li>
<li class="chapter" data-level="1.3" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#graphical-techniques"><i class="fa fa-check"></i><b>1.3</b> Graphical Techniques</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#histograms"><i class="fa fa-check"></i><b>1.3.1</b> Histograms</a></li>
<li class="chapter" data-level="1.3.2" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#box-plots"><i class="fa fa-check"></i><b>1.3.2</b> Box-Plots</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#introduction-to-r"><i class="fa fa-check"></i><b>1.4</b> Introduction to R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html"><i class="fa fa-check"></i><b>2</b> Sampling Distributions Related to a Normal Population</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#normal-distribution"><i class="fa fa-check"></i><b>2.1</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.2" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#gamma-and-chi-square-distribution"><i class="fa fa-check"></i><b>2.2</b> Gamma and Chi-square Distribution</a></li>
<li class="chapter" data-level="2.3" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#students-t-distribution-and-f-distribution"><i class="fa fa-check"></i><b>2.3</b> Student’s t-Distribution and F-Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-central-limit-theorem.html"><a href="the-central-limit-theorem.html"><i class="fa fa-check"></i><b>3</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="4" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html"><i class="fa fa-check"></i><b>4</b> Normal Approximation to the Binomial Distribution</a>
<ul>
<li class="chapter" data-level="4.1" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#bernoulli-distribution"><i class="fa fa-check"></i><b>4.2</b> Bernoulli Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#sampling-distribution-of-the-sum-and-mgf-derivation"><i class="fa fa-check"></i><b>4.3</b> Sampling Distribution of the Sum and MGF Derivation</a></li>
<li class="chapter" data-level="4.4" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#binomial-distribution"><i class="fa fa-check"></i><b>4.4</b> Binomial Distribution</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#visualizing-the-pmf-of-binomial-distributions"><i class="fa fa-check"></i><b>4.4.1</b> Visualizing the PMF of Binomial Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#sampling-distribution-of-a-sample-proportion-and-the-normal-approximation"><i class="fa fa-check"></i><b>4.5</b> Sampling Distribution of a Sample Proportion and the Normal Approximation</a>
<ul>
<li class="chapter" data-level="" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#conditions-for-using-the-normal-approximation"><i class="fa fa-check"></i>Conditions for Using the Normal Approximation</a></li>
<li class="chapter" data-level="" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#bernoulli-distribution-binomial-with-n-1"><i class="fa fa-check"></i>Bernoulli Distribution (Binomial with <span class="math inline">\(n = 1\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#normal-approximation-to-binomial"><i class="fa fa-check"></i><b>4.6</b> Normal Approximation to Binomial</a></li>
<li class="chapter" data-level="4.7" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#continuity-correction"><i class="fa fa-check"></i><b>4.7</b> Continuity Correction</a>
<ul>
<li class="chapter" data-level="" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#continuity-correction-table"><i class="fa fa-check"></i>Continuity Correction Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html"><i class="fa fa-check"></i><b>5</b> Law of Large Numbers</a>
<ul>
<li class="chapter" data-level="5.1" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#convergence-in-probability"><i class="fa fa-check"></i><b>5.1</b> Convergence in Probability</a></li>
<li class="chapter" data-level="5.2" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#weak-law-of-large-numbers-wlln"><i class="fa fa-check"></i><b>5.2</b> Weak Law of Large Numbers (WLLN)</a>
<ul>
<li class="chapter" data-level="" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#proof-of-the-weak-law-of-large-numbers-wlln"><i class="fa fa-check"></i>Proof of the Weak Law of Large Numbers (WLLN)</a></li>
<li class="chapter" data-level="" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#empirical-probability-insight"><i class="fa fa-check"></i>Empirical Probability Insight</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><i class="fa fa-check"></i><b>6</b> One Sample Confidence Intervals on a Mean When the Population Variance is Known</a>
<ul>
<li class="chapter" data-level="6.1" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#interpretation"><i class="fa fa-check"></i><b>6.2</b> Interpretation</a></li>
<li class="chapter" data-level="6.3" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#confidence-interval-for-mu-known-variance"><i class="fa fa-check"></i><b>6.3</b> Confidence Interval for <span class="math inline">\(\mu\)</span> (Known Variance)</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#confidence-interval-for-the-mean-of-a-normal-population"><i class="fa fa-check"></i>Confidence Interval for the Mean of a Normal Population</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#large-sample-ci-for-mu-normal-data"><i class="fa fa-check"></i>Large Sample CI for <span class="math inline">\(\mu\)</span> (Normal data)</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#one-sample-ci-on-the-population-mean-mu"><i class="fa fa-check"></i>One Sample CI on the Population Mean <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#table-of-common-z-values"><i class="fa fa-check"></i>Table of Common <span class="math inline">\(z\)</span>-values</a></li>
<li class="chapter" data-level="6.4" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#appendix"><i class="fa fa-check"></i><b>6.4</b> APPENDIX</a>
<ul>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#pivotal-quantities"><i class="fa fa-check"></i>Pivotal quantities</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><i class="fa fa-check"></i><b>7</b> One-Sample Confidence Intervals on a Mean When the Population Variance is Unknown</a>
<ul>
<li class="chapter" data-level="7.1" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#cis-for-mu"><i class="fa fa-check"></i><b>7.1</b> CIs for <span class="math inline">\(\mu\)</span></a>
<ul>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#independence-assumption"><i class="fa fa-check"></i>Independence Assumption</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#randomization-condition"><i class="fa fa-check"></i>Randomization Condition</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#standard-error"><i class="fa fa-check"></i>Standard Error</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#a-few-final-comments"><i class="fa fa-check"></i>A few final comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="one-sample-confidence-intervals-on-a-proportion.html"><a href="one-sample-confidence-intervals-on-a-proportion.html"><i class="fa fa-check"></i><b>8</b> One Sample Confidence Intervals On a Proportion</a></li>
<li class="chapter" data-level="9" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html"><i class="fa fa-check"></i><b>9</b> Sample Size Selection using Confidence Intervals</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#empirical-rule"><i class="fa fa-check"></i><b>9.0.1</b> Empirical Rule</a></li>
<li class="chapter" data-level="9.1" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#secSampleSizeCIMean"><i class="fa fa-check"></i><b>9.1</b> Calculating Sample Size for a Confidence Interval on a Mean</a>
<ul>
<li class="chapter" data-level="" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#when-sigma-is-known"><i class="fa fa-check"></i>When <span class="math inline">\(\sigma\)</span> is Known</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#calculating-sample-size-for-a-confidence-interval-on-a-proportion"><i class="fa fa-check"></i><b>9.2</b> Calculating Sample Size for a Confidence Interval on a Proportion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html"><i class="fa fa-check"></i><b>10</b> Two Sample Confidence Interval</a>
<ul>
<li class="chapter" data-level="10.1" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-a-difference-of-mean"><i class="fa fa-check"></i><b>10.1</b> Two Sample Confidence Interval on a Difference of Mean</a></li>
<li class="chapter" data-level="10.2" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-paired-data"><i class="fa fa-check"></i><b>10.2</b> Two Sample Confidence Interval on Paired Data</a></li>
<li class="chapter" data-level="10.3" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-proportions"><i class="fa fa-check"></i><b>10.3</b> Two Sample Confidence Interval on Proportions</a></li>
<li class="chapter" data-level="10.4" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-variances"><i class="fa fa-check"></i><b>10.4</b> Two Sample Confidence Interval on Variances</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html"><i class="fa fa-check"></i><b>11</b> Introduction to Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="11.1" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#test-of-hypothesis-for-one-mean"><i class="fa fa-check"></i><b>11.1</b> Test of Hypothesis for One Mean</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-1-decide-on-a-level-of-significance-alpha"><i class="fa fa-check"></i>Step 1: Decide on a Level of Significance (<span class="math inline">\(\alpha\)</span>)</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-2-state-the-null-hypothesis-h_0-and-the-alternative-hypothesis-h_a"><i class="fa fa-check"></i>Step 2: State the Null Hypothesis (<span class="math inline">\(H_0\)</span>) and the Alternative Hypothesis (<span class="math inline">\(H_a\)</span>)</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-3-calculate-an-appropriate-test-statistic"><i class="fa fa-check"></i>Step 3: Calculate an appropriate test statistic</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-4-calculate-the-p-value"><i class="fa fa-check"></i>Step 4: Calculate the p-value</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-5-compare-p-value-to-level-of-significance-alpha-and-make-a-conclusion"><i class="fa fa-check"></i>Step 5: Compare <em>p</em>-value to level of significance <span class="math inline">\(\alpha\)</span> and make a conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#test-of-hypothesis-for-one-proportion"><i class="fa fa-check"></i><b>11.2</b> Test of Hypothesis for One Proportion</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#introduction-to-hypothesis-testing-significance-test"><i class="fa fa-check"></i>Introduction to Hypothesis Testing (Significance Test)</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#steps-in-conducting-hypothesis-testing"><i class="fa fa-check"></i>Steps in conducting Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#some-additional-examples"><i class="fa fa-check"></i>Some Additional Examples</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#test-of-hypothesis-for-one-variance"><i class="fa fa-check"></i><b>11.3</b> Test of Hypothesis for One Variance</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="one-sample-hypothesis-test-on-a-proportion-and-variance.html"><a href="one-sample-hypothesis-test-on-a-proportion-and-variance.html"><i class="fa fa-check"></i><b>12</b> One Sample Hypothesis Test on a Proportion and Variance</a>
<ul>
<li class="chapter" data-level="12.1" data-path="one-sample-hypothesis-test-on-a-proportion-and-variance.html"><a href="one-sample-hypothesis-test-on-a-proportion-and-variance.html#one-sample-hypothesis-test-on-a-proportion"><i class="fa fa-check"></i><b>12.1</b> One Sample Hypothesis Test on a Proportion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="statistical-power.html"><a href="statistical-power.html"><i class="fa fa-check"></i><b>13</b> Statistical Power</a>
<ul>
<li class="chapter" data-level="13.1" data-path="statistical-power.html"><a href="statistical-power.html#statistical-power-1"><i class="fa fa-check"></i><b>13.1</b> Statistical Power</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-power.html"><a href="statistical-power.html#type-i-and-ii-errors"><i class="fa fa-check"></i>Type I and II Errors</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="statistical-power.html"><a href="statistical-power.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>13.2</b> Type I and Type II Errors</a></li>
<li class="chapter" data-level="13.3" data-path="statistical-power.html"><a href="statistical-power.html#using-power-to-determine-sample-size"><i class="fa fa-check"></i><b>13.3</b> Using Power to Determine Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html"><i class="fa fa-check"></i><b>14</b> Two Sample Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="14.1" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-means-with-independent-samples"><i class="fa fa-check"></i><b>14.1</b> Comparing Means with Independent Samples</a>
<ul>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#setting-up-hypotheses"><i class="fa fa-check"></i>Setting Up Hypotheses</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#structure-of-a-test-statistic"><i class="fa fa-check"></i>Structure of a Test Statistic</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#hypothesis-test-on-a-difference-of-means-mu_1---mu_2"><i class="fa fa-check"></i>Hypothesis Test on a Difference of Means (<span class="math inline">\(\mu_1 - \mu_2\)</span>)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-means-of-independent-samples-normal-population-assumptions"><i class="fa fa-check"></i>Comparing Means of Independent Samples (Normal Population Assumptions)</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-two-populations-means-independent-sampling-equal-variances-assumed"><i class="fa fa-check"></i><b>14.1.1</b> Comparing Two Populations Means: Independent Sampling (Equal Variances Assumed)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#small-sample-confidence-interval-for-mu_1---mu_2-with-equal-variances"><i class="fa fa-check"></i>Small-Sample Confidence Interval for <span class="math inline">\(\mu_1 - \mu_2\)</span> (with equal variances)</a></li>
<li class="chapter" data-level="14.1.2" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-two-populations-means-independent-sampling-unequal-variances-assumed"><i class="fa fa-check"></i><b>14.1.2</b> Comparing Two Populations Means: Independent Sampling (Unequal Variances Assumed)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#small-sample-confidence-interval-for-mu_1---mu_2-unequal-variances"><i class="fa fa-check"></i>Small-Sample Confidence Interval for <span class="math inline">\(\mu_1 - \mu_2\)</span> (Unequal Variances)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#degrees-of-freedom"><i class="fa fa-check"></i>Degrees of Freedom</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-t-test-unequal-variances"><i class="fa fa-check"></i>Two-Sample t-Test (Unequal Variances)</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#the-fold-rule"><i class="fa fa-check"></i><b>14.2</b> The Fold Rule</a></li>
<li class="chapter" data-level="14.3" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-hypothesis-test-on-paired-data"><i class="fa fa-check"></i><b>14.3</b> Two Sample Hypothesis Test on Paired Data</a></li>
<li class="chapter" data-level="14.4" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-hypothesis-test-on-proportions"><i class="fa fa-check"></i><b>14.4</b> Two Sample Hypothesis Test on Proportions</a></li>
<li class="chapter" data-level="14.5" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-hypothesis-test-on-variances"><i class="fa fa-check"></i><b>14.5</b> Two Sample Hypothesis Test on Variances</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="normal-approximation-to-the-binomial-distribution" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Normal Approximation to the Binomial Distribution<a href="normal-approximation-to-the-binomial-distribution.html#normal-approximation-to-the-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-2" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction<a href="normal-approximation-to-the-binomial-distribution.html#introduction-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-32" class="definition"><strong>Definition 4.1  </strong></span>A <em>statistic</em> is a function of the observable random variables in a
sample and known constants.</p>
<p>Since statistics are functions of the random variables observed in a
sample, they themselves are random variables. As such, all statistics
have a corresponding probability distribution, which we refer to as
their <em>sampling distribution</em>.</p>
</div>
<div class="tcolorbox">
<p><strong>Bernoulli Distribution:</strong></p>
<p>A Bernoulli trial is a single experiment with two outcomes:</p>
<ul>
<li><p>Success: <span class="math inline">\(X = 1\)</span> with probability <span class="math inline">\(p\)</span></p></li>
<li><p>Failure: <span class="math inline">\(X = 0\)</span> with probability <span class="math inline">\(1 - p\)</span></p></li>
</ul>
<div class="center">
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(X = x\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(P(X = x)\)</span></td>
<td align="center"><span class="math inline">\(1 - p\)</span></td>
<td align="center"><span class="math inline">\(p\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>The probability mass function (PMF) is:
<span class="math display">\[f(x) = p^x (1 - p)^{1 - x}, \quad x \in \{0, 1\}\]</span></p>
<p><strong>Binomial Distribution:</strong></p>
<p>A binomial distribution arises from <span class="math inline">\(n\)</span> independent Bernoulli trials.
Let: <span class="math display">\[X = \text{number of successes in } n \text{ trials}\]</span> Then:
<span class="math display">\[X \sim \text{Binomial}(n, p)\]</span></p>
<p>where:</p>
<ul>
<li><p>Each trial results in either success (with probability <span class="math inline">\(p\)</span>) or failure
(with probability <span class="math inline">\(1 - p\)</span>)</p></li>
<li><p><span class="math inline">\(X \in \{0, 1, \dots, n\}\)</span></p></li>
</ul>
<p>The PMF is: <span class="math display">\[P(X = x) = \binom{n}{x} p^x (1 - p)^{n - x}\]</span></p>
<p><strong>Moment Generating Function (MGF):</strong></p>
<p>The moment generating function (MGF) of a random variable <span class="math inline">\(X\)</span> is defined
as: <span class="math display">\[M_X(t) = \mathbb{E}[e^{tX}]\]</span> The MGF uniquely characterizes the
distribution of <span class="math inline">\(X\)</span> (if it exists in an open interval around 0), and it
can be used to compute moments such as the mean and variance.</p>
</div>
</div>
<div id="bernoulli-distribution" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Bernoulli Distribution<a href="normal-approximation-to-the-binomial-distribution.html#bernoulli-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bernoulli random variable is a discrete random variable that has exactly
two possible outcomes which are either a <strong>success</strong> or a <strong>failure</strong>.
An experiment in which there are exactly 2 outcomes (which are success
or failure) is called a <strong>Bernoulli trial</strong>.<br />
<br />
When <span class="math inline">\(x = 1\)</span> we have a success and when <span class="math inline">\(x = 0\)</span> we have a failure. The
term success and failure are relative to the problem being studied.</p>
<div class="tcolorbox">
<p>We chose to label a person who refuses to administer the worst shock a
“success” and all others as “failures”. However, we could just as easily
have reversed these labels. The mathematical framework we will build
does not depend on which outcome is labeled a success and which a
failure, as long as we are consistent.</p>
</div>
<p>Consider the random experiment of rolling a die once. Define the random
variable:</p>
<p><span class="math display">\[X_i =
\begin{cases}
1 &amp; \text{if the } i\text{-th roll is a six}, \\
0 &amp; \text{otherwise}
\end{cases}\]</span></p>
<p>Then <span class="math inline">\(X_i \sim \text{Bernoulli}(p)\)</span>, where
<span class="math inline">\(p = P(\text{rolling a six})\)</span>.</p>
<div class="tcolorbox">
<p><em>Let <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span>. The mass function of <span class="math inline">\(X\)</span> is</em></p>
<p><span class="math display">\[P(X = x) = p^x (1 - p)^{1 - x}, \quad x = 0, 1\]</span></p>
<p><em>where <span class="math inline">\(p\)</span> represents the probability of success.</em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-33" class="definition"><strong>Definition 4.2  </strong></span>Let <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span>. The mean of <span class="math inline">\(X\)</span> is <span class="math display">\[E(X) = \mu = p\]</span>
and the variance of <span class="math inline">\(X\)</span> is <span class="math display">\[\text{Var}(X) = \sigma^2 = p(1 - p)\]</span></p>
</div>
<p><em>To support the earlier result, we now provide a derivation of the mean,
variance, and standard deviation of a Bernoulli random variable.</em><br />
Let <span class="math inline">\(X\)</span> be a Bernoulli random variable with the probability of a success
as <span class="math inline">\(p\)</span>. Then</p>
<p><span class="math display">\[\begin{aligned}
E[X] = \mu &amp; = \sum_{i=1}^{n} x_i \cdot P(X = x_i) \\
&amp;= 0 \cdot P(X = 0) + 1 \cdot P(X = 1) \\
&amp;= 0 \cdot (1 - p) + 1 \cdot p \\
&amp;= p
\end{aligned}\]</span> Similarly, the variance of <span class="math inline">\(X\)</span> can be computed:</p>
<p><span class="math display">\[\begin{aligned}
V(X) = \sigma^2 &amp; = \sum_{i=1}^{k} (x_i - \mu)^2 \cdot P(X = x_i) \\
&amp;= (0 - p)^2 \cdot P(X = 0) + (1 - p)^2 \cdot P(X = 1) \\
&amp;= p^2 (1 - p) + (1 - p)^2 p \\
&amp;= p(1 - p)
\end{aligned}\]</span></p>
<p>The standard deviation is</p>
<p><span class="math display">\[\begin{aligned}
\sigma &amp; = \sqrt{\sigma^2} \\
            &amp; = \sqrt{p(1 - p)}
\end{aligned}\]</span></p>
</div>
<div id="sampling-distribution-of-the-sum-and-mgf-derivation" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Sampling Distribution of the Sum and MGF Derivation<a href="normal-approximation-to-the-binomial-distribution.html#sampling-distribution-of-the-sum-and-mgf-derivation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider determining the sampling distribution of the sample total:
<span class="math display">\[T_n = X_1 + X_2 + \dots + X_n\]</span> Suppose
<span class="math inline">\(X_i \overset{iid}{\sim} \text{Bernoulli}(p)\)</span>. Then the
moment-generating function of <span class="math inline">\(T_n\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
M_{T_n}(t) &amp;= \mathbb{E}[e^{t T_n}] \\
           &amp;= \mathbb{E}\left[e^{t(X_1 + X_2 + \dots + X_n)}\right] \\
           &amp;= \mathbb{E}\left[e^{tX_1} e^{tX_2} \dots e^{tX_n} \right] \quad \text{(independence)} \\
           &amp;= \mathbb{E}[e^{tX_1}] \cdot \mathbb{E}[e^{tX_2}] \cdots \mathbb{E}[e^{tX_n}] \\
           &amp;= M_{X_1}(t) \cdot M_{X_2}(t) \cdots M_{X_n}(t) \\
           &amp;= \left[pe^t + (1 - p)\right]^n
\end{aligned}\]</span></p>
<p>Since this is the MGF of a binomial random variable with parameters <span class="math inline">\(n\)</span>
and <span class="math inline">\(p\)</span>, we conclude:</p>
<p><span class="math display">\[T_n \sim \text{Binomial}(n, p)\]</span></p>
<div class="tcolorbox">
<p>We can think of rolling a die <span class="math inline">\(n\)</span> times as an example of the binomial
setting. Each roll gives either a six (a “success”) or a number
different from six (a “failure”).</p>
<p>Knowing the outcome of one roll doesn’t tell us anything about the
others, so the <span class="math inline">\(n\)</span> rolls are independent.</p>
<p>If we call a six a success, then:</p>
<ul>
<li><p>The probability of success on each trial is
<span class="math inline">\(p = P(\text{rolling a six}) = \frac{1}{6}\)</span></p></li>
<li><p>The probability of failure is <span class="math inline">\(1 - p = \frac{5}{6}\)</span></p></li>
</ul>
<p>Let <span class="math inline">\(Y\)</span> be the number of sixes rolled in <span class="math inline">\(n\)</span> trials. Then
<span class="math inline">\(Y \sim \text{Binomial}(n, p)\)</span>, and the distribution of <span class="math inline">\(Y\)</span> is called a
<strong>binomial distribution</strong>.</p>
</div>
</div>
<div id="binomial-distribution" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Binomial Distribution<a href="normal-approximation-to-the-binomial-distribution.html#binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In section 4.2 we learnt about Bernoulli random variables in which we
were interested in the outcome of just a single trial. A <strong>binomial
random variable</strong> is a generalization of several independent Bernoulli
trials. Instead of performing just a single Bernoulli trial and
observing whether we have a success or not, we are now performing
several Bernoulli trials and observing whether we have a certain number
of successes and failures. The <strong>binomial distribution</strong> describes the
probability of having exactly <span class="math inline">\(k\)</span> successes in <span class="math inline">\(n\)</span> independent Bernoulli
trials with probability of a success <span class="math inline">\(p\)</span>.<br />
</p>
<div class="tcolorbox">
<p><em>Let <span class="math inline">\(X \sim \text{Bin}(n, p)\)</span>. The probability of observing <span class="math inline">\(x\)</span>
successes in these <span class="math inline">\(n\)</span> independent trials is given by</em></p>
<p><span class="math display">\[P(X = x) = \binom{n}{x} p^x (1 - p)^{n - x}\]</span></p>
<p><em>where</em></p>
<ul>
<li><p><span class="math inline">\(n\)</span> represents the number of trials,</p></li>
<li><p><span class="math inline">\(x\)</span> represents the number of successes,</p></li>
<li><p><span class="math inline">\(p\)</span> represents the probability of success on any given trial,</p></li>
</ul>
<p><span class="math display">\[\binom{n}{x} = \frac{n!}{x!(n - x)!} \quad \text{is the binomial coefficient.}\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-34" class="definition"><strong>Definition 4.3  </strong></span>Let <span class="math inline">\(X \sim \text{Bin}(n, p)\)</span>. The mean of <span class="math inline">\(X\)</span> is <span class="math display">\[E(X) = \mu = np\]</span>
<em>and the variance of <span class="math inline">\(X\)</span> is</em> <span class="math display">\[\text{Var}(X) = \sigma^2 = np(1 - p)\]</span></p>
</div>
<div id="visualizing-the-pmf-of-binomial-distributions" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Visualizing the PMF of Binomial Distributions<a href="normal-approximation-to-the-binomial-distribution.html#visualizing-the-pmf-of-binomial-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>R code:</strong></p>
<div class="tcolorbox">
<pre><code>## Pmf of Binomial with n=10 and p=1/6.

x &lt;- seq(0, 10, by=1)
y &lt;- dbinom(x, 10, 1/6)

plot(x, y, type=&quot;p&quot;, col=&quot;blue&quot;, pch=19)</code></pre>
</div>
<p><strong>Probability Mass Functions (PMFs) for increasing <span class="math inline">\(n\)</span>:</strong></p>
<p>The following plots display the probability mass functions (PMFs) for a
binomial distribution with <span class="math inline">\(p = \frac{1}{6}\)</span> and increasing values of
<span class="math inline">\(n\)</span>. As <span class="math inline">\(n\)</span> increases, the binomial distribution begins to resemble a
normal distribution.</p>
<div class="float">
<embed src="Section4/images/pmf_plot.pdf" style="width:80.0%" />
<div class="figcaption">PMF of Binomial distribution with <span class="math inline">\(n = 10\)</span> and
<span class="math inline">\(p = \frac{1}{6}\)</span>.</div>
</div>
<div class="float">
<embed src="Section4/images/pmf_n50.pdf" style="width:80.0%" />
<div class="figcaption">PMF of Binomial distribution with <span class="math inline">\(n = 50\)</span> and
<span class="math inline">\(p = \frac{1}{6}\)</span>.</div>
</div>
<div class="float">
<embed src="Section4/images/pmf_n100.pdf" style="width:80.0%" />
<div class="figcaption">PMF of Binomial distribution with <span class="math inline">\(n = 100\)</span> and
<span class="math inline">\(p = \frac{1}{6}\)</span>.</div>
</div>
<div class="float">
<embed src="Section4/images/pmf_n300.pdf" style="width:80.0%" />
<div class="figcaption">PMF of Binomial distribution with <span class="math inline">\(n = 300\)</span> and
<span class="math inline">\(p = \frac{1}{6}\)</span>.</div>
</div>
</div>
</div>
<div id="sampling-distribution-of-a-sample-proportion-and-the-normal-approximation" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Sampling Distribution of a Sample Proportion and the Normal Approximation<a href="normal-approximation-to-the-binomial-distribution.html#sampling-distribution-of-a-sample-proportion-and-the-normal-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When studying categorical data, we are often interested not just in
individual outcomes, but in the proportion of successes observed in a
sample. Understanding how this proportion behaves across repeated
samples is crucial for making inferences about a population. In this
section, we explore the sampling distribution of a sample proportion and
how it can be approximated by a normal distribution under certain
conditions.<br />
</p>
<p>Draw a <em>Simple Random Sample (SRS)</em> of size <span class="math inline">\(n\)</span> from a large population
that contains proportion <span class="math inline">\(p\)</span> of “successes”. Let <span class="math inline">\(\hat{p}\)</span> be the
<strong><em>sample proportion</em></strong> of successes:</p>
<p><span class="math display">\[\hat{p} = \frac{\text{number of successes in the sample}}{n}\]</span></p>
<p>Then:</p>
<ul>
<li><p>The <strong>mean</strong> of the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(p\)</span>.</p></li>
<li><p>The <strong>standard deviation</strong> of the sampling distribution is
<span class="math inline">\(\sqrt{ \frac{p(1 - p)}{n} }\)</span>.</p></li>
</ul>
<div class="float">
<embed src="Section4/images/binomial_normal.pdf" style="width:90.0%" />
<div class="figcaption">Binomial distribution with <span class="math inline">\(n = 300\)</span>, <span class="math inline">\(p = \frac{1}{6}\)</span> and its Normal
approximation.</div>
</div>
<p>According to the Central Limit Theorem (CLT), the sampling distribution
of a sample proportion becomes approximately normal as the sample size
increases.</p>
<p>That is:
<span class="math display">\[\hat{p} \sim \mathcal{N}\left(p, \sqrt{\frac{p(1 - p)}{n}}\right)\]</span></p>
<p>This approximation is most accurate when both <span class="math inline">\(np \geq 10\)</span> and
<span class="math inline">\(n(1 - p) \geq 10\)</span>.</p>
<p>These are called the <strong>success-failure conditions</strong>.</p>
<p><em>Key Point:</em> When the success-failure conditions are met, the normal
approximation to the sampling distribution of <span class="math inline">\(\hat{p}\)</span> can be used for
probability calculations.</p>
<div id="conditions-for-using-the-normal-approximation" class="section level3 unnumbered hasAnchor">
<h3>Conditions for Using the Normal Approximation<a href="normal-approximation-to-the-binomial-distribution.html#conditions-for-using-the-normal-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose <span class="math inline">\(X \sim \text{Binomial}(n, p)\)</span>. Then:</p>
<p><span class="math display">\[\mu = np, \quad \sigma^2 = np(1 - p)\]</span></p>
<p><strong>Binomial probabilities can be approximated by the normal
distribution:</strong> <span class="math display">\[X \approx \mathcal{N}(np, \, np(1 - p))\]</span></p>
<p>This approximation is <em>useful for large <span class="math inline">\(n\)</span></em> and valid under the
following conditions:</p>
<div class="tcolorbox">
<p>The binomial setting holds (i.e., independent trials, fixed <span class="math inline">\(n\)</span>, same
probability <span class="math inline">\(p\)</span>) and</p>
<p><span class="math display">\[np \geq 10 \quad \text{and} \quad np(1 - p) \geq 10\]</span></p>
</div>
<p>Alternatively, a more conservative criterion for using the normal
approximation is:
<span class="math display">\[n &gt; 9 \cdot \left( \frac{\max(p, \, 1 - p)}{\min(p, \, 1 - p)} \right)\]</span></p>
<p>These ensure that the binomial distribution is sufficiently symmetric
and smooth to approximate with the normal distribution.</p>
<p>We derive the sampling distribution of <span class="math inline">\(\hat{p}\)</span> using properties of the
Bernoulli distribution.</p>
</div>
<div id="bernoulli-distribution-binomial-with-n-1" class="section level3 unnumbered hasAnchor">
<h3>Bernoulli Distribution (Binomial with <span class="math inline">\(n = 1\)</span>)<a href="normal-approximation-to-the-binomial-distribution.html#bernoulli-distribution-binomial-with-n-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[X_i =
\begin{cases}
1 &amp; \text{if the $i$-th roll is a six} \\
0 &amp; \text{otherwise}
\end{cases}\]</span></p>
<p><span class="math display">\[\mu = \mathbb{E}(X_i) = p, \quad \sigma^2 = \mathrm{Var}(X_i) = p(1 - p)\]</span></p>
<p>Let <span class="math inline">\(\hat{p}\)</span> be our estimate of <span class="math inline">\(p\)</span>. Note that
<span class="math inline">\(\hat{p} = \frac{1}{n} \sum_{i=1}^{n} X_i = \bar{X}\)</span>. Let
<span class="math inline">\(\hat{p} = \frac{\text{\# successes } (X)}{\text{sample size } (n)}\)</span></p>
<p>Recall that for <span class="math inline">\(X \sim \text{Binomial}(n, p)\)</span>:
<span class="math display">\[X \overset{\cdot}{\sim} \mathcal{N}(np, np(1 - p))\]</span></p>
<p>Let <span class="math inline">\(\hat{p} = \frac{X}{n}\)</span></p>
<div id="mean-of-hatp" class="section level5 unnumbered hasAnchor">
<h5>Mean of <span class="math inline">\(\hat{p}\)</span>:<a href="normal-approximation-to-the-binomial-distribution.html#mean-of-hatp" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[\mathbb{E}(\hat{p}) = \mathbb{E} \left( \frac{X}{n} \right) = \frac{1}{n} \cdot \mathbb{E}(X) = \frac{1}{n} \cdot np = p\]</span></p>
</div>
<div id="variance-of-hatp" class="section level5 unnumbered hasAnchor">
<h5>Variance of <span class="math inline">\(\hat{p}\)</span>:<a href="normal-approximation-to-the-binomial-distribution.html#variance-of-hatp" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[\mathrm{Var}(\hat{p}) = \mathrm{Var} \left( \frac{X}{n} \right) = \frac{1}{n^2} \cdot \mathrm{Var}(X) = \frac{1}{n^2} \cdot np(1 - p) = \frac{p(1 - p)}{n}\]</span></p>
<p>By the Central Limit Theorem (CLT), for sufficiently large <span class="math inline">\(n\)</span>:
<span class="math display">\[\hat{p} \sim \mathcal{N} \left( p, \frac{p(1 - p)}{n} \right)\]</span></p>
</div>
<div id="standardization-of-hatp" class="section level5 unnumbered hasAnchor">
<h5>Standardization of <span class="math inline">\(\hat{p}\)</span>:<a href="normal-approximation-to-the-binomial-distribution.html#standardization-of-hatp" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[Z = \frac{\hat{p} - p}{\sqrt{ \frac{p(1 - p)}{n} }}\]</span></p>
<p>If <span class="math inline">\(n\)</span> is large, then by the Central Limit Theorem:
<span class="math display">\[\bar{X} \approx \mathcal{N} \left( \mu, \frac{\sigma}{\sqrt{n}} \right)
\quad \Rightarrow \quad
\hat{p} \sim \mathcal{N} \left( p, \sqrt{\frac{p(1 - p)}{n}} \right)\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-35" class="example"><strong>Example 4.1  </strong></span>In the last election, a state representative received 52% of the votes
cast. One year after the election, the representative organized a survey
that asked a random sample of 300 people whether they would vote for him
in the next election. If we assume that his popularity has not changed,
what is the probability that more than half the sample would vote for
him?</p>
<div id="solution-1-using-normal-approximation" class="section level4 unnumbered hasAnchor">
<h4>Solution 1 (using Normal Approximation)<a href="normal-approximation-to-the-binomial-distribution.html#solution-1-using-normal-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We want to determine the probability that the sample proportion is
greater than <span class="math inline">\(50\%\)</span>. In other words, we want to find
<span class="math inline">\(P(\hat{p} &gt; 0.50)\)</span>.</p>
<p>We want to determine the probability that the sample proportion is
greater than 50%. In other words, we want to find <span class="math inline">\(P(\hat{p} &gt; 0.50)\)</span>.</p>
<p>Thus, we calculate <span class="math display">\[\begin{aligned}
P(\hat{p} &gt; 0.50) &amp; = P\left( \frac{\hat{p} - p}{\sqrt{p(1-p)/n}} &gt; \frac{0.50 - 0.52}{0.0288} \right) \\
&amp;= P(Z &gt; -0.69) = 1 - P(Z &lt; -0.69) \quad \text{(Z is symmetric)} \\
&amp;= P(Z &gt; -0.69) = 1 - P(Z &gt; 0.69) \\
&amp;= 1 - 0.2451 = 0.7549.
\end{aligned}\]</span></p>
<p>If we assume that the level of support remains at 52%, the probability
that more than half the sample of 300 people would vote for the
representative is 0.7549.</p>
<p><strong>R code (Normal Approximation):</strong></p>
<div class="tcolorbox">
<pre><code>1 - pnorm(0.50, mean = 0.52, sd = 0.0288)
## [1] 0.7562982</code></pre>
</div>
<p>Recall that, <code>pnorm</code> will give you the area to the left of 0.50, for a
Normal distribution with mean 0.52 and standard deviation 0.0288.</p>
</div>
<div id="solution-2-using-binomial" class="section level4 unnumbered hasAnchor">
<h4>Solution 2 (using Binomial)<a href="normal-approximation-to-the-binomial-distribution.html#solution-2-using-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We want to determine the probability that the sample proportion is
greater than 50%. In other words, we want to find <span class="math inline">\(P(\hat{p} &gt; 0.50)\)</span>.
We know that <span class="math inline">\(n = 300\)</span> and <span class="math inline">\(p = 0.52\)</span>.<br />
Thus, we calculate <span class="math display">\[\begin{aligned}
P(\hat{p} &gt; 0.50) &amp;= P\left(\frac{\sum_{i=1}^{n} x_i}{n} &gt; 0.50\right) \\
&amp;= P\left(\sum_{i=1}^{300} x_i &gt; 150\right) \\
&amp;= 1 - P\left(\sum_{i=1}^{300} x_i \leq 150\right) \\
&amp;\text{(it can be shown that } Y = \sum_{i=1}^{300} x_i \text{ has a Binomial distribution with} \\
&amp;n = 300 \text{ and } p = 0.52\text{)} \\
&amp;= 1 - F_Y(150)
\end{aligned}\]</span></p>
<p><strong>R code (using Binomial distribution):</strong></p>
<div class="tcolorbox">
<pre><code>1- pbinom(150, size = 300, prob = 0.52);
## [1] 0.7375949</code></pre>
</div>
<p>Recall that, <code>pbinom</code> will give you the CDF at 150, for a Binomial
distribution with <span class="math inline">\(n = 300\)</span> and <span class="math inline">\(p = 0.52\)</span>.</p>
</div>
<div id="solution-3-using-continuity-correction" class="section level3 unnumbered hasAnchor">
<h3>Solution 3 (using continuity correction)<a href="normal-approximation-to-the-binomial-distribution.html#solution-3-using-continuity-correction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have that <span class="math inline">\(n = 300\)</span> and <span class="math inline">\(p = 0.52\)</span>. Thus, we calculate
<span class="math display">\[\begin{aligned}
P(\hat{p} &gt; 0.50) &amp;= P\left( \frac{\sum_{i=1}^{n} x_i}{n} &gt; 0.50 \right) \\
&amp;= P\left( \sum_{i=1}^{300} x_i &gt; 150 \right) \\
&amp;= 1 - P\left( \sum_{i=1}^{300} x_i \leq 150 \right) \\
&amp;\text{(it can be shown that } Y = \sum_{i=1}^{300} x_i \text{ has a Binomial distribution with} \\
&amp;n = 300 \text{ and } p = 0.52\text{)}. \\
&amp;\approx 1 - P\left( \sum_{i=1}^{300} x_i \leq 150.5 \right) \quad \text{(continuity correction)} \\
&amp;= 1 - P\left( \frac{\sum_{i=1}^{300} x_i}{n} \leq \frac{150.5}{300} \right) \\
&amp;= 1 - P(\hat{p} \leq 0.5017) \\
&amp;= 1 - P\left( Z \leq -0.6354 \right) \quad \text{(Why?)}
\end{aligned}\]</span> <strong>R code (Normal approximation with continuity
correction):</strong></p>
<div class="tcolorbox">
<pre><code>1 - pnorm(0.5017, mean = 0.52, sd = 0.0288)
## [1] 0.7374216</code></pre>
</div>
<p>Recall that, <code>pnorm</code> will give you the area to the left of 0.5017, for a
Normal distribution with mean 0.52 and standard deviation 0.0288.</p>
</div>
</div>
</div>
</div>
</div>
<div id="normal-approximation-to-binomial" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Normal Approximation to Binomial<a href="normal-approximation-to-the-binomial-distribution.html#normal-approximation-to-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(X = \sum_{i=1}^{n} Y_i\)</span> where <span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span> are iid
Bernoulli random variables. Note that <span class="math inline">\(X = n\hat{p}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(n\hat{p}\)</span> is approximately Normally distributed provided that
<span class="math inline">\(np \geq 10\)</span> and <span class="math inline">\(n(1 - p) \geq 10\)</span>.</p></li>
<li><p>Another criterion is that the Normal approximation is adequate if
<span class="math display">\[n &gt; 9 \left( \frac{\text{larger of $p$ and $q$}}{\text{smaller of $p$ and $q$}} \right)\]</span></p></li>
<li><p>The expected value: <span class="math inline">\(E(\hat{p}) = np\)</span>.</p></li>
<li><p>The variance: <span class="math inline">\(V(\hat{p}) = np(1 - p) = npq\)</span>.</p></li>
</ol>
</div>
<div id="continuity-correction" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Continuity Correction<a href="normal-approximation-to-the-binomial-distribution.html#continuity-correction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The normal distribution is continuous, while the binomial distribution
is discrete. When we approximate a binomial probability using the normal
distribution, this mismatch can lead to inaccuracy—especially near the
boundaries of discrete values. A continuity correction improves the
approximation by adjusting for this difference. In this section, we
explore how and why this correction is applied.<br />
</p>
<div id="continuity-correction-table" class="section level3 unnumbered hasAnchor">
<h3>Continuity Correction Table<a href="normal-approximation-to-the-binomial-distribution.html#continuity-correction-table" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="center">
<p><strong>Binomial Probability</strong> <strong>Continuity Correction</strong> <strong>Normal Approximation</strong><br />
———————————— ———————————————— —————————————————————————————————– – –</p>
<p><span class="math inline">\(\displaystyle P(X = x)\)</span> <span class="math inline">\(\displaystyle P(x - 0.5 \leq X \leq x + 0.5)\)</span> <span class="math inline">\(\displaystyle P\left(\frac{x - 0.5 - \mu}{\sigma} \leq Z \leq \frac{x + 0.5 - \mu}{\sigma}\right)\)</span></p>
<p><span class="math inline">\(\displaystyle P(X \leq x)\)</span> <span class="math inline">\(\displaystyle P(X \leq x + 0.5)\)</span> <span class="math inline">\(\displaystyle P\left(Z \leq \frac{x + 0.5 - \mu}{\sigma}\right)\)</span></p>
<p><span class="math inline">\(\displaystyle P(X &lt; x)\)</span> <span class="math inline">\(\displaystyle P(X \leq x - 0.5)\)</span> <span class="math inline">\(\displaystyle P\left(Z \leq \frac{x - 0.5 - \mu}{\sigma}\right)\)</span></p>
<p><span class="math inline">\(\displaystyle P(X \geq x)\)</span> <span class="math inline">\(\displaystyle P(X \geq x - 0.5)\)</span> <span class="math inline">\(\displaystyle P\left(Z \geq \frac{x - 0.5 - \mu}{\sigma}\right)\)</span></p>
<p><span class="math inline">\(\displaystyle P(X &gt; x)\)</span> <span class="math inline">\(\displaystyle P(X \geq x + 0.5)\)</span> <span class="math inline">\(\displaystyle P\left(Z \geq \frac{x + 0.5 - \mu}{\sigma}\right)\)</span></p>
<p><span class="math inline">\(\displaystyle P(a \leq X \leq b)\)</span> <span class="math inline">\(\displaystyle P(a - 0.5 \leq X \leq b + 0.5)\)</span> <span class="math inline">\(\displaystyle P\left(\frac{a - 0.5 - \mu}{\sigma} \leq Z \leq \frac{b + 0.5 - \mu}{\sigma}\right)\)</span></p>
</div>
<p>Suppose that <span class="math inline">\(Y\)</span> has a Binomial distribution with <span class="math inline">\(n = 20\)</span> and
<span class="math inline">\(p = 0.4\)</span>. We will find the exact probabilities that <span class="math inline">\(Y \leq y\)</span> and
compare these to the corresponding values found by using two Normal
approximations. One of them, when <span class="math inline">\(X\)</span> is Normally distributed with
<span class="math inline">\(\mu_X = np\)</span> and <span class="math inline">\(\sigma_X = \sqrt{np(1 - p)}\)</span>. The other one, <span class="math inline">\(W\)</span>, a
shifted version of <span class="math inline">\(X\)</span>.</p>
<p>For example, <span class="math display">\[P(Y \leq 8) = 0.5955987\]</span></p>
<p>As previously stated, we can think of <span class="math inline">\(Y\)</span> as having approximately the
same distribution as <span class="math inline">\(X\)</span>. <span class="math display">\[P(Y \leq 8) \approx P(X \leq 8)
= P\left[ \frac{X - np}{\sqrt{np(1 - p)}} \leq \frac{8 - 8}{\sqrt{20(0.4)(0.6)}} \right]
= P(Z \leq 0) = 0.5\]</span></p>
<p><span class="math display">\[P(Y \leq 8) \approx P(W \leq 8.5)
= P\left[ \frac{W - np}{\sqrt{np(1 - p)}} \leq \frac{8.5 - 8}{\sqrt{20(0.4)(0.6)}} \right]
= P(Z \leq 0.2282) = 0.5902615\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-36" class="example"><strong>Example 4.2  </strong></span>Fifty-one percent of adults in the U. S. whose New Year’s resolution was
to exercise more achieved their resolution. You randomly select 65
adults in the U. S. whose resolution was to exercise more and ask each
if he or she achieved that resolution. What is the probability that
exactly forty of them respond yes?<br />
We are given that <span class="math inline">\(p = 0.51\)</span>, <span class="math inline">\(n = 65\)</span>, and we want to find <span class="math inline">\(P(X = 40)\)</span>
where <span class="math inline">\(X \sim Binomial(n = 65, p = 0.51)\)</span>.<br />
<strong>Use Normal Approximation</strong> We use normal approximation to the
binomial. First, compute the mean and standard deviation:
<span class="math display">\[\begin{aligned}
\mu &amp;= np = 65 \times 0.51 = 33.15 \\
\sigma^2 &amp;= np(1-p) = 65 \times 0.51 \times 0.49 = 16.485 \\
\sigma &amp;= \sqrt{16.485} \approx 4.06
\end{aligned}\]</span></p>
<p>We apply continuity correction: <span class="math display">\[P(X = 40) = P(39.5 \leq X \leq 40.5)\]</span></p>
<p><span class="math display">\[= P\left(\frac{39.5 - 33.15}{4.06} \leq Z \leq \frac{40.5 - 33.15}{4.06}\right) = P(1.56 \leq Z \leq 1.81)\]</span></p>
<p>From the standard normal table:
<span class="math display">\[= P(Z \leq 1.81) - P(Z \leq 1.56) = 0.0594 - 0.0352 = 0.0242\]</span></p>
<p>So the approximate probability is: <span class="math display">\[P(X = 40) \approx 0.0242\]</span></p>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-central-limit-theorem.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="law-of-large-numbers.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
