\setcounter{equation}{0}
\chapter{Random Variables III:  Sampling}
\label{sec.appli} \pagestyle{myheadings}
\markboth{\ref{sec.appli}. \titleref{sec.appli}}{}

\section{Introduction}

\subsection{What is Statistics}

%\frame{ \frametitle{What is Statistics?}

 \textbf{Statistics} is the science of using data to:
 \begin{itemize}
  \item describe characteristics of a group of objects;
  \item make inferences about the group.
 \end{itemize}
 \textbf{Operationally}, this involves:
 \begin{itemize}
  \item collection of data through various means;
  \item analysis and interpretation of the data.
 \end{itemize}

\PutAt<2-2>{(1.75cm,4cm)}{\NormalBox{\parbox[position]{8.5cm}{\begin{center}\Huge{Describe characteristics of a group of objects}\end{center}}}}
\PutAt<4-4>{(4cm,4cm)}{\NormalBox{\parbox[position]{4.5cm}{\begin{center} \Huge{Make group inferences}\end{center}}}}
\PutAt<6-6>{(3cm,4cm)}{\NormalBox{\parbox[position]{6.5cm}{\begin{center} \Huge{Methods of data collection}\end{center}}}}
\PutAt<8-8>{(3cm,4cm)}{\NormalBox{\parbox[position]{6.5cm}{\Huge{\begin{center}Data analysis and interpretation \end{center}}}}}
}
%________________________________________________________________________


\subsection{Goal of Statistics}

%\frame{ \frametitle{Goal of Statistics}%\pause

As part of the interpretation, we include levels of accuracy in terms of probability statement.%\pause
\begin{goal}<2->[Inference]
Is to produce estimators of the population parameters based on usually a smaller sample, and to
quantify the accuracy of these estimators through probabilities.
\end{goal}
}
%________________________________________________________________________


\subsection{Statistics and Business Decision making}

%\frame{ \frametitle{Statistics and Business Decision making}%\pause

Typically, for business, statistics is used for the purpose of informed decision
making that will lead to the most ``profitable" outcome.  Some examples incude:%\pause
\begin{itemize}
  \item data mining%\pause
  \item e-commerce%\pause
  \item forecasting%\pause
  \item investment analysis%\pause
  \item marketing%\pause
  \item pricing strategies%\pause
\end{itemize}

\PutAt<2-2>{(6cm,4.5cm)}{\NormalBox{\parbox[position]{5cm}{\begin{center}\Huge{Data mining}\end{center}}}}
\PutAt<3-3>{(6cm,4.5cm)}{\NormalBox{\parbox[position]{5cm}{\begin{center}\Huge{e-Commerce}\end{center}}}}
\PutAt<4-4>{(6.25cm,4.5cm)}{\NormalBox{\parbox[position]{4.5cm}{\begin{center}\Huge{Forecasting}\end{center}}}}
\PutAt<5-5>{(6.25cm,4.5cm)}{\NormalBox{\parbox[position]{4.5cm}{\begin{center}\Huge{Investment analysis}\end{center}}}}
\PutAt<6-6>{(6.5cm,4.5cm)}{\NormalBox{\parbox[position]{4cm}{\begin{center}\Huge{Marketing}\end{center}}}}
\PutAt<7-7>{(6.5cm,4.5cm)}{\NormalBox{\parbox[position]{4cm}{\begin{center}\Huge{Pricing strategies}\end{center}}}}
}
%________________________________________________________________________

\iffalse
\subsection{Objectives of this course}

\frame{ \frametitle{Objectives of this course}%\pause

The objectives of this course is to provide:%\pause
\begin{itemize}
\item<3-> logical and critical thinking; %\pause
\item<5-> good statistical skills. %\pause
\end{itemize}
For use in a business decision setting.

\PutAt<2-2>{(2.2cm,4.5cm)}{\NormalBox{\parbox[position]{8cm}{\begin{center}\Huge{Logical and Critical Thinking}\end{center}}}}
\PutAt<4-4>{(2.2cm,4.5cm)}{\NormalBox{\parbox[position]{8cm}{\begin{center}\Huge{Good Statistical Skills}\end{center}}}}
}
\fi
%________________________________________________________________________
































\iffalse
In this chapter we define three vector spaces associated with an
$m\times n$ matrix $A$: the row space, the column space and the
null space. Each of these has a significance in solving the linear
system $A{\bf x}={\bf b}$. We investigate this significance and
develop insight into the form of the solution to a system of
linear equations. Finally, we learn how to solve over-determined
systems of equations.

\noindent {\bf Learning Objectives}

\noindent After completing this chapter, you should be able to:
\begin{itemize}

\item find bases for the row space, the column space and the null
space of a matrix
\item find the rank and nullity of a matrix
\item understand the Dimension Theorem
\item understand the Characterization Theorem
\item find least squares solutions to over-determined systems of equations.
\end{itemize}

\section{Row space, Column space and Null space}
\label{ssec.rowspaceetc}\markright{\ref{ssec.rowspaceetc}
\titleref{ssec.rowspaceetc}}
\index{row space}\index{column space}\index{null space}
In this section we introduce three important vector spaces,
associated with a matrix $A$. These are the row space, the column
space and the null space of $A$.

Let
$$
A_{m\times n}=\left [
\begin{array}{cccc}a_{11}&a_{12}& \cdots & a_{1n}\\
                   a_{21}&a_{22}& \cdots &a_{2n}\\
                   \vdots&\vdots& \ddots & \vdots\\
                   a_{m1}& a_{m2}& \cdots & a_{mn}
                   \end{array} \right ].
$$
We call the vectors
\begin{eqnarray*}
{\bf r}_1&=& [a_{11} \ a_{12} \ \cdots \ a_{1n}]\\ {\bf
r}_2&=&[a_{21} \ a_{22} \ \cdots \ a_{2n} ]\\ \vdots && \ \ \ \
\vdots\\ {\bf r}_m&=&[a_{m1} \  a_{m2} \cdots a_{mn}]
\end{eqnarray*}
the {\bf row vectors}\index{row vector} of $A$. We call the vectors
$$
{\bf c}_1= \left [ \begin{array}{c} a_{11}\\a_{21}\\ \vdots \\
a_{m1} \end{array} \right ] ,\ {\bf c}_2=\left [
\begin{array}{c} a_{12}\\a_{22}\\ \vdots \\ a_{m2} \end{array}
\right ] ,\ldots\,, {\bf c}_n=\left [ \begin{array}{c}
a_{1n}
\\ a_{2n} \\ \vdots
\\ a_{mn} \end{array} \right ]
$$
the {\bf column vectors} of $A$.  Once the column vectors have
been identified, there is no need to write them as columns.  One
could write ${\bf c}_1$ as ${\bf c}_1 = (a_{11}, a_{21},
\ldots\,,a_{m1})$ if it were more convenient.

\begin{example}
\label{exam6.matrixvectors}  Find the row and column vectors of
the following matrix
$$
A=\left[ \begin{array}{rrrr}3&7&-2&9\\-1&0&2&0\\11&4&2&6 \end{array} \right].
$$
\indent The row vectors of $A$ are
$$
{\bf r}_1=[3 \ 7 \ -2 \ 9 ] \quad {\bf r}_2=[-1 \ 0 \ 2 \ 0 ] \quad
{\bf r}_3=[11 \ 4 \ 2 \ 6 ].
$$
\indent The column vectors of $A$ are
$$
{\bf c}_1=\left[ \begin{array}{r} 3\\-1\\11 \end{array} \right ] \quad {\bf c}_2=
\left [ \begin{array}{r} 7\\ 0\\ 4 \end{array} \right ] \quad {\bf
c}_3=\left [ \begin{array}{r} -2\\ 2\\ 2 \end{array} \right ]
\quad {\bf c}_4=\left [ \begin{array}{r} 9\\ 0\\ 6 \end{array}
\right ].
$$
\end{example}


Let $A$ be an $m \times n$ matrix.
\begin{enumerate}
\item The subspace of $\real{n}$ spanned by the
row vectors of $A$ is called the {\bf row space} of $A$.
\item The subspace of $\real{m}$ spanned by the column
vectors of $A$ is called the {\bf column space} of $A$.
\item The solution space of the homogeneous system $A{\bf x}={\bf
0}$, which is also a subspace of $\real{n}$, is called
the {\bf null space} of $A$. \end{enumerate}

Recall that the three elementary row operations
\begin{enumerate}
\item Multiply a row by a non-zero constant
\item Interchange two rows
\item Add multiplies of one row to another
\end{enumerate}
do not change the solution set of a linear system. Thus they do
not change the null space of a matrix (because the null space is
the solution to the system $A{\bf x}={\bf 0}$). A moment's thought
shows that these three operations do not change the row space of
the matrix either. They do however change the column space.

Let us illustrate these ideas by finding bases for the null space
and the row space of the matrix
$$A=\left [ \begin{array}{rrrrr}
                    1&2&0&4&0\\
                    -1&-1&1&-4&2\\
                    1&3&1&4&-5\\
                    0&5&5&0&-4 \end{array} \right ].$$
Using Gaussian reduction, we find the row echelon form of the
matrix $A$ is
$$R=\left [\begin{array}{rrrrr}
                    1&2&0&4&0\\
                    0&1&1&0&2\\
                    0&0&0&0&1\\
                    0&0&0&0&0 \end{array} \right ].$$
Since row reduction does not alter the null space, we find the
null space of $A$ by finding the null space of $R$.
Variables three and four are non-leading (free) variables, and we
find the null space by assigning these value. Let $x_3=s$,
$x_4=t$. Then
\begin{align*}
x_5&=0,& x_4&=t,& x_3 &= s,& x_2&=-s,
\end{align*}
and
\begin{align*}
x_1&=-4t-2x_2\\&=-4t+2s,
\end{align*}
is the solution. The null space is therefore
$$\{(-4t+2s,-s,s,t,0):\ s,\ t\ \epsilon\ \real{}\}.$$
A basis for the null space is obtained by
\begin{itemize}
    \item[(i)] letting $s=0$, $t=1$: $(-4,0,0,1,0)$
    \item[(ii)] letting $s=1$, $t=0$: $(2,-1,1,0,0)$
\end{itemize}
These two vectors clearly span the null space, and are necessarily
independent because of the way values have been assigned to $s$
and $t$. The dimension of the null space, is called the \textbf{nullity}\index{nullity} of $A$, and is of dimension 2 in this example.

This methodology applies in general. Once you have found the null
space of $A$, you find a basis by letting each free (non-leading)
variable equal 1 and the others equal 0. This means that the \textbf{nullity}
of $A$ is therefore the number of free variables. If there were
three free variables $r,\,s$ and $t$, then you would obtain the three
basis vectors for the null space by letting $r=1$, $s=0$, $t=0$,
then $r=0$, $s=1$, $t=0$, and finally $r=0$, $s=0$, $t=1$.

Since row reduction does not alter the row space, we can find a
basis for the row space of $A$ by finding a basis for the row
space of $R$. The three non-zero rows of $R$ are guaranteed to be
linearly independent, because of the form of the row-echelon
matrix. To see that $\{(1,2,0,4,0), (0,1,1,0,2), (0,0,0,0,1)\}$ is
independent, let
$$c_1(1,2,0,4,0)+c_2(0,1,1,0,2)+c_3(0,0,0,0,1)=(0,0,0,0,0)$$
The first coordinate gives
$$c_1\cdot1+c_2\cdot0+c_3\cdot0=0,~i.e.~c_1=0.$$
The second coordinate gives
$$c_1\cdot2+c_2\cdot1+c_3\cdot0=0,$$
and since $c_1=0$ it follows that $c_2=0$.
Finally, the fifth coordinate gives
$$c_1\cdot0+c_2\cdot2+c_3\cdot1=0.$$
Since $c_1 = c_2 = 0$ it follows that $c_3= 0$.
It follows that the vectors are independent, and hence a basis for
the space that they span, which is the row space.

This example illustrates the general principle that the non-zero
rows of $R$ are a basis for the row space of $A$. To see that the
non--zero rows of $R$ are independent, you consider the
coordinates corresponding to the leading $1's$ in $R$. In this
case the leading variables are variables $1$, $2$ and $5$, but the
method applies in general. It follows from this that the dimension
of the row space of $A$ is equal to the number of non-zero rows in
$R$, which is the same as the number of leading variables. This
number, the dimension of the row space of $A$, is called the {\bf
row rank} of $A$. {The row rank of $A$ is therefore the number of
leading variables}. In this particular example the row rank is 3.

Before leaving this example, notice that the row of zeros in $R$
means that one row of $A$ is a linear combination of the other
rows. In this case,
$$(0,5,5,0,-4)=(1,2,0,4,0)+3(-1,-1,1,-4,2)+2(1,3,1,4,-5).$$
In terms of equations, this means that the fourth equation is a
linear combination of the first three. The row rank therefore
gives the number of {independent} equations.

\begin{example}
\label{exam6.rowspace} Find a basis for the space spanned by the
vectors:
$$
{\bf v}_1=(1,3,0,-2,0),\ {\bf v}_2=(1,3,-1,-3,-2),\ {\bf v}_3=( 2,6,0,-3,-2),\ {\bf v}_4=(3,9,6,6,0).
$$
The space spanned by the four vectors is the same as the row space of the matrix formed
with the ${\bf v}_i$ as row vectors.
$$A=\left [
\begin{array}{rrrrr}
                    1&3&0&-2&0\\
                    1&3&-1&-3&-2\\
                    2&6 &0&-3&-2\\
                    3&9&6&6&0 \end{array} \right ].$$
Recall that elementary row operations do not change the row space
of the matrix. This matrix written in row-echelon form is:
$$R=\left [
\begin{array}{rrrrr}
                    1&3&0&-2&0\\
                    0&0&1&3&2\\
                    0&0&0&1&-2\\
                    0&0&0&0&0 \end{array} \right ].$$
The non-zero row vectors are: ${\bf w}_1=(1,3,0,-2,0),$ ${\bf
w}_2=(0,0,1,3,2)$ and ${\bf w}_3=(0,0,0,1,-2)$.  These three
vectors form a basis for the subspace of $\mbox{\tebbb R}^5$
spanned by ${\bf v}_1,{\bf v}_2,{\bf v}_3$ and ${\bf v}_4$ (that
is {\rm lin}$\{ {\bf v}_1,{\bf v}_2,{\bf v}_3,{\bf v}_4 \}$).
\end{example}

To find a basis for the column space of a matrix, transpose the
matrix and then find a basis for the row space,
$$\mathrm{column\ space}(A)=\mathrm{row\ space}(A^t).$$
The dimension of column space of $A$ is called the \textbf{column rank}\index{column rank} of
$A$. It is a fact that for any matrix $A$, the column rank equals
the row rank\index{row rank}. This number is called the \textbf{rank} of $A$,\index{rank}
\begin{eqnarray*}
\mathrm{rank}(A)&=&\mathrm{row\ rank}(A) \\ &=&\mathrm{column\ rank}(A).
\end{eqnarray*}
Recall theorem~\ref{eq3invert}.  Once again with the definitions of nullity and rank, more equivalent
statements can be added.
\begin{theorem}
\label{thm6.3}
Let $A$ be an $n \times n$ matrix.  The following statements are equivalent, i.e. if one of them is true, they are
all true, conversely if one of them is false they are all false.
\begin{enumerate}
\item $A$ is invertible.
\item $A{\bf{x}} = 0$ (the homogenous system) has only the trivial solution.
\item $A$ is row equivalent to $I_n$.
\item $A{\bf{x}} = b$ is consistent (and has exactly one solution) for every $n \times 1$ matrix ${\bf {b}}$.
\item ${\det{(A)}} \neq 0$.
\item The column vectors of $A$ are linearly independent.
\item The row vectors of $A$ are linearly independent.
\item The rank of $A$ is $n$.
\item The nullity of $A$ is 0.
\end{enumerate}
\end{theorem}
\noindent {\bf Activity \ref{ssec.rowspaceetc}:} For the following
matrix,
$$
\left [ \begin{array}{rrrr}-1&0&2&-1\\0&1&6&1\\-1&0&2&0 \end{array} \right ]
$$
\begin{enumerate}
\item [(a)] What are the row vectors?
\item [(b)] What are the column vectors?
\item [(c)] Find a basis for the row space.
\item [(d)] Find a basis for the nullspace.
\item [(e)] Find the rank and nullity of the matrix.
\end{enumerate}
\noindent Answers can be found in section \ref{answers6}

\section{Application to Systems of Equations}
\label{ssec.appsys}\markright{\ref{ssec.appsys}
\titleref{ssec.appsys}}

We now draw a conclusion from facts established in the previous
section.

As noted in the previous section, for any $m\times n$ matrix $A$ the nullity of $A$
is the number of free (non-leading) variables in the system of
equations $A{\bf x}={\bf 0}$. Similarly, the rank of $A$ is the
number of leading variables in $A{\bf x}={\bf 0}$. Since every
variable is either leading or free, we have
\begin{eqnarray*}
\mathrm{nullity}(A)+\mathrm{rank}(A)&=&\mathrm{number\ of\ variables} \\
&=&n \quad \mathrm{(the\ number\ of\ columns\ of\ A)}.
\end{eqnarray*}
This fact is sometimes called the \textbf{Dimension Theorem}\index{dimension theorem}.
\begin{example}
Verify the \textbf{Dimension Theorem} with the matrix:
$$A=\left [ \begin{array}{rrr}1&2&0\\ 4&8&1 \\-3&-6&-1 \end{array}
\right ].$$

\noindent Since $A$ is a $3 \times 3$ matrix $n=3$.

\noindent Row reduction on $A$ yields
$$
R=\left [ \begin{array}{rrr}1&2&0\\ 0&0&1 \\0&0&0 \end{array} \right ].
$$
The ${\rm rank}(A)$ is the dimension of the row space of $A$,
which is the same as the dimension of the row space of $R$.  The
dimension of the row space is 2 and therefore the rank is 2.

\noindent To find the nullity, solve:
$$
\left [ \begin{array}{rrr}1&2&0\\ 0&0&1 \\0&0&0 \end{array} \right ] \left [
\begin{array}{r}x_1 \\x_2\\x_3 \end{array} \right ]=
\left [ \begin{array}{r} 0\\0\\0 \end{array} \right ].
$$
Thus $x_{2}$ is a free variable.  If $x_{2}=s$ then $x_{1}=-2s$,
$x_{3}=0$ and the nullspace is
$$\{s(-2,1,0):s\ \epsilon\ \real{}\}.$$
A basis for the null space is
$$
\{(-2,1,0)\}
$$
and the ${\rm nullity}$ of $A$ is $1$.  Therefore
$$
{\rm rank}(A)+{\rm nullity}(A)=2+1=3=n.
$$
\end{example}

In terms of the equation $A{\bf x}={\bf 0}$, the nullity of $A$ is
the dimension of the solution space, while the rank of $A$ is the
number of non-zero rows in the row-echelon matrix $R$ (i.e. the
number of independent equations). We therefore have.
$\mathrm{dimension\ of\ solution\ space}+\mathrm{number\ of\ nonzero\ rows\ in\ }R=\mathrm{number\ of\ variables},$
or
$\mathrm{\textbf{dimension\ of\ solution\ space}}=\mathrm{\textbf{number\ of\ variables}}\\$ $-\mathrm{\textbf{number\ of\ nonzero\ rows\ in}\ }R.$

\subsection{The System $A{\bf x}={\bf b}$} \label{sssec.sys}

Suppose ${\bf x}_0$ is a solution to $A{\bf x}={\bf b}$ so that $A{\bf
x}_0={\bf {b}}$. If ${\bf x}_1$ is any other solution then
$$
A({\bf x}_1-{\bf x}_0)=A{\bf x}_1-A{\bf x}_0={\bf b}-{\bf
b}={\bf 0}.
$$
Thus ${\bf x}_1-{\bf x}_0$ is in the null space of
$A$. If ${\bf u}_1,\ldots,{\bf u}_r$ is a basis of the null space,
then there are scalars $c_1,\ldots,c_r$ such that
$$
{\bf x}_1-{\bf x}_0=c_1{\bf u}_1+c_2{\bf u}_2+\cdots+c_r{\bf
u}_r,
$$
$$
{\bf x}_1={\bf x}_0+c_1{\bf u}_1+c_2{\bf
u}_2+\ldots+c_r{\bf u}_r.
$$
Since ${\bf x}_1$ is {any} solution to $A{\bf x}={\bf b}$, it
follows that the solution set can be written
$$
\{{\bf x}_0+c_1{\bf u}_1+c_2{\bf u}_2+\cdots+c_r{\bf u}_r:\ c_1,\,c_2,\,\ldots,\,c_r\ \epsilon\ \real{} \},$$
where ${\bf x}_0$ is a particular solution and $\{ {\bf u}_1,{\bf
u}_2,\cdots,{\bf u}_r \}$ is a basis for the null space of $A$.
This fact is sometimes called the \textbf{Representation Theorem}\index{Representation Theorem}. It gives
a useful way of checking your solution to $A{\bf x}={\bf b}$: the
${\bf x}_0$ must satisfy $A{\bf x}_0={\bf b}$ and the ${\bf u}_1,{\bf
u}_2,\ldots,{\bf u}_r$ must satisfy $A{\bf u}_i={\bf 0}$.

\begin{example}
\label{exam6.character} Find a particular solution and the general
solution of the following system, and use it to find the basis for
the null space.
\begin{align*}
x_1 +3x_2-2x_3+2x_5&=0\\
2x_1+6x_2-5x_3-2x_4+4x_5-3x_6&=-1 \\
5x_3+10x_4+15x_6&=5\\ 2x_1+6x_2+8x_4+4x_5+18x_6&=6.
\end{align*}
The augmented matrix for the system reduces to reduced row echelon form as follows:
\begin{align*}
\left[ \begin{array}{rrrrrrcr} 1 & 3 & -2 & 0 & 2 & 0 & \vline & 0 \\
2 & 6 & -5 & -2 & 4 & -3 & \vline & -1 \\
0 & 0 & 5 & 10 & 0 & 15 & \vline & 5 \\
2 & 6 & 0 & 8 & 4 & 18 & \vline & 6
\end{array} \right] \leadsto
\left[ \begin{array}{rrrrrrcr} 1 & 3 & 0 & 4 & 2 & 0 & \vline & 0 \\
0 & 0 & 1 & 2 & 0 & 0 & \vline & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & \vline & \frac{1}{3} \\
0 & 0 & 0 & 0 & 0 & 0 & \vline & 0
\end{array} \right ].
\end{align*}
The non--leading variables are $x_2 = r$, $x_4 = s$, $x_5 = t$, and the solution to the
system is
$$
x_6 = \tfrac{1}{3},\ x_5=t,\ x_4 = s,\ x_3 = -2s,\ x_2 = r,\ x_1 = -3r - 4s - 2t.
$$
The solution set is
$$
\bigl\{(-3r-4s-2t,\ r,\ -2s,\ s,\ t,\ \tfrac{1}{3});\ r,\ s,\ t\ \epsilon\ \real{} \bigr\}.
$$
This can be written as
$
\bigl\{ (0, 0, 0, 0, 0, \tfrac{1}{3}) + r(-3, 1, 0, 0, 0, 0) + s(-4, 0, -2, 1, 0, 0) + t(-2, 0, 0, 0, 1, 0);\ r,\ s,\ t\ \epsilon\ \real{} \bigr\}.
$
Thus $(0, 0, 0, 0, 0, \tfrac{1}{3})$ is a particular solution, and the vectors
$$
\{(-3, 1, 0, 0, 0, 0),(-4, 0, -2, 1, 0, 0), (-2, 0, 0, 0, 1, 0)\}
$$
are a basis for the nullspace.
\end{example}

\section{Overdetermined Systems}
\label{ssec.oversys} \markright{\ref{ssec.oversys}
\titleref{ssec.oversys}}
\index{overdetermined systems}
The matrix $A$ can be represented as either
$A=\begin{bmatrix} {\bf c}_1 & {\bf c}_2 & \ldots & {\bf c}_n \end{bmatrix}$
or as
$$
A=\begin{bmatrix} {\bf r}_1 \\ {\bf r}_2 \\ \vdots \\ {\bf r}_m \end{bmatrix}.
$$
Similarly, if ${\bf x}=\left [ \begin{array}{c} x_1\\ x_2\\ \vdots
\\ x_n \end{array} \right ]$, then the product $A{\bf x}$ can be
written as follows:
$$
A{\bf x} = \begin{bmatrix} {\bf c}_1 & {\bf c}_2 & \ldots & {\bf c}_n \end{bmatrix}
\begin{bmatrix} x_1\\ x_2\\ \vdots \\ x_n \end{bmatrix}= x_1{\bf c}_1+x_2{\bf c}_2 +\cdots +x_n{\bf c}_n .
$$
Therefore, the linear system can be written as a linear
combination of the column vectors:
$$
{\bf b}=A{\bf x}=x_1{\bf c}_1+x_2{\bf c}_2 +\cdots +x_n{\bf c}_n
$$
and we can conclude that $A{\bf x}={\bf b}$ is consistent if and only if {\bf b} can be
expressed as a linear combination of the column vectors. This of
course is the same as requiring that  {\bf b} be in the column
space of $A$.

If ${\bf {b}}$ is not in the column space of $A$ (so the system is inconsistent), is there
any way we can approximate a solution?  Often a system is inconsistent when there are more equations
than unknowns.  In such cases we say a system is {\textbf{overdetermined}} and we obtain an approximate
solution as follows.


\begin{center}
\begin{picture}(110,110)(-50,-50)
\put(0,0){\vector(1,0){30}} \put(30,0){\dashbox{.6}(10,0)}
\put(30,0){\dashbox{.6}(0,40)} \put(30,0){\dashbox{.3}(4,4)}
\put(15,3){${\bf b}'$} \put(13,28){${\bf b}$}
\put(0,0){\vector(2,-1){50}} \put(0,0){\vector(1,1){50}}
\put(0,0){\vector(3,4){30}} \put(40,10){column space of $A$}
\end{picture}
\end{center}


$A{\bf{x}} = {\bf {b}}$ is inconsistent because ${\bf{b}}$ is not
in the column space of $A$. We obtain an approximate solution  by
replacing ${\bf {b}}$ by ${\bf{b}}'$, the point nearest ${\bf{b}}$
that is in the column space of $A$.  The point ${\bf {b}}'$ is the
orthogonal projection of ${\bf{b}}$ onto the column space of $A$.
Hence there exits ${\bf{x}}^*$ such that $A{\bf{x}}^* = {\bf
{b}}'$, and this is the best possible approximation to a solution
to $A{\bf x}={\bf b}$ because
$$
\| A{\bf{x}}^* - {\bf{b}} \| = \| {\bf{b}}' - {\bf{b}}\|
$$
is as small as possible.  This is called the \textbf{least squares solution}\index{least squares}.

To find ${\bf{x}}^*$, note that the vector $({\bf{b}}' - {\bf {b}})$ is orthogonal to the column space of $A$.
Hence the dot product of the column vectors $A{\bf{x}}$ and $({\bf{b}} - {\bf{b}}')$ is zero $ \forall\ {\bf{x}}$.
Writing the dot product in terms of matrix multiplication
\begin{align*}
(A{\bf{x}})^{t}({\bf{b}} - {\bf{b}}') &= 0 \quad \forall {\bf{x}},
\intertext{that is}
(A{\bf{x}})^{t}({\bf{b}} - A{\bf{x}}^*) &= 0 \quad \forall {\bf{x}}.
\intertext{Rearranging yields}
{\bf{x}}^t(A^t{\bf{b}} - A^tA{\bf{x}}^*) &=0 \quad \forall {\bf{x}}.
\end{align*}
Since this is true $\forall {\bf{x}}$,
\begin{align*}
A^t{\bf{b}} = A^tA{\bf{x}}^*
\intertext{and so}
{\bf{x}}^* = (A^tA)^{-1}A^t{\bf{b}},
\end{align*}
provided that $(A^tA)$ is invertible.

\begin{example}\label{exam7.lssol}
Find the least squares solution to $A{\bf{x}} = {\bf{b}}$ when
\begin{align*}
A &= \begin{bmatrix} 1 & 2 & 1\\2 & 3 & 4\\1 & 0 & 1\\1 & 1 & 1 \end{bmatrix}& \mathrm{and}&
&{\bf{b}} &= \left[ \begin{array}{r} 1 \\ 2\\-1\\0 \end{array} \right].
\end{align*}
\textrm{\textbf{Solution:}}\
$\|A{\bf{x}} - {\bf{b}}\|$ is minimized when $\hat{{\bf{x}}} = (A^tA)^{-1}A^t{\bf{b}}$, and this is the
least squares solution to $A{\bf{x}} = {\bf{b}}$. Hence,
\begin{align*}
\hat{{\bf{x}}} &= \left( \begin{bmatrix} 1 & 2 & 1 & 1 \\ 2 & 3 & 0 & 1 \\ 1 & 4 & 1 & 1 \end{bmatrix}
\begin{bmatrix} 1 & 2 & 1 \\ 2 & 3 & 4 \\ 1 & 0 & 1 \\ 1 & 1 & 1 \end{bmatrix} \right)^{-1}
\begin{bmatrix} 1 & 2 & 1 & 1 \\ 2 & 3 & 0 & 1 \\ 1 & 4 & 1 & 1 \end{bmatrix}
\begin{bmatrix}1 & 2 & -1 & 0 \end{bmatrix} \\
&= \frac{1}{24} \left[ \begin{array}{rrr}  41 & -6 & -19 \\ -6 & 12 & -6 \\ -19 & -6 & 17 \end{array} \right]
\begin{bmatrix} 1 & 2 & 1 & 1 \\ 2 & 3 & 0 & 1 \\ 1 & 4 & 1 & 1 \end{bmatrix}
\begin{bmatrix}1 & 2 & -1 & 0 \end{bmatrix} \\
&= \left[ \begin{array}{r} \vspace{1mm} -\tfrac{3}{2} \\ \vspace{1mm} 1 \\ \tfrac{1}{2} \end{array} \right].
\end{align*}
\end{example}

\section{Fitting Polynomials} \label{ssec.fitpoly} \markright{\ref{ssec.fitpoly} \titleref{ssec.fitpoly}}

An example of an overdetermined system is when we are given $n$ points in the plane, $(x_1,y_1),\,(x_2,y_2),\,\ldots\,(x_n,y_n)$, and
we would like to find the degree of $m$ polynomial $(m \leq n-1)$ that ''best fits'' the points.  If the polynomial is
\begin{align*}
y = a_0 + a_1x + a_2x^2 + \cdots + a_mx^m
\end{align*}
then all the points would lie on the polynomial if
\begin{align*}
y_1 &= a_0 + a_1x_1 + a_2x_1^2 + \cdots + a_mx_1^m \\
y_2 &= a_0 + a_1x_2 + a_2x_2^2 + \cdots + a_mx_2^m \\
&\vdots\\
y_n &= a_0 + a_1x_n + a_2x_n^2 + \cdots + a_mx_n^m \\
\end{align*}
that is, if
$$
\begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} =
\begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^m \\
1 & x_2 & x_2^2 & \cdots & x_2^m \\
\ &\ &\ &\vdots &\ \\
1 & x_n & x_n^2 & \cdots & x_n^m  \end{bmatrix}
\begin{bmatrix} a_0 \\ a_1 \\ \vdots \\ a_m \end{bmatrix}\ \mathrm{i.e\ }{\bf{y}} = M{\bf{a}}.
$$
Since all the points will probably not lie on the polynomial, there will most likely not be a solution to the equation
$$
M{\bf{a}} = {\bf{y}},
$$
so we seek ${\bf {a}}^*$ such that
$$
\|{\bf {y}} - M{\bf {a}}^*\|
$$
is a minimum.  This is given by
$$
{\bf{a}}^* = (M^tM)^{-1}M^t{\bf{y}}.
$$
The polynomial
$$
y = a_0^* + a_1^*x + a_1^*x^2 + \cdots + a_m^*x^m,
$$
has the property that
$$
\sum_{i=1}^{n}{\bigl(y_i - (a_0^* + a_1^*x_i + a_1^*x_i^2 + \cdots + a_m^*x_i^m)\bigr)^2}
$$
is a minimum over all choices of the coefficients $a_0,\,a_1,\ldots\,,a_m$.

\begin{example} \label{exam.fitpoly}
The quadratic equation $y = a_0 + a_1x + a_2x^2$ that best fits the four points $(1, 7.9),$ $(2, 15.1),\ (3, 24.1),\ (4, 34.8)$ is found by forming
$$
y = \begin{bmatrix} 7.9 \\ 15.1 \\ 24.1 \\ 34.8 \end{bmatrix} \quad
\mathrm{and} \quad M = \begin{bmatrix} 1 & 1 & 1 \\1 & 2 & 4 \\ 1 & 3 & 9\\1 & 4 & 16 \end{bmatrix},
$$
and then computing
$$
M^tM = \begin{bmatrix} 4 & 10 & 30 \\ 10 & 30 & 100 \\ 30 & 100 & 354 \end{bmatrix} \quad
\mathrm{and} \quad (M^tM)^{-1} = \left[ \begin{array}{rrr}\vspace{1mm} \frac{31}{4} & -\frac{27}{4} & \frac{5}{4} \\
\vspace{1mm} -\frac{27}{4} & \frac{129}{20} & -\frac{5}{4} \\ \vspace{1mm} \frac{5}{4} & -\frac{5}{4} & \frac{1}{4} \end{array} \right]
$$
Finally we calculate $a_i^*$ to be
$$
(M^tM)^{-1}M^t{\bf{y}} = \begin{bmatrix} 2.425 \\4.595 \\0.875 \end{bmatrix}
$$
Thus the quadratic equation best fitting the points is
$$
y = 2.425 + 4.595x + 0.875x^2.
$$
\end{example}

\section{Summary} \label{ssec.sum6}\markright{\ref{ssec.sum6} \titleref{ssec.sum6}}

{\bf Keywords: column rank; column space; column vectors; least
squares solution; nullity; null space; overdetermined system;
rank; row rank; row space; row vectors. }
\\ \\
In this chapter we studied the three fundamental vector spaces associated with a matrix:
the row space, the column space and the nullspace.  We saw how these vector spaces relate
to the solution of systems of equations.  In particular the nullspace of $A$ is the solution
to the homogenous system $A{\bf{x}} = {\bf{0}}$, and the Characterization Theorem tells us that
the solution set to $A{\bf{x}} = {\bf{b}}$ consists of a particular solution plus all vectors in the nullspace of
$A$.  The rank of a matrix is the dimension of the row space (or column space) and the nullity of a matrix
is the dimension of the nullspace.  The Dimension Theorem states that $\mathrm{rank}(A) + \mathrm{nullity}(A) =
\mathrm{number\ of\ columns\ of}\ A$. Finally we learned how to find least squares solutions to overdetermined systems
of equations.  The arithmetic required to find least squares solutions can be extensive, but the ideas are very simple.
\input{exercise6a.tex}
%\end{document}
\fi