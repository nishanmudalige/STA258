[["index.html", "Introduction", " Introduction Text goes here. "],["descriptive-statistics-and-an-introduction-to-r.html", "Chapter 1 Descriptive Statistics and an Introduction to R 1.1 Introduction 1.2 Descriptive Statistics 1.3 Graphical Techniques 1.4 Introduction to R", " Chapter 1 Descriptive Statistics and an Introduction to R 1.1 Introduction Intuitively, statistics can be considered the science of uncertainty. Formally, Definition 1.1 Statistics is the science of collecting, classifying, summarizing, analyzing and interpreting data. Population, Sample, Parameter In statistics, researchers need to observe behavior, pattern, trends and other types of data to give a conclusion. To make the conclusion more persuasive, researchers require huge amount of data to support them, that’s why study statistics need population. Definition 1.2 In statistics, a population is a set of similar observations which is of interest for some experimental questions. It can be a set of existing objects such as all people in Canada, or hypothetical group of existing objects such as the set of all possible hands in a game of poker. However, data collection from population is a lot work. Usually, researchers select a finite number of observations to study. Definition 1.3 It refers to a selection of a subset from population that researchers use it to estimate population characteristics. Now, we have already chosen a sample, but how do we use it to estimate population characteristics? This is the point where parameter comes to play. Definition 1.4 A parameter is a quantity of statistical population which summerizes characteristics of the population. For example, mean, variance and standard deviation. Descriptive and Inferential Statistics Now, we have set everything we need. A population, a chosen sample in that population with its parameters. Next step is studying. There are two major types of analysis: descriptive and Inferential statistics. In this section, we are only going to give you a rough idea about what they are, more detailed materials will be introduced in later chapters. Definition 1.5 It refers to the summation of all quantitive values that describe characteristics of the population. Usually, we use descriptive statistics to summerize characteristics of a data set. Furthermore, we use inferential statistics to do statistical analysis. Definition 1.6 It refers to the process of using data analysis to indicate properties of a population. For example, testing hypothesis and confidence interval (both will be introduced in later chapters). Qualitative and Quantitative Data At this point, assume that we have finished all procedures such as obtaining parameters and analyzing properties. Now, another important thing is illustrating all the discovery. Definition 1.7 This type of illustration refers to showing categorical data. For example, lecture notes from a course, open-question survey. To illustrate numerical data, we use quantitative data. Definition 1.8 Unless the previous type of illustration, quantitative data is represented numerically, including anything that can be counted, measured, or given a numerical value. For example, STA258 final mark score range from 100 different students who have taken this course. 1.2 Descriptive Statistics Previously, we defined descriptive statistics. Now, let’s introduce what exact they are. Sample Mean, Variance and Standard Deviation Sample mean (or sample average) is the average value of a sample which is selected from an interested population of an experiment. Usually, the sample mean is used to estimate population mean. In other words, we say that the sample mean is an estimator of population mean. Definition 1.9 Let \\(x_1, x_2, x_3, ..., x_n\\) be a sample of data points. We define sample mean of the sample data points (\\(\\bar{x}\\)) as the following: \\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i.\\] Also, we define sample variance of the sample data points (\\(s^2\\)) as: \\[s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n}(x_i - \\bar{x})^2.\\] Moreover, the standard deviation of the sample of data points (\\(s\\)) is: \\[s = \\sqrt{s^2}, \\quad \\text{for } s &gt; 0.\\] Now, let’s move to variance. It refers to the expected value of the squared deviation from the mean of a random variable in a population. Similarly, we do have sample variance as well, which is the expected value of the squared deviation from the mean of a random variable in a selected sample. At this point, we can still use sample variance to estimate population variance with adjustment, because the sample variance may differ significantly based on what data points are chosen from that population. Definition 1.10 Let \\(x_1, x_2, x_3, ..., x_n\\) be a sample of data points, we define sample variance of the sample data points (\\(s^2\\)) as: \\[s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n}(x_i - \\bar{x})^2, \\text{ where $\\bar{x}$ is the sample mean of the data points.}\\] Next is standard deviation. It is a measure of the amount of variation of the values of a variable about its mean. If standard deviation is relatively larger, then data points are widely spread out from the mean. Otherwise, data points stay close from the mean. Also, standard deviation is obtained by taking squared root from variance which is dependent on the choices of data points as well. To use sample standard deviation as an estimator to population standard deviation, we still need to adjust it. Definition 1.11 Let \\(x_1, x_2, x_3, ..., x_n\\) be a sample of data points. The standard deviation of the sample of data points (\\(s\\)) is: \\[s = \\sqrt{s^2}, \\quad \\text{for } s &gt; 0.\\] Median and Mode The median and mode are two important measures of central tendency used in statistics to summarize and understand data. The median represents the middle value in a sorted dataset, giving a sense of the center that is not affected by extreme values or outliers. In contrast, the mode is the value that appears most frequently in a dataset, making it useful for identifying common or repeated observations. Definition 1.12 Let: \\(x_1, x_2, x_3, ... , x_n\\) be a collection of data points which is arranged in ascending order from the smallest value to the largest value (or descending order from the largest value to the smallest value in that collection). The median of the given collection of data points is the middle value in that collection, which equally spreads the collection into two parts. Half of all the collection values are above the median value and the rest of the values in the collection is below the median value. Case 1: when n is an odd number. (i.e. \\(1, 3, 11, 237,...\\)). Then, the median \\(M\\) is defined as: \\[M = \\frac{n+1}{2} \\text{, where n represents the $n^{th}$ position}.\\] Case 2: when n is an even number (i.e. \\(2, 6, 100, 500,...\\)). Then, the median \\(M\\) is: the average value of \\(\\frac{n}{2}\\)’s and \\(\\frac{n+2}{2}\\)’s position, where n represents the \\(n^{th}\\) position. Now, let’s introduce mode. Definition 1.13 It refers to a value that appears the most frequent than the appearance of all other values in a given dataset. Percentile and Quartile Percentiles and quartiles are statistical measures used to describe the distribution of data. A percentile indicates the value below which a given percentage of observations fall, helping to understand relative standing within a dataset. Quartiles, a specific type of percentile, divide the data into four equal parts (Q1, Q2/median, and Q3), providing insights into the spread and central tendency. Definition 1.14 Let: \\(x_1, x_2, ..., x_n\\) be a collection of data points in either ascending order. Percentile is denoted as: \\(p^{th}\\), which indicates \\(p \\%\\) of observations are below to a such value. Quartiles, are special cases of percentile which equally spread the collection of data into four parts. Each part contains \\(25\\%\\) of the entire collection. More specifically, we define quartiles as the following: \\(Q_1\\): the \\(25\\) percentile (or \\(25^{th}\\)), which shows that \\(25\\%\\) of the data points are below the value \\(Q_1\\). \\(Q_2\\): the \\(50\\) percentile (or \\(50^{th}\\)), which shows that \\(50\\%\\) of the data points are below the value \\(Q_2\\). \\(Q_3\\): the \\(75\\) percentile (or \\(75^{th}\\)), which shows that \\(75\\%\\) of the data points are below the value \\(Q_3\\). \\(Q_2\\) is qual to median. Moreover, we use \\(Q_3 - Q_1\\) to calculate interquartile range (I.P.R), which shows the spread of the whole data set. Skewness and Symmetry The two terms ‘skewness’ and ‘symmetry’ are used to describe the shape of probability distribution. There are two types of skewness: left (or negative) skew and right (or positive) skew. In real life, a famous distribution highly used in hypothesis testing which is \\(\\chi_{n}^{2}\\) with \\(n\\) degrees of freedom, is right skewed probability distribution function. Another example regarding to symmetry is normal distribution such that its probability under its curve greater than \\(\\mu\\) is same as the probability below than \\(\\mu\\). Now let’s introduce the proper definition of skewness and symmetry. Definition 1.15 Skewness refers to such a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative or undefined. Now, let’s break down the main definition of skewness and symmetry: Definition 1.16 By observing given probability distribution curve, if the left tail of the curve is longer than the right tail the mass of the distribution is concentrated on the right of the figure, then we say that probability distribution is left skew or negative skew. (See figure below) Visualization of left skew probability distribution Definition 1.17 By observing given probability distribution curve, if the right tail of the curve is longer than the left tail the mass of the distribution is concentrated on the left of the figure, then we say that probability distribution is right skew or positive skew. (See figure below) Visualization of right skew probability distribution Symmetry is a special case of skewness when the value of skewness is \\(0\\). Definition 1.18 In statistics, symmetry s a probability distribution is reflected around a vertical line at some value of the random variable represented by the distribution. Probability under the curve below that value is equal to probability under the curve greater than that value. (see figure below) Since symmetry is a special case, so that it has a unique property as the following: Theorem 1.1 For any symmetric (bell-shaped) curve, let \\(\\mu\\) be its mean and \\(\\sigma\\) be its standard deviation, the following probability set function is true: \\(1.\\) \\(P(\\mu - \\sigma &lt; X &lt; \\mu + \\sigma) = 68.27\\%;\\) \\(2.\\) \\(P(\\mu - 2\\sigma &lt; X &lt; \\mu + 2\\sigma) = 95.45\\%;\\) \\(3.\\) \\(P(\\mu - 3\\sigma &lt; X &lt; \\mu + 3\\sigma) = 99.73\\%.\\) Visualization of symmetric probability distribution Practice Example Example 1.1 Let: \\(x_1 = 1, x_2 = 3\\) and \\(x_3 = 7\\). Calculate the sample mean, sample variance and sample standard deviation for this collection of data points. Solution (all results are kept in four digits): By Definition \\(1.9 \\text{, } 1.10 \\text{, } 1.11\\), sample mean: \\[\\bar{x} = \\frac{1+3+7}{3} \\approx 3.6667.\\] Then, we use sample mean to calculate sample variance: \\[s^2 = \\frac{1}{3-1} \\times [(1-3.6667)^2+(3-3.6667)^2+(7-3.6667)^2] \\approx 9.3333.\\] Finally, we take the square root of sample variance to get sample deviation, and remember that \\(s &gt; 0\\): \\[s = \\sqrt{s^2} \\approx 3.0551.\\] Example 1.2 Given two distinct collections of data points: \\(S_1\\) = \\(\\{2, 4, 6\\}\\) and \\(S_2\\) = \\(\\{1, 5, 16, 28\\}\\). Calculate the median of both two sets. Solution: For \\(S_1\\), since \\(n = 3\\) which is an odd number, so by \\(Definition \\text{ } 1.3\\), \\(M_{S_1} = 4\\). For \\(S_2\\), \\(n = 4\\) in this case, so that we need to calculate the average of \\(\\frac{n}{2}\\) and \\(\\frac{n+1}{2}\\). Then, \\[M_{S_2} = \\frac{5+16}{2} = 10.5.\\] Example 1.3 Consider the data set \\(S =\\) \\(\\{4, 25, 30, 30, 30, 32, 32, 35, 50, 50, 50, 55, 60, 74, 110\\}\\). Calculate its median and \\(Q_1\\) (\\(25^{th}\\)). Solution: Simply counting the number of data points, \\(n = 15\\), such that \\(M_{S}\\) = \\(\\frac{15 + 1}{2}\\) = \\(8\\). Thus, the \\(8^{th}\\) value in the set which is \\(35\\). Since we know the median of this collection of data points, we just need to find the median of the lower half of this data, which is exactly going to be \\(25\\) percentile (\\(25^{th}\\)). In the lower half of the given collection (all values below the median), \\(n_{lower} = 7\\). By \\(Definition \\text{ } 1.3\\), then median of the lower half (\\(25^{th}\\)) is going to be: \\[25^{th} = \\frac{7+1}{2} = 4, \\text{ the $4^{th}$ position in the data set}.\\] Thus, \\(Q_1\\) (\\(25^{th}\\)) \\(= 30\\). To find \\(Q_3\\) (\\(75^{th}\\)), apply the same strategy will guide you to find the correct answer, and we leave this as an exercise to you. 1.3 Graphical Techniques In statistics, there are lots of types of graph to illustrate data, for example histograms and box-plots. This technique is used in the field of statistics for data visualization. Our objective is to both be able to identify some classical types of graph and interpret key statistical values (descriptive statistical values) from it. 1.3.1 Histograms Introduction to Histograms Histogram is a graphical representation of data that uses bars to display the frequency distribution of a dataset. Unlike bar graphs, which represent categorical data, histograms group numerical data into intervals (bins) and show how many values fall into each range. This makes histograms ideal for visualizing the shape, spread, and central tendency of continuous data, helping identify patterns such as symmetry, skewness, and outliers. Visualization of histograms Advantages and Disadvantages of Histograms Advantages of Histograms: \\(1.\\) Histograms are easily to used for visualise data (relatively). It allows us to get the idea of the \"shape\" of distribution (i.e. skewness which will be discussed late in this section). \\(2.\\) It is also flexible that people are able to modify bin widths. Disadvantages of Histograms: \\(1.\\) It is not suitable for small data sets. \\(2.\\) The values from histograms close to breaking points are likely similar, in fact they need to be classified into different bins (i.e. Student A and B scores 79 and 80 respectively in STA258, we consider a breaking point between 79 and 80. The two students have similar score, but student A is \\(B+\\) and student B is \\(A-\\) in GPA from). Histograms with Skewness and Symmetry A histogram visually represents the distribution of numerical data, making it a useful tool for assessing skewness and symmetry. It is quite straightforward to estimate the skewness of histograms by simply drawing a curve above bins on the histogram. For a histogram to have a left (or negative) skew probability distribution: Visualization of a histogram has a left (or negative) skew probability distribution For a histogram to have a right (or positive) skew probability distribution: Visualization of a histogram has a right (or positive) skew probability distribution For a histogram to have a symmetric probability distribution: Visualization of a histogram has a symmetric probability distribution 1.3.2 Box-Plots A boxplot (or box-and-whisker plot) is a standardized way to display data distribution based on a five-number summary: minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum. The box represents the interquartile range (IQR), while the whiskers show variability outside Q1 and Q3. Outliers are plotted as individual points. Boxplots efficiently compare distributions and highlight skewness, spread, and outliers. (See figure below) Visualization of a box-plot Similar to histograms, we can still obtain information about skewness and symmetry, by observing the cut from the line of Q2. If the median (Q2) cuts the box with upper area smaller than lower area, then we say that box-plot with left skew probability distribution. Or, if the median (Q2) cuts the box with upper area larger than lower area, then we say that box-plot with right skew probability distribution. Otherwise, if the median (Q2) cuts the box with upper area equal to lower area, then we say that box-plot with symmetric probability distribution. Visualization of a box-plot with skew and symmetric probability distribution 1.4 Introduction to R R is used for data manipulation, statistics, and graphics. It is made of: operations (\\(+\\),\\(-\\), \\(&lt;\\)) which is for calculations on vectors, arrays and matrices; a huge collection of functions; facilities for making unlimited types quality graphs; user contributed packages (sets of related functions); the ability to interface with procedures written in C, C+, or FORTRAN and to write additional primitives. R is also an open-source computing package which has seen a huge growth in popularity in the last few years (Please use this website: https://cran.r-project.org, to download R). What is R-studio? RStudio is a relatively new editor specially targeted at R. RStudio is cross-platform, free and open-source software (Please use: https://www.rstudio.com, to download Rstudio). Make a Histogram Using R-studio This is just a demonstration of how to start and use R-studio. . First of all, we need to know which dataset are we going to make into a histogram. In this case, as an example, we are going to use the waiting time in faithful in R-studio. 2. For any dataset, use the code: names(faithful) to get it. (inside the parentheses, type the names of variables you want in faithful dataset) 3. Then, we proceed with the code: hist(faithful$waiting) to get a basic plot. R-studio first three steps (by following the instructions, you should get this histogram) . Furthermore, we can also get more information. For example, by keep proceeding with the code: hist(faithful$waiting,plot=FALSE)$breaks, R-studio will show you all the breaking points between histogram cells. R-studio the forth step(by following the instructions, you should get this histogram) "],["sampling-distributions-related-to-a-normal-population.html", "Chapter 2 Sampling Distributions Related to a Normal Population 2.1 Normal Distribution 2.2 Gamma and Chi-square Distribution 2.3 Student’s t-Distribution and F-Distribution", " Chapter 2 Sampling Distributions Related to a Normal Population Previously, we have introduced lots of definitions and given you a rough idea about what really statistics it and what people do in statistics. Now, we are going to proceed statistical distributions. 2.1 Normal Distribution In probability theory and statistics, normal distribution also called Gaussian distribution which is discovered by a famous German mathematician Johann Carl Friedrich Gauss in 1809. It is one of the most important distribution that used to approximate other types of probability distribution, such as binomial, hypergeometric, inverse (or negative) hypergeometric, negative binomial and Poisson distribution. Generally, it is denote as \\(N(\\mu, \\sigma^2)\\) with probability density function as the following: \\[f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\cdot e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}.\\] Formally, let’s begin with its definition: Definition 2.1 Suppose a random variable \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(E(X) = \\mu \\text{ and } Var(X) = \\sigma^2\\). And \\(-\\infty &lt; \\mu &lt; \\infty, \\sigma^2 &gt; 0.\\) Moreover, \\(X\\) has probability density function as: \\[f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\cdot e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}, \\text{ for $-\\infty &lt; x &lt; \\infty$ (same as above).}\\] The only special case of normal distribution is standard normal distribution, such that a random variable \\(Y \\sim N( \\mu = E(Y) = 0, \\sigma^2 = Var(Y) = 1)\\), then \\(Y\\) has probability density function as: \\[f(y) = \\frac{1}{\\sqrt{2\\pi}} \\cdot e^{-\\frac{y^2}{2}}.\\] 2.2 Gamma and Chi-square Distribution The Chi-square and Gamma distributions are two fundamental probability distributions widely used in statistical theory and applications. The Gamma distribution is a continuous distribution characterized by its shape and scale parameters, making it versatile for modeling waiting times and various positively skewed data. The Chi-square distribution, a special case of the Gamma distribution, arises naturally in the context of hypothesis testing and confidence interval estimation, especially in tests involving variance and categorical data. Gamma Distribution Definition 2.2 Suppose a random variable \\(X\\) is Gamma distributed with \\(\\alpha &gt; 0\\) (shape parameter) and \\(\\beta &gt; 0\\) (scale parameter) if and only if the probability density function of \\(X\\) is \\[f(x) = \\frac{x^{\\alpha - 1} e^{\\frac{-x}{\\beta}}}{\\beta^{\\alpha} \\Gamma(\\alpha)}, \\text{ for $0 &lt; x &lt; \\infty.$}\\] Then, \\(E(X) = \\alpha \\beta \\text{, } Var(X) = \\alpha \\beta^2 \\text{ and its moment generating function is } M_{X}(t) = \\frac{1}{(1 - \\beta t)^{\\alpha}},\\) for \\(t &lt; \\frac{1}{\\beta}.\\) Now, let’s introduce some properties of Gamma function: Gamma function (not a distribution): \\[\\Gamma(x) = \\int_{0}^{\\infty}t^{x-1}e^{-t}\\,dt, \\text{ for $x &gt; 0$.}\\] Properties 1. \\(\\Gamma(x) = x \\cdot \\Gamma(x-1)\\); 2. For all \\(n \\in \\mathbb{N} \\text{, } \\Gamma(n) = (n - 1)!\\); 3. \\(\\Gamma(\\frac{1}{2}) = \\sqrt{\\pi}\\). Chi-square Distribution Here is its formal definition: Definition 2.3 A random variable \\(X\\) has a Chi-squared distribution with \\(n\\) degrees of freedom \\((\\chi_{n}^{2})\\) if and only if \\(X\\) is a random variable with a Gamma distribution with parameters \\(\\alpha = \\frac{n}{2} \\text{ and } \\beta = 2.\\) Then, the probability density function of \\(X\\) is given by \\[f(x) = \\frac{1}{2^{\\frac{n}{2}} \\Gamma(\\frac{n}{2})} x^{\\frac{k}{2} - 1} e^{\\frac{-x}{2}}.\\] Moreover, \\(E(X) = n \\text{, } Var(X) = 2n \\text{ and moment generating function of $X$ is } M_{X}(t) = (1 - 2t)^{\\frac{-n}{2}}, \\text{ for $t &lt; \\frac{1}{2}$}.\\) We claim that Chi-square distribution is a special case of Gamma distribution with \\(\\alpha = \\frac{n}{2} \\text{ and } \\beta = 2\\). Now, let’s prove it by using moment generating function. The proof is quite straightforward as the following shows: Proof. Proof. Suppose \\(X \\sim Gamma( \\alpha = \\frac{n}{2} \\text{, } \\beta = 2).\\) Then the following moment generating function holds for \\(X\\): \\[M_{X}(t) = (1- 2t)^{\\frac{-n}{2}}, \\text{ for $t &lt; \\frac{1}{2}$}.\\] Compare the moment generating function of \\(X\\) under Gamma distribution with Chi-square distribution, we can conclude that \\(X \\sim \\chi_{n}^{2}\\). ◻ Obtaining Chi-square Distribution by Normal Distribution Previously, we showed how to use Gamma distribution to get Chi-square distribution by moment generating function method. Now, let’s do something interestingly, to use normal distribution to get Chi-square distribution. We will begin with a theorem, then prove it. Suppose a random variable \\(Z\\) is standard normally distributed, such that \\(Z \\sim N(0 \\text{, }1).\\) Then, \\(Z^2\\) is Chi-square distributed with \\(1\\) degree of freedom, so that \\(Z^2 \\sim \\chi_{1}^{2}\\). The proof of Theorem \\(2.1\\) isn’t that trivial to see. We still need moment generating function, but in a different way. Before we get into the proper proof, let’s grab everything we need: 1. Recall STA256 about how to get moment generating function for a given continuous random variable that: \\[M_{Z}(t) = \\int_{-\\infty}^{\\infty} e^{tx}f_{X}(x)\\,dx.\\] 2. We also need Gaussian integral: $$ \\[\\begin{aligned} \\int_{-\\infty}^{\\infty} e^{-x^2}\\,dx &amp;= \\sqrt{\\pi}; \\label{eq:gaussian1} \\\\ \\int_{-\\infty}^{\\infty} e^{-kx^2}\\,dx &amp;= \\sqrt{\\frac{\\pi}{k}}, \\text{ for $k &gt; 0$}; \\label{eq:gaussian2} \\\\ \\int_{-\\infty}^{\\infty} e^{kx^2}\\,dx &amp;= \\sqrt{\\frac{\\pi}{-k}}, \\text{ for $k &lt; 0$}. \\label{eq:gaussian3} \\end{aligned}\\] $$ Proof. Proof. Suppose that \\(Z \\sim N(0, 1)\\), then \\(f_{Z}(z) = \\frac{1}{\\sqrt{2\\pi}} \\cdot e^{\\frac{-z^2}{2}}\\). The moment generating function (MGF) of \\(Z^2\\) is: \\[\\begin{aligned} M_{Z^2}(t) &amp;= \\mathbb{E}\\left(e^{tZ^2}\\right) \\\\ &amp;= \\int_{-\\infty}^{\\infty} e^{tz^2} f_Z(z) \\, dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{tz^2} e^{-\\frac{z^2}{2}} \\, dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-\\left(\\frac{1}{2}-t\\right)z^2} \\, dz. \\end{aligned}\\] Apply substitution with \\(u = z\\sqrt{\\frac{1}{2}-t}\\), \\(dz = \\frac{du}{\\sqrt{\\frac{1}{2}-t}}\\): \\[\\begin{aligned} M_{Z^2}(t) &amp;= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-u^2} \\cdot \\frac{1}{\\sqrt{\\frac{1}{2}-t}} \\, du \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{\\frac{1}{2}-t}} \\int_{-\\infty}^{\\infty} e^{-u^2} \\, du \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{\\frac{1}{2}-t}} \\cdot \\sqrt{\\pi} \\\\ &amp;= \\frac{1}{\\sqrt{1-2t}}. \\end{aligned}\\] This is the MGF of a chi-squared distribution with 1 degree of freedom, \\(Z^2 \\sim \\chi_1^2\\). ◻ Now, we can do another proof by using Theorem \\(2.1\\). Suppose \\(Z_1, Z_2, ..., Z_n \\overset{\\text{i.i.d.}}{\\sim} N(0,1)\\), then the sum of \\(n\\) independent \\(Z^2\\) is going to be Chi-square distributed with \\(n\\) degrees of freedom, as the following: \\[\\sum_{i=1}^{n}Z_{i}^{2} \\sim \\chi_{n}^{2}.\\] We need Theorem \\(2.1\\) to prove this, but it going to be easier. Proof. Proof. Suppose \\(Z \\sim \\mathcal{N}(0,1)\\), then its probability density function is: \\[f_Z(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}.\\] Let \\(\\delta = \\sum_{i=1}^{n} Z_i^2\\), where \\(Z_1, \\ldots, Z_n \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,1)\\). The moment generating function (MGF) of \\(\\delta\\) is: \\[\\begin{aligned} M_{\\delta}(t) &amp;= \\mathbb{E}\\left[e^{t\\delta}\\right] \\\\ &amp;= \\mathbb{E}\\left[e^{t(Z_1^2 + \\cdots + Z_n^2)}\\right] \\\\ &amp;= \\mathbb{E}\\left[\\prod_{i=1}^n e^{tZ_i^2}\\right]. \\end{aligned}\\] Since \\(Z_1, \\ldots, Z_n\\) are independent and identically distributed: \\[\\begin{aligned} M_{\\delta}(t) &amp;= \\prod_{i=1}^n \\mathbb{E}\\left[e^{tZ_i^2}\\right] \\\\ &amp;= \\prod_{i=1}^n M_{Z_i^2}(t). \\end{aligned}\\] From Theorem 2.1, we know \\(Z_i^2 \\sim \\chi_1^2\\) with MGF \\((1-2t)^{-1/2}\\), therefore: \\[\\begin{aligned} M_{\\delta}(t) &amp;= \\prod_{i=1}^n (1-2t)^{-1/2} \\\\ &amp;= (1-2t)^{-n/2}. \\end{aligned}\\] This is exactly the MGF of a chi-squared distribution with \\(n\\) degrees of freedom, proving that \\(\\delta \\sim \\chi_n^2\\) as required. ◻ Here is the last theorem for Chi-square and normal distribution, but we won’t show you the proof due to its complexity. For people who are interested in that, please see STA260 lecture notes or power point slide to figure out. Let \\(n\\) be sample size, \\(s^2\\) be sample variance and \\(\\sigma^2\\) be population variance, then \\(\\frac{(n-1)s^2}{\\sigma^2}\\) is Chi-square distributed with \\(n - 1\\) degrees of freedom. As the following: \\[\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi_{n-1}^{2}.\\] 2.3 Student’s t-Distribution and F-Distribution The t-distribution and F-distribution are essential tools in inferential statistics, particularly in the context of hypothesis testing and variance analysis. The t-distribution, which resembles the normal distribution but with heavier tails, is primarily used when estimating population means in situations where the sample size is small and the population standard deviation is unknown. On the other hand, the F-distribution is used to compare variances between two populations and plays a central role in analysis of variance (ANOVA) and regression analysis. Student’s t-Distribution Definition 2.4 Suppose \\(X\\) is t-distributed with \\(n\\) degrees of freedom, then the probability density function of \\(X\\) is given by: \\[f_{X}(x) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{\\pi n} \\Gamma(\\frac{n}{2})} (1+\\frac{x^2}{n})^{\\frac{-n+1}{2}}.\\] Alternatively, define a new variable \\(T\\) is the following: \\[T = \\frac{W}{\\sqrt{\\frac{V}{r}}}, \\text{ for $W \\sim N(0, 1) \\text{ and } V \\sim \\chi_{r}^{2}$.}\\] Or suppose \\(X_1, ..., X_n \\overset{\\text{i.i.d.}}{\\sim} N(\\mu, \\text{ } \\sigma^2)\\), then \\(\\bar{X} \\sim N(\\mu, \\text{ } \\frac{\\sigma^2}{n})\\). Thus, \\[T = \\frac{ \\bar{x} - \\mu}{(\\frac{s}{\\sqrt{n}})}.\\] Same as normal distribution, student’s t-distribution is also symmetric. Also, as the degrees of freedom of t-distribution getting larger, the curve of student’s t-distribution getting closer to standard normal distribution. F-Distribution Definition 2.5 We define a new variable \\(F\\) as the following shows: \\[F = \\frac{ (\\frac{W_1}{v_1}) }{ (\\frac{W_2}{v_2})} \\sim F_{v_1, \\text{ } v_2}; \\text{ for $W_1 \\sim \\chi_{v_1}^{2}$ and $W_2 \\sim \\chi_{v_2}^{2}$; also both $W_1$ and $W_2$ are independent.}\\] Alternatively, we select two samples (with same population variance) with size \\(n\\) and \\(m\\), and also sample variance \\(s_x\\) and \\(s_y\\) respectively. Then, F-distribution is: \\[F = \\frac{ [\\frac{ (\\frac{(n-1)}{\\sigma^2}) s_x^2}{n-1}] }{ [\\frac{ (\\frac{(m-1)}{\\sigma^2}) s_y^2}{m-1}] } \\sim F_{n-1, \\text{ } m-1 }.\\] Both student’s t-distribution and F-distribution are highly used in inferential statistics, until confidence interval, testing hypothesis and ANOVA analysis, these two distributions will come to play a lot. At this point, just guarantee that you know how to obtain those distribution from random given information is sufficient. "],["the-central-limit-theorem.html", "Chapter 3 The Central Limit Theorem", " Chapter 3 The Central Limit Theorem The Central Limit Theorem (CLT) is one of the most important results in probability and statistics. It states that, given a sufficiently large sample size, the distribution of the sample mean of independent and identically distributed (i.i.d.) random variables approaches a normal distribution, regardless of the shape of the original distribution. Real-life Application of Central Limit Theorem in Financial Analysis. The CLT is often used by financial experts to examine stock market results. Now, let’s discuss Central Limit Theorem with more details. Suppose we have a finite number of populations and each population follows a distribution with population mean \\(\\mu\\) and population variance \\(\\sigma^2.\\). Then we take samples of same size \\(n\\) from each population, such that we have \\(\\bar{x}_1, \\bar{x}_2, ..., \\bar{x}_m\\) from population group \\(1 \\text{ to } m, \\text{ respectively.}\\) Next, we make a histogram using the large collection of sample taken from each population group. Then, what we are doing right row is sampling distribution of \\(\\bar{x}\\). As a result, \\(\\bar{x}\\) follows a normal distribution with mean \\(\\mu_{\\bar{x}} = \\mu\\) and variance \\(\\sigma_{\\bar{x}}^{2} = \\frac{\\sigma^2}{n}\\), which is denoted as the following: \\[\\bar{x} \\sim N(\\mu_{\\bar{x}} = \\mu, \\text{ } \\sigma_{\\bar{x}}^{2} = \\frac{\\sigma^2}{n}).\\] Figure \\(3.1\\) below shows the entire procedure about the Central Limit Theorem. Procedure of the Central Limit Theorem Now, let’s begin with the proper definition of Central Limit Theorem. Definition 3.1 Let \\(X_1, X_2, ..., X_n\\) be independent and identically distributed random variables with \\(E(X_i) = \\mu\\) and \\(Var(X_i) = \\sigma^2 &lt; \\infty\\). Then, we define the following: \\[U_{n} = \\frac{\\bar{X} - \\mu}{(\\frac{\\sigma}{\\sqrt{n}})} \\sim N(\\mu = 0, \\sigma^2 = 1), \\text{ where $\\bar{X} = \\frac{1}{n} \\sum_{i =1}^{n}X_i$.}\\] Then the distribution function of \\(U_{n}\\) converges to the standard Normal distribution function as \\(n \\longrightarrow \\infty\\). That is, \\[\\lim_{n\\to\\infty}P(U_n \\le u) = \\int_{\\infty}^{u} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{t^2}{2}}\\,dt; \\text{ for all u.}\\] For this course in particular, we do not need to pay that much attention to the proving part of the definition above. However, we use Central Limit Theorem to approximate distributions. Here are the two important approximations: \\(\\bar{X_n} \\approx N(\\mu, \\frac{\\sigma^2}{n});\\) \\(T = \\sum_{i = 1}^{n}X_i \\approx N(n\\mu, n\\sigma^2).\\) A reminder that the distribution of \\(U_n\\) in definition \\(3.1\\) and the two approximation of distribution above are extremely important in this course, until later chapters you may see some materials that are similar. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
