<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Introduction to Simple Linear Regression | _main.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Introduction to Simple Linear Regression | _main.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Introduction to Simple Linear Regression | _main.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="two-sample-hypothesis-tests.html"/>
<link rel="next" href="inference-for-simple-linear-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>STA258: Statistics with Applied Probability</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#nishan-mudalige-masoud-ataei-nurlana-alili-bryan-xu"><i class="fa fa-check"></i>Nishan Mudalige, Masoud Ataei, Nurlana Alili, Bryan Xu</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html"><i class="fa fa-check"></i><b>1</b> Descriptive Statistics and an Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#descriptive-statistics"><i class="fa fa-check"></i><b>1.2</b> Descriptive Statistics</a></li>
<li class="chapter" data-level="1.3" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#graphical-techniques"><i class="fa fa-check"></i><b>1.3</b> Graphical Techniques</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#histograms"><i class="fa fa-check"></i><b>1.3.1</b> Histograms</a></li>
<li class="chapter" data-level="1.3.2" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#box-plots"><i class="fa fa-check"></i><b>1.3.2</b> Box-Plots</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="descriptive-statistics-and-an-introduction-to-r.html"><a href="descriptive-statistics-and-an-introduction-to-r.html#introduction-to-r"><i class="fa fa-check"></i><b>1.4</b> Introduction to R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html"><i class="fa fa-check"></i><b>2</b> Sampling Distributions Related to a Normal Population</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#normal-distribution"><i class="fa fa-check"></i><b>2.1</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.2" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#gamma-and-chi-square-distribution"><i class="fa fa-check"></i><b>2.2</b> Gamma and Chi-square Distribution</a></li>
<li class="chapter" data-level="2.3" data-path="sampling-distributions-related-to-a-normal-population.html"><a href="sampling-distributions-related-to-a-normal-population.html#students-t-distribution-and-f-distribution"><i class="fa fa-check"></i><b>2.3</b> Student’s t-Distribution and F-Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-central-limit-theorem.html"><a href="the-central-limit-theorem.html"><i class="fa fa-check"></i><b>3</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="4" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html"><i class="fa fa-check"></i><b>4</b> Normal Approximation to the Binomial Distribution</a>
<ul>
<li class="chapter" data-level="4.1" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#bernoulli-distribution"><i class="fa fa-check"></i><b>4.2</b> Bernoulli Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#sampling-distribution-of-the-sum-and-mgf-derivation"><i class="fa fa-check"></i><b>4.3</b> Sampling Distribution of the Sum and MGF Derivation</a></li>
<li class="chapter" data-level="4.4" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#binomial-distribution"><i class="fa fa-check"></i><b>4.4</b> Binomial Distribution</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#visualizing-the-pmf-of-binomial-distributions"><i class="fa fa-check"></i><b>4.4.1</b> Visualizing the PMF of Binomial Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#sampling-distribution-of-a-sample-proportion-and-the-normal-approximation"><i class="fa fa-check"></i><b>4.5</b> Sampling Distribution of a Sample Proportion and the Normal Approximation</a>
<ul>
<li class="chapter" data-level="" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#conditions-for-using-the-normal-approximation"><i class="fa fa-check"></i>Conditions for Using the Normal Approximation</a></li>
<li class="chapter" data-level="" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#bernoulli-distribution-binomial-with-n-1"><i class="fa fa-check"></i>Bernoulli Distribution (Binomial with <span class="math inline">\(n = 1\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#normal-approximation-to-binomial"><i class="fa fa-check"></i><b>4.6</b> Normal Approximation to Binomial</a></li>
<li class="chapter" data-level="4.7" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#continuity-correction"><i class="fa fa-check"></i><b>4.7</b> Continuity Correction</a>
<ul>
<li class="chapter" data-level="" data-path="normal-approximation-to-the-binomial-distribution.html"><a href="normal-approximation-to-the-binomial-distribution.html#continuity-correction-table"><i class="fa fa-check"></i>Continuity Correction Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html"><i class="fa fa-check"></i><b>5</b> Law of Large Numbers</a>
<ul>
<li class="chapter" data-level="5.1" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#convergence-in-probability"><i class="fa fa-check"></i><b>5.1</b> Convergence in Probability</a></li>
<li class="chapter" data-level="5.2" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#weak-law-of-large-numbers-wlln"><i class="fa fa-check"></i><b>5.2</b> Weak Law of Large Numbers (WLLN)</a>
<ul>
<li class="chapter" data-level="" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#proof-of-the-weak-law-of-large-numbers-wlln"><i class="fa fa-check"></i>Proof of the Weak Law of Large Numbers (WLLN)</a></li>
<li class="chapter" data-level="" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html#empirical-probability-insight"><i class="fa fa-check"></i>Empirical Probability Insight</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><i class="fa fa-check"></i><b>6</b> One Sample Confidence Intervals on a Mean When the Population Variance is Known</a>
<ul>
<li class="chapter" data-level="6.1" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#interpretation"><i class="fa fa-check"></i><b>6.2</b> Interpretation</a></li>
<li class="chapter" data-level="6.3" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#confidence-interval-for-mu-known-variance"><i class="fa fa-check"></i><b>6.3</b> Confidence Interval for <span class="math inline">\(\mu\)</span> (Known Variance)</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#confidence-interval-for-the-mean-of-a-normal-population"><i class="fa fa-check"></i>Confidence Interval for the Mean of a Normal Population</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#large-sample-ci-for-mu-normal-data"><i class="fa fa-check"></i>Large Sample CI for <span class="math inline">\(\mu\)</span> (Normal data)</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#one-sample-ci-on-the-population-mean-mu"><i class="fa fa-check"></i>One Sample CI on the Population Mean <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#table-of-common-z-values"><i class="fa fa-check"></i>Table of Common <span class="math inline">\(z\)</span>-values</a></li>
<li class="chapter" data-level="6.4" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#appendix"><i class="fa fa-check"></i><b>6.4</b> APPENDIX</a>
<ul>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-known.html#pivotal-quantities"><i class="fa fa-check"></i>Pivotal quantities</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><i class="fa fa-check"></i><b>7</b> One-Sample Confidence Intervals on a Mean When the Population Variance is Unknown</a>
<ul>
<li class="chapter" data-level="7.1" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#cis-for-mu"><i class="fa fa-check"></i><b>7.1</b> CIs for <span class="math inline">\(\mu\)</span></a>
<ul>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#independence-assumption"><i class="fa fa-check"></i>Independence Assumption</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#randomization-condition"><i class="fa fa-check"></i>Randomization Condition</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#standard-error"><i class="fa fa-check"></i>Standard Error</a></li>
<li class="chapter" data-level="" data-path="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html"><a href="one-sample-confidence-intervals-on-a-mean-when-the-population-variance-is-unknown.html#a-few-final-comments"><i class="fa fa-check"></i>A few final comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="one-sample-confidence-intervals-on-a-proportion.html"><a href="one-sample-confidence-intervals-on-a-proportion.html"><i class="fa fa-check"></i><b>8</b> One Sample Confidence Intervals On a Proportion</a></li>
<li class="chapter" data-level="9" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html"><i class="fa fa-check"></i><b>9</b> Sample Size Selection using Confidence Intervals</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#empirical-rule"><i class="fa fa-check"></i><b>9.0.1</b> Empirical Rule</a></li>
<li class="chapter" data-level="9.1" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#secSampleSizeCIMean"><i class="fa fa-check"></i><b>9.1</b> Calculating Sample Size for a Confidence Interval on a Mean</a>
<ul>
<li class="chapter" data-level="" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#when-sigma-is-known"><i class="fa fa-check"></i>When <span class="math inline">\(\sigma\)</span> is Known</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="sample-size-selection-using-confidence-intervals.html"><a href="sample-size-selection-using-confidence-intervals.html#calculating-sample-size-for-a-confidence-interval-on-a-proportion"><i class="fa fa-check"></i><b>9.2</b> Calculating Sample Size for a Confidence Interval on a Proportion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html"><i class="fa fa-check"></i><b>10</b> Two Sample Confidence Interval</a>
<ul>
<li class="chapter" data-level="10.1" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-a-difference-of-mean"><i class="fa fa-check"></i><b>10.1</b> Two Sample Confidence Interval on a Difference of Mean</a></li>
<li class="chapter" data-level="10.2" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-paired-data"><i class="fa fa-check"></i><b>10.2</b> Two Sample Confidence Interval on Paired Data</a></li>
<li class="chapter" data-level="10.3" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-proportions"><i class="fa fa-check"></i><b>10.3</b> Two Sample Confidence Interval on Proportions</a></li>
<li class="chapter" data-level="10.4" data-path="two-sample-confidence-interval.html"><a href="two-sample-confidence-interval.html#two-sample-confidence-interval-on-variances"><i class="fa fa-check"></i><b>10.4</b> Two Sample Confidence Interval on Variances</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html"><i class="fa fa-check"></i><b>11</b> Introduction to Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="11.1" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#test-of-hypothesis-for-one-mean"><i class="fa fa-check"></i><b>11.1</b> Test of Hypothesis for One Mean</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-1-decide-on-a-level-of-significance-alpha"><i class="fa fa-check"></i>Step 1: Decide on a Level of Significance (<span class="math inline">\(\alpha\)</span>)</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-2-state-the-null-hypothesis-h_0-and-the-alternative-hypothesis-h_a"><i class="fa fa-check"></i>Step 2: State the Null Hypothesis (<span class="math inline">\(H_0\)</span>) and the Alternative Hypothesis (<span class="math inline">\(H_a\)</span>)</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-3-calculate-an-appropriate-test-statistic"><i class="fa fa-check"></i>Step 3: Calculate an appropriate test statistic</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-4-calculate-the-p-value"><i class="fa fa-check"></i>Step 4: Calculate the p-value</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#step-5-compare-p-value-to-level-of-significance-alpha-and-make-a-conclusion"><i class="fa fa-check"></i>Step 5: Compare <em>p</em>-value to level of significance <span class="math inline">\(\alpha\)</span> and make a conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#test-of-hypothesis-for-one-proportion"><i class="fa fa-check"></i><b>11.2</b> Test of Hypothesis for One Proportion</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#introduction-to-hypothesis-testing-significance-test"><i class="fa fa-check"></i>Introduction to Hypothesis Testing (Significance Test)</a></li>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#steps-in-conducting-hypothesis-testing"><i class="fa fa-check"></i>Steps in conducting Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#some-additional-examples"><i class="fa fa-check"></i>Some Additional Examples</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#test-of-hypothesis-for-one-variance"><i class="fa fa-check"></i><b>11.3</b> Test of Hypothesis for One Variance</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="one-sample-hypothesis-test-on-a-proportion-and-variance.html"><a href="one-sample-hypothesis-test-on-a-proportion-and-variance.html"><i class="fa fa-check"></i><b>12</b> One Sample Hypothesis Test on a Proportion and Variance</a>
<ul>
<li class="chapter" data-level="12.1" data-path="one-sample-hypothesis-test-on-a-proportion-and-variance.html"><a href="one-sample-hypothesis-test-on-a-proportion-and-variance.html#one-sample-hypothesis-test-on-a-proportion"><i class="fa fa-check"></i><b>12.1</b> One Sample Hypothesis Test on a Proportion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="statistical-power.html"><a href="statistical-power.html"><i class="fa fa-check"></i><b>13</b> Statistical Power</a>
<ul>
<li class="chapter" data-level="13.1" data-path="statistical-power.html"><a href="statistical-power.html#statistical-power-1"><i class="fa fa-check"></i><b>13.1</b> Statistical Power</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-power.html"><a href="statistical-power.html#type-i-and-ii-errors"><i class="fa fa-check"></i>Type I and II Errors</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="statistical-power.html"><a href="statistical-power.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>13.2</b> Type I and Type II Errors</a></li>
<li class="chapter" data-level="13.3" data-path="statistical-power.html"><a href="statistical-power.html#using-power-to-determine-sample-size"><i class="fa fa-check"></i><b>13.3</b> Using Power to Determine Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html"><i class="fa fa-check"></i><b>14</b> Two Sample Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="14.1" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-means-with-independent-samples"><i class="fa fa-check"></i><b>14.1</b> Comparing Means with Independent Samples</a>
<ul>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#setting-up-hypotheses"><i class="fa fa-check"></i>Setting Up Hypotheses</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#structure-of-a-test-statistic"><i class="fa fa-check"></i>Structure of a Test Statistic</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#hypothesis-test-on-a-difference-of-means-mu_1---mu_2"><i class="fa fa-check"></i>Hypothesis Test on a Difference of Means (<span class="math inline">\(\mu_1 - \mu_2\)</span>)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-means-of-independent-samples-normal-population-assumptions"><i class="fa fa-check"></i>Comparing Means of Independent Samples (Normal Population Assumptions)</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-two-populations-means-independent-sampling-equal-variances-assumed"><i class="fa fa-check"></i><b>14.1.1</b> Comparing Two Populations Means: Independent Sampling (Equal Variances Assumed)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#small-sample-confidence-interval-for-mu_1---mu_2-with-equal-variances"><i class="fa fa-check"></i>Small-Sample Confidence Interval for <span class="math inline">\(\mu_1 - \mu_2\)</span> (with equal variances)</a></li>
<li class="chapter" data-level="14.1.2" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#comparing-two-populations-means-independent-sampling-unequal-variances-assumed"><i class="fa fa-check"></i><b>14.1.2</b> Comparing Two Populations Means: Independent Sampling (Unequal Variances Assumed)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#small-sample-confidence-interval-for-mu_1---mu_2-unequal-variances"><i class="fa fa-check"></i>Small-Sample Confidence Interval for <span class="math inline">\(\mu_1 - \mu_2\)</span> (Unequal Variances)</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#degrees-of-freedom"><i class="fa fa-check"></i>Degrees of Freedom</a></li>
<li class="chapter" data-level="" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-t-test-unequal-variances"><i class="fa fa-check"></i>Two-Sample t-Test (Unequal Variances)</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#the-fold-rule"><i class="fa fa-check"></i><b>14.2</b> The Fold Rule</a></li>
<li class="chapter" data-level="14.3" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-hypothesis-test-on-paired-data"><i class="fa fa-check"></i><b>14.3</b> Two Sample Hypothesis Test on Paired Data</a></li>
<li class="chapter" data-level="14.4" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-hypothesis-test-on-proportions"><i class="fa fa-check"></i><b>14.4</b> Two Sample Hypothesis Test on Proportions</a></li>
<li class="chapter" data-level="14.5" data-path="two-sample-hypothesis-tests.html"><a href="two-sample-hypothesis-tests.html#two-sample-hypothesis-test-on-variances"><i class="fa fa-check"></i><b>14.5</b> Two Sample Hypothesis Test on Variances</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="introduction-to-simple-linear-regression.html"><a href="introduction-to-simple-linear-regression.html"><i class="fa fa-check"></i><b>15</b> Introduction to Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="15.1" data-path="introduction-to-simple-linear-regression.html"><a href="introduction-to-simple-linear-regression.html#measures-of-linear-relationship"><i class="fa fa-check"></i><b>15.1</b> Measures of Linear Relationship</a></li>
<li class="chapter" data-level="15.2" data-path="introduction-to-simple-linear-regression.html"><a href="introduction-to-simple-linear-regression.html#least-squares-method"><i class="fa fa-check"></i><b>15.2</b> Least Squares Method</a></li>
<li class="chapter" data-level="15.3" data-path="introduction-to-simple-linear-regression.html"><a href="introduction-to-simple-linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>15.3</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="15.4" data-path="introduction-to-simple-linear-regression.html"><a href="introduction-to-simple-linear-regression.html#sst-sse-and-ssr"><i class="fa fa-check"></i><b>15.4</b> SST, SSE and SSR</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html"><i class="fa fa-check"></i><b>16</b> Inference for Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#inference-on-regression"><i class="fa fa-check"></i><b>16.1</b> Inference on Regression</a>
<ul>
<li class="chapter" data-level="" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#estimating-variance-in-linear-regression"><i class="fa fa-check"></i>Estimating Variance in Linear Regression</a></li>
<li class="chapter" data-level="" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#the-regression-model"><i class="fa fa-check"></i>The Regression Model</a></li>
<li class="chapter" data-level="" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#interpreting-confidence-intervals-for-beta_1"><i class="fa fa-check"></i>Interpreting Confidence Intervals for <span class="math inline">\(\beta_1\)</span></a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#anova-table-analysis-of-variance"><i class="fa fa-check"></i><b>16.2</b> ANOVA Table (ANalysis Of VAriance)</a></li>
<li class="chapter" data-level="16.3" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#residual-plots"><i class="fa fa-check"></i><b>16.3</b> Residual Plots</a>
<ul>
<li class="chapter" data-level="" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#what-to-look-for-in-a-good-residual-plot"><i class="fa fa-check"></i>What to Look for in a Good Residual Plot</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-simple-linear-regression" class="section level1 hasAnchor" number="15">
<h1><span class="header-section-number">Chapter 15</span> Introduction to Simple Linear Regression<a href="introduction-to-simple-linear-regression.html#introduction-to-simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In statistics, simple linear regression (SLR) is a linear regression
model with a single explanatory variable. In other words, we use linear
functions to illustrate the relationship of variables (ie. time and
one’s height). The goal of simple linear regression is to find the
best-fitting straight line, known as the regression line, that predicts
the dependent variable based on the independent variable. For example we
are interested a people’s height within 10 months. Then, we use
coordinate system to draw each data point and use simple linear
regression to find a function which perfectly describes the relationship
between height and time.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="_main_files/figure-html/unnamed-chunk-3-1.png" alt="An illustration of simple linear regression. The blue points are measures of height monthly, and the red line is our SLR model. In this case $m$ is the slope which tells you the rate of change, $b$ is the intercept which may have a special meaning depending on the case." width="90%" />
<p class="caption">
Figure 15.1: An illustration of simple linear regression. The blue points are measures of height monthly, and the red line is our SLR model. In this case <span class="math inline">\(m\)</span> is the slope which tells you the rate of change, <span class="math inline">\(b\)</span> is the intercept which may have a special meaning depending on the case.
</p>
</div>
<p>Now, you may wonder the accuracy of this model. In statistics, we do
have parameters that approximate the slope and intercept of the function
<span class="math inline">\(y = mx + b\)</span>. The model we are going to use is:
<span class="math inline">\(\hat{y} = \hat{\beta_1}x + \hat{\beta_0} + \epsilon\)</span>. From this model,
the slope and the intercept are <span class="math inline">\(\hat{\beta_1}\)</span>, <span class="math inline">\(\hat{\beta_{0}}\)</span>
respectively (<span class="math inline">\(\hat{\beta_1}\)</span> and <span class="math inline">\(\hat{\beta_{0}}\)</span> are unbiased
estimators). Moreover, the <span class="math inline">\(\epsilon\)</span>-term is called error term, which
we will discuss it later.</p>
<div id="measures-of-linear-relationship" class="section level2 hasAnchor" number="15.1">
<h2><span class="header-section-number">15.1</span> Measures of Linear Relationship<a href="introduction-to-simple-linear-regression.html#measures-of-linear-relationship" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we formally introduce simple linear regression, there are some
measures of SLR that should be discussed.<br />
<strong>Covariance (Sample Covariance)</strong></p>
<p>In probability theory and statistics, covariance is a measure of the
joint variability of two random variables. The covariance sign shows the
direction of the linear relationship between two variables. If higher
values of one variable tend to occur with higher values of the other
(and lower with lower), the covariance is positive, meaning the
variables move in the same direction. If higher values of one variable
tend to occur with lower values of the other, the covariance is
negative, meaning they move in opposite directions. The size (magnitude)
of the covariance reflects how much the two variables vary together,
based on the variances they share.</p>
<div class="definition">
<p><span id="def:unlabeled-div-106" class="definition"><strong>Definition 15.1  </strong></span>The formula of sample covariance is given by:
<span class="math display">\[S_{xy} = \frac{1}{n-1} \cdot \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) = \frac{\sum_{i = 1}^{n}x_i \cdot y_i}{n -1} - \frac{n\bar{x}\bar{y}}{n-1}.\]</span>
These are the two ways to compute covariance. Both will give you the
same answer.</p>
</div>
<p>Basically, covariance indicates that how two variables move together.</p>
<ul>
<li><p>If covariance of two random variables is greater than <span class="math inline">\(0\)</span>
(<span class="math inline">\(cov(x,y) &gt; 0\)</span>), then the two random variables show the same trend.
That is: if one random variable is increasing, then the other one is
also increasing; while if one random variable is decreasing, then the
other one is also decreasing.</p></li>
<li><p>If covariance of two random variables is less than <span class="math inline">\(0\)</span>
(<span class="math inline">\(cov(x,y) &lt; 0\)</span>), then the two random variables show the opposite
trend. That is: if one random variable is increasing, then the other
one is decreasing; while if one random variable is decreasing, then
the other one is increasing.</p></li>
<li><p>If covariance of two random variables is equal to <span class="math inline">\(0\)</span>
(<span class="math inline">\(cov(x,y) = 0\)</span>), then we say that there is no relationship
(systematically linear) between the two random variables.</p></li>
</ul>
<p>Note that covariance is not standardized, so it can be difficult to
interpret directly.<br />
<strong>Coefficient of Correlation</strong></p>
<p>In statistics, correlation or dependence is any statistical
relationship, whether causal or not, between two random variables or
bivariate data. It helps us understand whether and how changes in one
variable are associated with changes in another. A positive correlation
means that as one variable increases, the other tends to increase as
well, while a negative correlation means that one variable tends to
decrease as the other increases. The degree of correlation is usually
expressed with a correlation coefficient, which ranges from <span class="math inline">\(-1\)</span> to
<span class="math inline">\(+1\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-107" class="definition"><strong>Definition 15.2  </strong></span>The coefficient of correlation is given by:
<span class="math display">\[r_{xy} = \frac{S_{xy}}{S_x \cdot S_y}.\]</span> Now, <span class="math inline">\(r_{xy} =\)</span> sample
correlation coefficient, <span class="math inline">\(S_{xy} =\)</span> sample covariance, <span class="math inline">\(S_{x} =\)</span> sample
standard deviation of <span class="math inline">\(x\)</span>, <span class="math inline">\(S_{y} =\)</span> sample standard deviation of <span class="math inline">\(y\)</span>.
Also, remember that the range of coefficient of correlation is between
<span class="math inline">\(-1\)</span> and <span class="math inline">\(+1\)</span>: <span class="math inline">\(r_{xy} \in [-1, +1]\)</span>.</p>
</div>
<p>The correlation r measures the strength and direction of the linear
association between two quantitative variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Although you
calculate a correlation for any scatter plot, <span class="math inline">\(r\)</span> measures only
straight-line relationships. In short, coefficient of correlation is a
measure of the strength of the linear relationship between two random
variables.</p>
<ul>
<li><p>If <span class="math inline">\(r_{xy} \approx +1\)</span>, then we say that the two random variables have
a strong positive correlation. (See figure 15.2)</p>
<figure>
<figcaption>
<p>An illustration of strong positive correlation (<span class="math inline"><em>r</em><sub><em>x</em><em>y</em></sub> ≈ +1</span>).</p>
</figcaption>
</figure></li>
<li><p>If <span class="math inline">\(r_{xy} \approx -1\)</span>, then we say that the two random variables have
a strong negative correlation. (See figure 15.3)</p>
<figure>
<figcaption>
<p>An illustration of strong negative correlation (<span class="math inline"><em>r</em><sub><em>x</em><em>y</em></sub> ≈ −1</span>).</p>
</figcaption>
</figure></li>
<li><p>If <span class="math inline">\(r_{xy} \approx 0\)</span>, then we say that there is essentially no
correlation between the two random variables. Note that if
<span class="math inline">\(r \approx 0\)</span>, then it suggests a linear relationship doesn’t exist
but other relationship may exist. (See figure 15.4)</p>
<figure>
<figcaption>
<p>An illustration of no correlation (<span class="math inline"><em>r</em><sub><em>x</em><em>y</em></sub> ≈ 0</span>).</p>
</figcaption>
</figure></li>
</ul>
<p>Note that correlation doesn’t imply causation:</p>
<ul>
<li><p><span class="math inline">\(cor(x,y) \approx +1\)</span> doesn’t necessarily imply on increase in <span class="math inline">\(x\)</span>
causes increase in <span class="math inline">\(y\)</span>.</p></li>
<li><p><span class="math inline">\(cor(x,y) \approx -1\)</span> doesn’t necessarily imply on increase in <span class="math inline">\(x\)</span>
causes decrease in <span class="math inline">\(y\)</span>.</p></li>
</ul>
<p><strong>Properties of Covariance and Correlation</strong></p>
<p>These two values are symmetric:</p>
<ul>
<li><p><span class="math inline">\(cov(x,y) = cov(y,x)\)</span>;</p></li>
<li><p><span class="math inline">\(cor(x,y) = cor(y,x)\)</span>.</p></li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-108" class="example"><strong>Example 15.1  </strong></span>Five observations taken for two variables follow.</p>
<div class="center">
<figure>
<table>
<tbody>
<tr>
<td style="text-align: center;">
<span class="math inline"><em>x</em><sub><em>i</em></sub></span>
</td>
<td style="text-align: center;">
<span class="math inline"><em>y</em><sub><em>i</em></sub></span>
</td>
<td style="text-align: center;">
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">4</span>
</td>
<td style="text-align: center;">
<span class="math inline">50</span>
</td>
<td style="text-align: center;">
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">6</span>
</td>
<td style="text-align: center;">
<span class="math inline">50</span>
</td>
<td style="text-align: center;">
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">11</span>
</td>
<td style="text-align: center;">
<span class="math inline">40</span>
</td>
<td style="text-align: center;">
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">3</span>
</td>
<td style="text-align: center;">
<span class="math inline">60</span>
</td>
<td style="text-align: center;">
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">16</span>
</td>
<td style="text-align: center;">
<span class="math inline">30</span>
</td>
<td style="text-align: center;">
</td>
</tr>
</tbody>
</table>
<figcaption>
Data of example 15.1
</figcaption>
</figure>
</div>
<p><span class="math inline">\(a\)</span> Compute the sample covariance.</p>
<p><span class="math inline">\(b\)</span> Compute and interpret the sample correlation coefficient.</p>
<p><strong>Solution:</strong></p>
<p>Step 1: Compute <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span>; <span class="math inline">\(\bar{x} = 8\)</span> and
<span class="math inline">\(\bar{y} = 46\)</span> (check this by yourself).</p>
<p>Step 2: Find <span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span>.</p>
<p><span class="math display">\[s_x^2 = \frac{1}{5-1} \cdot \sum_{i = 1}^{5}(x_i - \bar{x})^2 = 29.5 \text{ and } s_y^2 = \frac{1}{5-1} \cdot \sum_{i=1}^{5}(y_i - \bar{y})^2 = 130\]</span>
Then: <span class="math inline">\(s_x = 5.4313\)</span> and <span class="math inline">\(s_y = 11.4017\)</span>.</p>
<p>Step 3: Find <span class="math inline">\(s_{xy}\)</span> and <span class="math inline">\(r\)</span>.</p>
<p><span class="math display">\[\sum_{i=1}^{5}x_i \cdot y_i = 1600, \text{ then } s_{xy} = \frac{1600}{5-1} - \frac{5\cdot8\cdot46}{5-1} = -60 \text{ and } r_{xy} = \frac{s_{xy}}{s_x \cdot s_y} = 0.9688.\]</span></p>
<p><strong>R code</strong></p>
<p>Step 1: Entering data;</p>
<div class="tcolorbox">
<pre><code>X=c(4,6,11,3,16); 

Y=c(50,50,40,60,30);</code></pre>
</div>
<p>Step 2: Finding means;</p>
<div class="tcolorbox">
<pre><code>mean(X);

mean(Y);</code></pre>
</div>
<p>Step 3: Finding variances;</p>
<div class="tcolorbox">
<pre><code>var(X);

var(Y);</code></pre>
</div>
<p>Step 4: Finding standard deviations;</p>
<div class="tcolorbox">
<pre><code>sd(X);

sd(Y):</code></pre>
</div>
<p>Step 5: Finding covariance and correlation;</p>
<div class="tcolorbox">
<pre><code>cov(X,Y);

cor(X,Y);</code></pre>
</div>
</div>
</div>
<div id="least-squares-method" class="section level2 hasAnchor" number="15.2">
<h2><span class="header-section-number">15.2</span> Least Squares Method<a href="introduction-to-simple-linear-regression.html#least-squares-method" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The method of least squares is a mathematical optimization technique
used to find the best-fitting function by minimizing the sum of the
squared differences between the observed data points and the values
predicted by the model. It interested in a linear model of the form:
<span class="math inline">\(y  = \beta_0 + \beta_1 \cdot x_1 + \cdots + \beta_p \cdot x_p + \epsilon\)</span>,
where <span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span>, <span class="math inline">\(x_i\)</span>’s (<span class="math inline">\(i = 1,..., p\)</span>) are
independent predictors, <span class="math inline">\(\beta_j\)</span>’s (<span class="math inline">\(j = 0,..., p\)</span>) are coefficients,
<span class="math inline">\(y\)</span> is dependent variable.<br />
Using sample data, we get estimates of this model of the form:
<span class="math inline">\(\hat{y} = \hat{\beta_0} + \hat{\beta_1} \cdot x_1 + \cdots + \hat{\beta_p} \cdot x_p\)</span>.
Now, <span class="math inline">\(\hat{y}\)</span> is predicated <span class="math inline">\(y\)</span>, <span class="math inline">\(x_i\)</span>’s (<span class="math inline">\(i = 1,...,p\)</span>) are
independent predicators, <span class="math inline">\(\hat{\beta_j}\)</span>’s (<span class="math inline">\(j = 0,...,p\)</span>) are estimated
coefficients. Moreover, <span class="math inline">\(\hat{\beta_0}\)</span> is intercept;
<span class="math inline">\(\hat{\beta_1},...,\hat{\beta_p}\)</span> are quantifiers how much <span class="math inline">\(y\)</span> changes
with a unit increase in <span class="math inline">\(x_i\)</span>’s.<br />
In this course, we focus on the following model a bit more:
<span class="math inline">\(\hat{y} = b_0 + b_1\cdot x\)</span>, where <span class="math inline">\(b_0\)</span> is the y-intercept, and <span class="math inline">\(b_1\)</span>
is the slope, and <span class="math inline">\(\hat{y}\)</span> is the value of <span class="math inline">\(y\)</span> determined by the line.
The coefficients <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are derived using Calculus so that we
minimize the sum of squared deviations:
<span class="math inline">\(\sum_{i=1}^{n}(y_i - \hat{y_i})^2\)</span>. Then the least squares line
coefficients are <span class="math inline">\(b_1 = r \cdot \frac{s_y}{s_x}\)</span> and
<span class="math inline">\(b_0 = \bar{y} - b_1\cdot \bar{x}\)</span>.<br />
<strong>Facts about Least Squares Method</strong></p>
<ul>
<li><p>The distinction between explanatory and response variables is
essential in Least Squares Method.</p></li>
<li><p>The least-squares line (trendline) always passes through the point
(<span class="math inline">\(\bar{x}\)</span>, <span class="math inline">\(\bar{y}\)</span>) on the graph of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span>.</p></li>
<li><p>The square of the correlation, <span class="math inline">\(r^2\)</span>, is the fraction of the variation
in the values of <span class="math inline">\(y\)</span> that is explained by the variation in <span class="math inline">\(x\)</span>.</p></li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-109" class="example"><strong>Example 15.2  </strong></span>A tool die maker operates out of a small shop making specialized tools.
He is considering increasing the size of his business and needs to know
more about his costs. One such cost is electricity, which he needs to
operate his machines and lights. He keeps track of his daily electricity
costs and the number of tools that he made that day. These data are
listed next. Determine the fixed and variable electricity costs using
the Least Squares Method.</p>
<div class="center">
<figure>
<table>
<tbody>
<tr>
<td style="text-align: center;">
Day
</td>
<td style="text-align: center;">
Number of tools (<span class="math inline"><em>X</em></span>)
</td>
<td style="text-align: center;">
Electricity costs (<span class="math inline"><em>Y</em></span>)
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">1</span>
</td>
<td style="text-align: center;">
<span class="math inline">7</span>
</td>
<td style="text-align: center;">
<span class="math inline">23.80</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">2</span>
</td>
<td style="text-align: center;">
<span class="math inline">3</span>
</td>
<td style="text-align: center;">
<span class="math inline">11.89</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">3</span>
</td>
<td style="text-align: center;">
<span class="math inline">2</span>
</td>
<td style="text-align: center;">
<span class="math inline">15.89</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">4</span>
</td>
<td style="text-align: center;">
<span class="math inline">5</span>
</td>
<td style="text-align: center;">
<span class="math inline">26.11</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">5</span>
</td>
<td style="text-align: center;">
<span class="math inline">8</span>
</td>
<td style="text-align: center;">
<span class="math inline">31.79</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">6</span>
</td>
<td style="text-align: center;">
<span class="math inline">11</span>
</td>
<td style="text-align: center;">
<span class="math inline">39.93</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">7</span>
</td>
<td style="text-align: center;">
<span class="math inline">5</span>
</td>
<td style="text-align: center;">
<span class="math inline">12.27</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">8</span>
</td>
<td style="text-align: center;">
<span class="math inline">15</span>
</td>
<td style="text-align: center;">
<span class="math inline">40.06</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">9</span>
</td>
<td style="text-align: center;">
<span class="math inline">3</span>
</td>
<td style="text-align: center;">
<span class="math inline">21.38</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">10</span>
</td>
<td style="text-align: center;">
<span class="math inline">6</span>
</td>
<td style="text-align: center;">
<span class="math inline">18.65</span>
</td>
</tr>
</tbody>
</table>
<figcaption>
Data of example 15.2
</figcaption>
</figure>
</div>
<p>Solution:</p>
<p>Step 1: Entering Data;</p>
<div class="tcolorbox">
<pre><code>tools=c(7,3,2,5,8,11,5,15,3,6);

cost=c(23.80,11.89,15.98,26.11,31.79, 39.93,12.27,40.06,21.38,18.65);</code></pre>
</div>
<p>Step 2: Finding Slope;</p>
<div class="tcolorbox">
<pre><code>Sx=sd(tools); 

Sy=sd(cost); 

r=cor(tools,cost); 

b1=r*(Sy/Sx); 

b1;
 
## [1] 2.245882</code></pre>
</div>
<p>Step 3: Finding <span class="math inline">\(y\)</span>-intercept;</p>
<div class="tcolorbox">
<pre><code>x.bar=mean(tools); 

y.bar=mean(cost);

b0=y.bar - b1*x.bar;

b0; 

## [1] 9.587765</code></pre>
</div>
<p>We can also use R-code to draw a graph:</p>
<div class="tcolorbox">
<pre><code>plot(tools,cost,pch=19);

abline(least.squares$coeff,col=&quot;red&quot;);

# pch=19 tells R to draw solid circles; 

# abline tells R to add trendline;</code></pre>
</div>
<p>Interpretation:</p>
<p>The slope measures the marginal rate of change in the dependent
variable. In this example, the slope is <span class="math inline">\(2.25\)</span>, which means that in this
sample, for each one-unit increase in the number of tools, the marginal
increase in the electricity cost is <span class="math inline">\(\$ 2.25\)</span> per tool.</p>
<p>The <span class="math inline">\(y\)</span>-intercept is <span class="math inline">\(9.57\)</span>; that is, the line strikes the <span class="math inline">\(y\)</span>-axis at
<span class="math inline">\(9.57\)</span>. However, when <span class="math inline">\(x = 0\)</span>, we are producing no tools and hence the
estimated ﬁxed cost of electricity is <span class="math inline">\(\$9.57\)</span> per day .</p>
</div>
</div>
<div id="simple-linear-regression" class="section level2 hasAnchor" number="15.3">
<h2><span class="header-section-number">15.3</span> Simple Linear Regression<a href="introduction-to-simple-linear-regression.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Estimating Regression Model Parameters</strong></p>
<p>The regression line which we are going to use is:
<span class="math inline">\(E(Y) = \beta_0 + \beta_1 \cdot x\)</span>. This is fitted to the data points
<span class="math inline">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span> by finding the line that is
closest to the data in some sense. There are many ways in which
closeness can be defined, but the method most generally used is to
consider the vertical deviations between the line and the data points:
<span class="math inline">\(y_i - (\beta_0 + \beta_1 \cdot x_i), 1 \leq i \leq n\)</span>.<br />
The fitted line is chosen to be the line that minimizes the sum of the
squares of these vertical deviations
<span class="math display">\[Q = \sum_{i=1}^{n} [y_i - (\beta_0 + \beta_1 \cdot x_i)]^2\]</span> and this
is referred to as the least squares fit. (The quantity <span class="math inline">\(Q\)</span> is also
called the <strong>sum of squares for error</strong>, SSE.)<br />
The parameter estimates <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are
therefore the values that minimize the quantity <span class="math inline">\(Q\)</span>. They are found
taking partial derivatives of <span class="math inline">\(Q\)</span> with respect to <span class="math inline">\(\hat{\beta_0}\)</span> and
<span class="math inline">\(\hat{\beta_1}\)</span> and setting the resulting expressions equal to <span class="math inline">\(0\)</span>.<br />
Now, you know the method to get the regression model
<span class="math inline">\(E(Y) = \beta_0 + \beta_1 \cdot x\)</span>, the following lines are the
derivation:</p>
<figure>
<figcaption>
An illustration of a simple linear regression model. The red
line is the fitted regression line, and the full black vertical lines
represent the residuals (SSE). The data points deviate from the line to
show errors clearly. Note that the sum of residuals is necessarily <span class="math inline">0</span>.
</figcaption>
</figure>
<p>The following proof is the derive of <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>
for simple linear regression model:
<span class="math inline">\(\hat{y} = \hat{\beta_0} + \hat{\beta_1} \cdot x + \epsilon\)</span>.</p>
<div class="proof">
<p><span id="unlabeled-div-110" class="proof"><em>Proof</em>. </span><em>Proof.</em> <span class="math display">\[\begin{aligned}
\intertext{Firstly, we examine sum of residual squared:}\\
\sum_{i=1}^{n}e_i^2 &amp;= \sum_{i=1}^{n}(y_i - \hat{y_i})^2 = 0\\
&amp;= \sum_{i=1}^{n}[y_i - (\hat{\beta_0} + \hat{\beta_1} \cdot x_i)]^2 = 0\\
&amp;= \sum_{i=1}^{n}(y_i - \hat{\beta_0} - \hat{\beta_1} \cdot x_i)^2 = 0\\
\end{aligned}\]</span> To find <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> which
minimizes <span class="math inline">\(\sum_{i=1}^{n}e_i^2\)</span>, we need partial derivative with respect
to <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>: <span class="math display">\[\begin{aligned}
\frac{\partial}{\partial \hat{\beta_0}}\sum_{i=1}^{n}e_i^2 &amp;= \frac{\partial}{\partial \hat{\beta_0}}\sum_{i=1}^{n}(y_i - \hat{\beta_0} - \hat{\beta_1} \cdot x_i)^2 = 0\\
&amp;= \sum_{i=1}^{n}2(\hat{\beta_0} + \hat{\beta_1}x_i - y_i) = 0\\
&amp;= \sum_{i=1}^{n}\hat{\beta_0} + \hat{\beta_1}\sum_{i=1}^{n}x_i - \sum_{i=1}^{n}y_i = 0 \text{; } (\text{consider:} \sum_{i=1}^{n}x_i = n\bar{x})\\
&amp;= n\cdot\hat{\beta_0} + \hat{\beta_1}\cdot n \bar{x} - n\bar{y} = 0\\
\text{Hence: } \hat{\beta_0} &amp;= \bar{y} - \hat{\beta_1}\cdot \bar{x}. \text{ (Equ 1)}\\
\frac{\partial}{\partial \hat{\beta_1}}\sum_{i=1}^{n}e_i^2 &amp;= \frac{\partial}{\partial \hat{\beta_1}}\sum_{i=1}^{n}(y_i - \hat{\beta_0} - \hat{\beta_1} \cdot x_i)^2 = 0\\
&amp;= \sum_{i=1}^{n}2(y_i - \hat{\beta_0} - \hat{\beta_1}x_i)(-x_i)=0\\
&amp;= \sum_{i=1}^{n}2(\hat{\beta_0}x_i + \hat{\beta_1}x_i^2 - y_ix_i)=0\\
&amp;= \hat{\beta_0}\sum_{i=1}^{n}x_i + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 - \sum_{i=1}^{n}x_iy_i = 0\\
&amp;= \hat{\beta_1}\sum_{i=1}^{n}x_i^2 + \sum_{i=1}^{n}(\bar{y} - \hat{\beta_1}\bar{x})x_i - \sum_{i=1}^{n}x_iy_i = 0 \text{; } (\text{sub Equ 1 into this line})\\
&amp;= \hat{\beta_1}\sum_{i=1}^{n}x_i^2 + \bar{y}\sum_{i=1}^{n}x_i - \hat{\beta_1}\bar{x}\sum_{i=1}^{n}x_i - \sum_{i=1}^{n}x_iy_i = 0\\
&amp;= \hat{\beta_1}\sum_{i=1}^{n}x_i^2 + n\bar{x}\bar{y} - n\hat{\beta_1}(\bar{x})^2 - \sum_{i=1}^{n}x_iy_i = 0\\
&amp;= \hat{\beta_1}[\sum_{i=1}^{n}x_i^2 - n(\bar{x})^2] + n\bar{x}\bar{y} - \sum_{i=1}^{n}x_iy_i = 0\\
\text{Hence: } \hat{\beta_1} &amp;= \frac{\sum_{i=1}^{n}x_iy_i - n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_i^2 - n(\bar{x})^2} = \frac{S_{xy}}{S_{xx}}.
\end{aligned}\]</span> ◻</p>
</div>
<p><strong>Introduction to Simple Linear Regression</strong></p>
<p>At this point, we are going to provide the definition of simple linear
regression model as the following:</p>
<div class="definition">
<p><span id="def:unlabeled-div-111" class="definition"><strong>Definition 15.3  </strong></span>Let <span class="math inline">\(x\)</span> be independent variable and <span class="math inline">\(y\)</span> be dependent variable, then the
model of simple linear regression is:
<span class="math inline">\(\hat{y} = \hat{\beta_0} + \hat{\beta_1} \cdot x + \epsilon\)</span>, where
<span class="math inline">\(\hat{\beta_0}\)</span> represents the <span class="math inline">\(y\)</span>-intercept, <span class="math inline">\(\hat{\beta_1}\)</span> represents
the slope and <span class="math inline">\(\epsilon\)</span> is the error term that
<span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span>. Moreover,
<span class="math inline">\(\hat{\beta_0} = \bar{y} - \hat{\beta_1}\cdot \bar{x}\)</span> and
<span class="math inline">\(\hat{\beta_1} = \frac{S_{xy}}{S_{xx}}\)</span>, which is also equal to:
<span class="math display">\[\frac{\sum_{i=1}^{n}x_iy_i - n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_i^2 - n(\bar{x})^2}.\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-112" class="example"><strong>Example 15.3  </strong></span>Suppose an appliance store conducts a 5-month experiment to determine
the effect of advertising on sales revenue. The results are shown in a
table below. The relationship between sales revenue, <span class="math inline">\(y\)</span>, and
advertising expenditure, <span class="math inline">\(x\)</span>, is hypothesized to follow a ﬁrst-order
linear model, that is, <span class="math inline">\(y = \beta_0 + \beta_1\cdot x + \epsilon\)</span>, where
<span class="math inline">\(y =\)</span> dependent variable, <span class="math inline">\(x =\)</span> independent variable, <span class="math inline">\(\beta_0\)</span>
y-intercept, <span class="math inline">\(\beta_1 =\)</span> slope of the line and <span class="math inline">\(\epsilon =\)</span> error
variable.</p>
<div class="center">
<figure>
<table>
<tbody>
<tr>
<td style="text-align: center;">
Month
</td>
<td style="text-align: center;">
Advertising Expenditure <span class="math inline"><em>x</em></span> ($ hundreds)
</td>
<td style="text-align: center;">
Sales Revenue <span class="math inline"><em>y</em></span> ($ thousands)
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">1</span>
</td>
<td style="text-align: center;">
<span class="math inline">1</span>
</td>
<td style="text-align: center;">
<span class="math inline">1</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">2</span>
</td>
<td style="text-align: center;">
<span class="math inline">2</span>
</td>
<td style="text-align: center;">
<span class="math inline">1</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">3</span>
</td>
<td style="text-align: center;">
<span class="math inline">3</span>
</td>
<td style="text-align: center;">
<span class="math inline">2</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">4</span>
</td>
<td style="text-align: center;">
<span class="math inline">4</span>
</td>
<td style="text-align: center;">
<span class="math inline">2</span>
</td>
</tr>
<tr>
<td style="text-align: center;">
<span class="math inline">5</span>
</td>
<td style="text-align: center;">
<span class="math inline">5</span>
</td>
<td style="text-align: center;">
<span class="math inline">4</span>
</td>
</tr>
</tbody>
</table>
<figcaption>
Data of example 15.3
</figcaption>
</figure>
</div>
<p>a) Obtain the least squares estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, and
state the estimated regression function.</p>
<p>b) Plot the estimated regression function and the data.</p>
<p><strong>Solution:</strong></p>
<p>a) <span class="math inline">\(\bar{x} = 3\)</span>, <span class="math inline">\(\bar{y} = 2\)</span>, <span class="math inline">\(S_{xx} = 10\)</span>, <span class="math inline">\(S_{xy} = 7\)</span></p>
<p>Then, the slope of the least squares line is
<span class="math inline">\(\hat{\beta_1} = \frac{S_{xy}}{S_{xx}} = 0.7\)</span> and
<span class="math inline">\(\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x} = -0.1\)</span>. Thus, the least
squares line is <span class="math inline">\(\hat{y} = -0.1 + 0.7x\)</span>.</p>
<p>b) R-code</p>
<div class="tcolorbox">
<pre><code>plot(x, y, main=&quot;Scatterplot: Simple Linear Regression&quot;,
xlab=&quot;x&quot;, ylab=&quot;y&quot;, pch=19,col=&quot;blue&quot;);

abline(coef(linear.reg), col=&quot;red&quot;,lty=2);</code></pre>
</div>
</div>
</div>
<div id="sst-sse-and-ssr" class="section level2 hasAnchor" number="15.4">
<h2><span class="header-section-number">15.4</span> SST, SSE and SSR<a href="introduction-to-simple-linear-regression.html#sst-sse-and-ssr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Early in Section <span class="math inline">\(15.3\)</span>, we introduced a value called the sum of
residual squared (SSE). There are two more values that are important in
simple linear regression, which are total sum of squares (SST) and sum
of squares for regression (SSR). We will introduce all these three
values with figures, so that you may have a better understanding of what
they measure.<br />
<strong>SST (Total Sum of Squares)</strong></p>
<p>It is defined as the sum over all squared differences between the
observations and their overall mean <span class="math inline">\(\bar{y}\)</span>.</p>
<figure>
<figcaption>
An illustration of Total Sum of Squares (SST). The blue
points represent height measurements over time, the dashed line is the
mean height <span class="math inline"><em>ȳ</em></span>, and the solid
black vertical lines represent the squared deviations from the mean (SST
components).
</figcaption>
</figure>
<div class="definition">
<p><span id="def:unlabeled-div-113" class="definition"><strong>Definition 15.4  </strong></span>For any simple linear regression model, SST (Toal sum of squares)
measures the sum over all squared differences between the observations
and their overall mean <span class="math inline">\(\bar{y}\)</span> is given by:
<span class="math display">\[SST = \sum_{i=1}^{n}(y_i - \bar{y})^2.\]</span></p>
</div>
<p><strong>SSE (Sum of Residual Squared)</strong></p>
<p>It is the sum of the squares of residuals (deviations predicted from
actual empirical values of data). It is a measure of the discrepancy
between the data and an estimation model, such as a linear regression. A
small SSE indicates a tight fit of the model to the data.</p>
<figure>
<figcaption>
A simple linear regression illustration with residuals
shown. Blue points are observed data, red line is the model, dashed line
is the average of <span class="math inline"><em>y</em></span>, and black
lines represent residuals <span class="math inline"><span class="math inline">\(y_i -
\hat{y_i}\)</span></span>.
</figcaption>
</figure>
<div class="definition">
<p><span id="def:unlabeled-div-114" class="definition"><strong>Definition 15.5  </strong></span>For any simple linear regression model, SSE (Sum of residual squared)
measures the distance between observed data and estimated data, which is
given by: <span class="math display">\[SSE = \sum_{i=1}^{n} (y_i - \hat{y_i})^2.\]</span></p>
</div>
<p>Note that SSE (sum of residual squared) is an explained variation.<br />
<strong>SSR (Sum Square Regression)</strong></p>
<p>It measures the distance between estimated value (estimated dependent
data) and the mean of dependent data (<span class="math inline">\(\bar{y}\)</span>).</p>
<figure>
<figcaption>
An illustration of simple linear regression. The blue points
represent monthly height measurements with added variability. The red
line is the fitted simple linear regression model. The dashed line shows
the mean of the dependent variable, <span class="math inline"><em>ȳ</em></span>, and the solid black lines
illustrate the deviation of the model’s predictions from this
mean.
</figcaption>
</figure>
<div class="definition">
<p><span id="def:unlabeled-div-115" class="definition"><strong>Definition 15.6  </strong></span>For any simple linear regression, the distance between the mean of
dependent value and estimated dependent value is called sum square
regression (SSR), which is given by
<span class="math display">\[SSR = \sum_{i=1}^{n}(\hat{y_i} - \bar{y})^2.\]</span></p>
</div>
<p>Note that sum square regression (SSR) is an explained variation.<br />
<strong>Summery</strong></p>
<p>Now, let’s zoom in to see SST, SSE and SSR. Note that the relationship
of these measures is that total deviation is equal to unexplained
deviation (error) plus explained deviation (regression), that is:
<span class="math inline">\((y_i - \bar{y}) = (y_i - \hat{y_i}) + (\hat{y_i} - \bar{y})\)</span>.<br />
We square all three deviations for each one of our data points, and sum
over all <span class="math inline">\(n\)</span> points. Here, cross terms drop out, and we are left with
the following equation:
<span class="math display">\[\sum_{i=1}^{n}(y_i - \bar{y})^2 = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 + \sum_{i=1}^{n}(\hat{y_i} - \bar{y})^2,\]</span>
SST = SSE + SSR.</p>
<p>Total sum of squares = Sum of squares for error + Sum of squares for
regression.</p>
<figure>
<figcaption>
Visual representation of regression variability.
<strong>SST</strong> (green) is total variability from the mean,
<strong>SSR</strong> (pink) is explained variability, and
<strong>SSE</strong> (orange curly brace) is the residual (unexplained)
variability between the observed value <span class="math inline"><em>y</em><sub><em>i</em></sub></span> and the
prediction <span class="math inline"><em>ŷ</em><sub><em>i</em></sub></span>.
</figcaption>
</figure>
<p><strong>Coefficient of Determination (<span class="math inline">\(r^2\)</span>)</strong></p>
<p>Moreover, we can use SST, SSE and SSR to calculate another value which
is important in simple linear regression, that is coefficient of
determination. It is proportion of variability in <span class="math inline">\(y\)</span> which is explained
by <span class="math inline">\(x\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-116" class="definition"><strong>Definition 15.7  </strong></span>We define the coefficient of determination as the sum of squares due to
the regression divided by the total sum of squares.
<span class="math display">\[r^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}.\]</span> The coefficient of
determination can be interpreted as the proportion of the variation in
<span class="math inline">\(Y\)</span> that is explained by the regression relationship of <span class="math inline">\(Y\)</span> with <span class="math inline">\(X\)</span> (or
the proportion of the total corrected sum of squares explained by the
regression). Note that: <span class="math inline">\(0 \leq r^2 \leq 1.\)</span></p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="two-sample-hypothesis-tests.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference-for-simple-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
