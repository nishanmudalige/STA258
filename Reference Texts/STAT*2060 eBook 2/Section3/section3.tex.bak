\setcounter{equation}{0}

\chapter{Probability}
\label{sec.inverse} \pagestyle{myheadings}
\markboth{\ref{sec.inverse}. \titleref{sec.inverse}}{}
\section{Basic Definitions}

\subsection{Preliminaries}

%\begin{frame}[<+->] \frametitle{Preliminaries} \pause

An \textbf{experiment} is the occurrence of an observation that is not predictable.  We will use
this definition for now but we will call it something else next week. %\pause

\smallskip

A \textbf{sample point} is the basic realization of an experiment. %\pause

\smallskip

The \textbf{sample space} is the collection of all sample points.%\pause

\smallskip

We attach a \textbf{probability} to each sample point, $p_i$, say for $i=1,\dots ,n$ sample points
and we insist that %\pause
\begin{itemize}
\item $0 \leq p_i \leq 1$ for all $i=1,\dots ,n$
\item $\sum_{i=1}^np_i = 1$
\end{itemize}


\iffalse %\fi line 147

\begin{frame}

\vspace{-.4 cm}
\includegraphics[width=11.5cm]{phew}

\end{frame}

\begin{frame}[<+->] \frametitle{Again} \pause

An \alert{experiment} is the occurrence of an observation that is not predictable.  We will use
this definition for now but we will call it something else next week. \pause

\smallskip

A \alert{sample point} is the basic realization of an experiment. \pause

\smallskip

The \alert{sample space} is the collection of all sample points.\pause

\smallskip

We attach a \alert{probability} to each sample point, $p_i$, say for $i=1,\dots ,n$ sample points
and we insist that \pause
\begin{itemize}
\item $0 \leq p_i \leq 1$ for all $i=1,\dots ,n$
\item $\sum_{i=1}^np_i = 1$
\end{itemize}

\end{frame}

\begin{frame}[<+->] \frametitle{One more time} \pause

An \alert{experiment} is the occurrence of an observation that is not predictable.  We will use
this definition for now but we will call it something else next week. \pause

\smallskip

A \alert{sample point} is the basic realization of an experiment. \pause

\smallskip

The \alert{sample space} is the collection of all sample points.\pause

\smallskip

We attach a \alert{probability} to each sample point, $p_i$, say for $i=1,\dots ,n$ sample points
and we insist that \pause
\begin{itemize}
\item $0 \leq p_i \leq 1$ for all $i=1,\dots ,n$
\item $\sum_{i=1}^np_i = 1$
\end{itemize}

\end{frame}

\fi



Will  be given in class.




%\begin{frame}[<+->] \frametitle{Events}

An \textbf{event} is a collection of sample point(s). % \pause

\smallskip

They are distinguished by a \textbf{simple event}, which consists only of one
sample point,  %\pause

\smallskip

as opposed to more than one sample point called a \textbf{compound event}.  %\pause

\smallskip

If we denote some event, either simple or compound, then the \textbf{probability of an event} is determined by adding the
probabilities for each of the sample point(s) in the event.



Will  be given in class.

%\end{frame}

%\begin{frame}[<+->] \frametitle{Diagrams}

Two very effective ways of illustrating these concepts is through what are called

\begin{itemize}

\item[1.] \textbf{trees}

\item[2.] \textbf{Venn diagrams}

\end{itemize} %\pause

Both are abstractions of these concepts that provide details (in different ways) of what  underlies the problem.

%\end{frame}

%\begin{frame}[<+->] \frametitle{Examples}

Will  be given in class.

%\end{frame}
\input{exercise1a.tex}

\section{Compound Events}

\subsection{Unions Intersections}

%\begin{frame}[<+->] \frametitle{Unions Intersections}

We always begin with $S$ the sample space.%\pause

\medskip

Given two events $A$ and $B$ say, we can create a third event in several ways.%\pause

\medskip

One way is include all sample points in $A$, or $B$ (or both) which we symbolically write as
$A \cup B$ which we refer to as
\textbf{union}.% \pause

\medskip

Another way is to include all sample points common to both $A$ and $B$ which we symbolically write as
$A \cap B$ which we refer to as
\textbf{intersection}.% \pause

\medskip

%\end{frame}

%\begin{frame}[<+->] \frametitle{ }

%\vspace{- 1 cm}
\includegraphics[height=12cm]{a2-fig-1}

%\end{frame}
\subsection{Two-way table}

%\begin{frame} %[shrink=5]
%frametitle{Two-way table}

\noindent A very informative procedure for representing probabilities is
a \textbf{two way table}.%\pause

\begin{center}
\begin{tabular}{|c|c|c|c|c|}\hline
$B/A$&$A_1$&$\cdots$&$A_k$&total\\ \hline
$B_1$&*&$\cdots$&*&*\\ \hline
$\cdots$&$\cdots$&$\cdots$&$\cdots$&$\cdots$\\ \hline
$B_l$&*&$\cdots$&*&*\\ \hline
total&*&$\cdots$&*&*\\ \hline
\end{tabular}
\end{center}

%\end{frame}

%\begin{frame} %[shrink=5]
%\frametitle{Example of a two-way table}

\noindent Suppose we want to measure income against age.  In one group we have
under 50 group, and 50 and over group.  We can measure income as less than
\$50,000, and \$50,000 and over.

\begin{center}
\begin{tabular}{|c|c|c|c|}\hline
Income/Age&$A_1=\{< 50\}$&$A_2=\{\geq 50\}$&total\\ \hline
$B_1=\{< \$50K\}$&40&10&50 \\ \hline
$B_2=\{\geq \$50K\}$&5&45&50\\ \hline
total&45&55&100\\ \hline
\end{tabular}
\end{center}

%\end{frame}
\subsection{Complementary Events}

%\begin{frame} \frametitle{Complementary Events}%\pause

For some event $A$ we denote it's \textbf{complementary event} by $A^c$, and what this means is all elements of
$S$ that are not in, or complementary to $A$.% \pause

\medskip

We note that the probabilities of an event and it's complementary event is
\[
P(A)+P(A^c) = 1.
\]


%\end{frame}

%\begin{frame}[<+->] \frametitle{Examples}

Will  be given in class.

%\end{frame}
\subsection{Mutually exclusive}

%\begin{frame} \frametitle{Mutually exclusive}%\pause

We have what is called the \textbf{additive rule} which asserts that given two events $A$ and $B$ if we form
a third event $A\cup B$ then
\[P\left(A\cup B\right) = P(A) + P(B) - P\left(A\cap B\right).\]%\pause

\medskip

Two events $A$ and $B$ are \textbf{mutually exclusive} if $A\cap B$ has no sample points.  In this case
$P(A\cap B) =0$ so that the \textbf{probability of mutually exclusive events}
\[P\left(A\cup B\right) = P(A) + P(B) .\]

%\end{frame}

%\begin{frame}[<+->] \frametitle{Examples}

%\vspace{- 1.5 cm}
\includegraphics[height=12cm]{a2-fig-1}
%\end{frame}

%\begin{frame}[<+->] \frametitle{ }

%\vspace{- 1 cm}
\includegraphics[height=12cm]{a2-fig-3}

%\end{frame}

%\begin{frame}[<+->] \frametitle{ }

%\vspace{- 1 cm}
\includegraphics[height=12cm]{a2-fig-2}

%\end{frame}
\input{exercise1b.tex}

\iffalse
\subsection{Exercises}

%\begin{frame}\frametitle{Exercises}

\noindent Exercises:

\bigskip

\noindent 11th edition, p. 127:  3.1-3.3, 3.6, 3.7

\bigskip

\noindent 10th edition, p. 143:  3.1-3.3, 3.6, 3.7


\end{frame}
\fi
\section{Conditional Probability and Independence}

\subsection{Conditional Probability}

%\begin{frame}
%\frametitle{Conditional Probability}%\pause

For events $A$ and $B$ we have the notion of a
\textbf{conditional probability} of $A$ given that the event $B$ has occurred.%\pause

\medskip

We can express this through the following
\[
P\left(A| B\right) = \frac{P\left(A\cap B\right)}{P(B)},
\]
where it is assumed that $P(B) \neq 0$.

%\end{frame}

%\begin{frame}[<+->] \frametitle{Examples}

%\vspace{- 1 cm}
\includegraphics[height=12cm]{a2-fig-2}

%\end{frame}






\iffalse
In the previous chapter, a system of linear equations was written in
matrix notation as $A{\bf{x}} = {\bf {b}}$.  If we were dealing
with numbers, rather than vectors and matrices, we would write
$ax=b$ and the solution (for $x$) would be $x = a^{-1}b$ provided
$a\neq0$.  It is natural to ask whether the equation $A{\bf{x}} =
{\bf{b}}$ has a solution ${\bf{x}} = A^{-1}{\bf{b}}$ for some
matrix $A^{-1}$.  The answer depends on the existence of a matrix
$A^{-1}$ with the property $A^{-1}A = AA^{-1} = I$.  Such a matrix
is called the inverse of $A$.  We will see how to calculate this
(when possible) and we will examine some of its properties.  The
determinant of a matrix will be introduced and some of the
properties of the determinant will be explored.  The determinant
gives an easy method for determining the existence of an inverse.

\noindent {\bf Learning Objectives}
\noindent After completing this unit, you should be able to:
\begin{itemize}
\item understand what is meant by $A^{-1}$, and know how to find
whether one matrix is the inverse of another
\item define the words invertible, and singular
\item find the inverse of a matrix and know the properties of the inverse
\item relate invertibility to the solution of linear systems
\item find the determinant of a matrix
\item know the properties of the determinant
\item relate the value of the determinant to the invertibility of
a matrix.
\end{itemize}

\section{Definition of an Inverse}
\label{ssec.definv}\markright{\ref{ssec.definv}
\titleref{ssec.definv}}

In this section we define the inverse of a matrix. This
concept of invertibility will be very important throughout the
rest of the course.

Let $A$ be a square matrix. If there exists a $B$ such that
$AB=BA=I$, then $A$ is said to be {\bf invertible}\index{invertible}. Furthurmore,
$B$ is called an {\bf inverse}\index{inverse} of $A$.

Lets assume that the inverse of a matrix is not unique.  That is,
more than one $B$ exists such that $AB=BA=I$. For the matrix $A_{n
\times n}$, let two such inverses be $B_{n \times n}$ and $C_{n
\times n}$. We must have $(BA)=(AB)=I$ and $(CA)=(AC)=I$, since
$B$ and $C$ are both inverses of $A$. Now,
\begin{eqnarray*}
C=IC&=&(BA)C\\&=&B(AC)=BI=B.
\end{eqnarray*}
Therefore the two inverses must be equal, and the inverse unique.

We have just shown that an inverse is unique. Hence, we speak of
`the inverse' and can denote the inverse by $A^{-1}$ without
ambiguity. This notation is intuitive; $A^{-1}$ plays an analogous
role to the real number $a^{-1} = \frac{1}{a}$, (when $a\neq0$)
since $aa^{-1}=1$ and $AA^{-1}=I$.

An important feature of invertible mattrices is that they can be
cancelled from equations:

If $AB = AC$ and $A$ is invertible, then multiplying on the left
by $A^{-1}$ gives

\begin{eqnarray*}
A^{-1}(AB) & = & A^{-1}(AC)\\
\mathrm{i.e.}\ (A^{-1}A)B & = & (A^{-1}A)C\\
\mathrm{i.e.}\ IB = IC&\mathrm{or}&B = C
\end{eqnarray*}

We have seen that B is the inverse of A $(B=A^{-1})$ if
$AB=BA=I$.  This same condition would show that A is the inverse of B
$(A=B^{-1})$.  Hence, $B=A^{-1}\Leftrightarrow A=B^{-1}$ and we say A and B
are inverses (of one another).
\begin{example}
\label{exam3.multinv} Are $A$ and $B$ inverses?
 $$ A =\begin{bmatrix} -2 & 3 \\ -3 & 4
\end{bmatrix} \quad B = \begin{bmatrix} 4 & -3 \\ 3 & -2 \end{bmatrix}$$
\begin{align*}
AB&= \begin{bmatrix}-2&3\\-3&4 \end{bmatrix}\,\begin{bmatrix}4&-3\\3&-2 \end{bmatrix}
=\begin{bmatrix}1&0\\0&1 \end{bmatrix}
\intertext{and}
BA&= \begin{bmatrix}4&-3\\3&-2 \end{bmatrix}\,\begin{bmatrix}-2&3\\-3&4\end{bmatrix}
=\begin{bmatrix}1&0\\ 0&1 \end{bmatrix}.
\end{align*}
Therefore $A$ and $B$ are inverses.
\end{example}
In the definition above, it was stated that $B$ will be the
inverse if it exists.  Not all matrices will have inverses. A
matrix which does not have an inverse is called a {\bf singular}\index{singular}
matrix.  The following example gives a singular matrix.

\begin{example}\label{exam3.noinv}
Consider the matrix
$$\begin{bmatrix}2&7&0\\6&4&0\\3&1&0 \end{bmatrix}.$$
Assume that $B$ is its inverse. Thus,
$$\begin{bmatrix}1&0&0\\0&1&0\\0&0&1 \end{bmatrix}=
  \begin{bmatrix}b_{11}&b_{12}&b_{13}\\
                 b_{21}&b_{22}&b_{23}\\
                 b_{31}&b_{32}&b_{33}
  \end{bmatrix}\,
  \begin{bmatrix}2&7&0\\
                 6&4&0\\
                 3&1&0
  \end{bmatrix}
$$
Equating the $(3, 3)$ places gives $0b_{31}+0b_{32}+0b_{33}=1$, i.e $0=1$. This is a contradiction.
Therefore, the matrix has no inverse.
\end{example}

\noindent {\bf Activity \ref{ssec.definv}}

\begin{enumerate}
\item Which sets of matrices are inverses?
\begin{enumerate}
\item $\left[ \begin{array}{rrr} 1&1&0\\-1&-1&3\\0&2&-1 \end{array} \right
], \quad \left [ \begin{array}{rrr}\vspace{1mm}
                    \frac{5}{6}&-\frac{1}{6}&-\frac{1}{2}\\\vspace{1mm}
                    \frac{1}{6}&\frac{1}{6}&\frac{1}{2}\\
                    \frac{1}{3}&\frac{1}{3}&0  \end{array} \right ]$
\item $\left [ \begin{array}{rr} 2&-1\\ -5&-3 \end{array} \right
], \quad \left [ \begin{array}{rr} 3&1 \\5&-2 \end{array} \right
]$
\end{enumerate}
\item What is the inverse of $I$?
\end{enumerate}
\noindent The answers can be found in section \ref{answers3}.

\section{Finding the Inverse of a Matrix}
\label{ssec.findinv}\markright{\ref{ssec.findinv}
\titleref{ssec.findinv}}

Although we have defined the inverse of a matrix, we have not yet
shown how to find it. For $2 \times 2$ matrices only, the following formula can
be used:

Let $A$ be an invertible matrix
$$ A= \begin{bmatrix} a&b\\
                      c&d \end{bmatrix}.$$
Then its inverse is:
\begin{equation} \label{inv1}
A^{-1}= \frac{1}{ad-bc}\left [
\begin{array}{rr}
            d&-b\\
            -c&a \end{array} \right ].
\end{equation}
Verify that this is indeed the inverse by finding the
product $AA^{-1}$.

\begin{example}
\label{exam3.findprod} Use {\rm (\ref{inv1})} to find $A^{-1}, B^{-1}\ and\ (AB)^{-1}$.

$$\mbox{let } A= \begin{bmatrix}1&2\\ 3&4 \end{bmatrix}, \quad B= \begin{bmatrix}5&6\\ 7&8
\end{bmatrix},$$

$$ \mbox{ then } AB= \begin{bmatrix} 19&22\\ 43&50
\end{bmatrix}.$$

By \ref{inv1} we have....
$$A^{-1}=\frac{1}{1\cdot4-2\cdot3}\left[\begin{array}{rr}4&-2\\
-3&1\end{array}\right]=
-\frac{1}{2}\left[\begin{array}{rr}4&-2\\ -3&1 \end{array}\right]=\left[
\begin{array}{rr}-2&1\\ \frac{3}{2}&-\frac{1}{2} \end{array}\right ]$$

$$ B^{-1} = \frac{1}{5\cdot8-6\cdot7} \left [ \begin{array}{rr} 8&-6\\ -7& 5\end{array}
\right ]=-\frac{1}{2} \left [ \begin{array}{rr} 8&-6\\ -7& 5\end{array}\right ]=
\left [ \begin{array}{rr} -4&3\\ \frac{7}{2}&-\frac{5}{2}\end{array}\right ]$$

$$ (AB)^{-1} = \frac{1}{19\cdot50-22\cdot43}\left [ \begin{array}{rr} 50&-22\\ -43&
19\end{array} \right ]=\frac{1}{4}\left [ \begin{array}{rr} 50&-22\\ -43&
19\end{array} \right ]= \left [ \begin{array}{rr}\vspace{1mm} \frac{25}{2}&-\frac{11}{2}\\
\vspace{1mm}-\frac{43}{4}&\frac{19}{4}\end{array} \right ]$$
\end{example}
We will illustrate how to find the inverse of a $n \times n$ matrix $(n \geq 3)$ by means of an example when
$n=3$.

\begin{example}
\label{exam4.findinv}
Find the inverse of
$$
\left[ \begin{array}{rrr} 1&2&1\\2&5&3\\-1&-1&1 \end{array} \right].
$$
We have to solve
$$
\left[ \begin{array}{rrr} 1&2&1\\2&5&3\\-1&-1&1 \end{array} \right]\,
\begin{bmatrix}a_1&b_1&c_1\\a_2&b_2&c_2\\a_3&b_3&c_3\end{bmatrix} =
\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}.
$$
Writing the equations involving the $a$'s gives
\begin{equation*}
\begin{aligned}
1a_1 + 2a_2 + 1a_3 &=1 \\
2a_1 + 5a_2 + 3a_3 &=0 \\
-1a_1 -1a_2 + 1a_3 &=0
\end{aligned},\ \mathrm{or}\
\left[ \begin{array}{rrrrr}
1&2&1&\vline&1\\
2&5&3&\vline&0\\
-1&-1&1&\vline&0
\end{array} \right].
\end{equation*}
If we row--reduce, we obtain
$$
\begin{bmatrix}1&0&0&\vline&a_1\\0&1&0&\vline&a_2\\0&0&1&\vline&a_3\end{bmatrix}.
$$
The values $a_1,\,a_2,\,a_3$ appear in the last column because $x_1=a_1,\, x_2= a_2,\,x_3=a_3$ is the
solution to the system.  Similarly, for the $b$'s,
\begin{equation*}
\begin{aligned}
1b_1 + 2b_2 + 1b_3 &=0 \\
2b_1 + 5b_2 + 3b_3 &=1 \\
-1b_1 -1b_2 + 1b_3 &=0
\end{aligned},\ \mathrm{or}\
\left[ \begin{array}{rrrrr}
1&2&1&\vline&0\\
2&5&3&\vline&1\\
-1&-1&1&\vline&0
\end{array} \right].
\end{equation*}
If we row--reduce, we obtain
$$
\begin{bmatrix}1&0&0&\vline&b_1\\0&1&0&\vline&b_2\\0&0&1&\vline&b_3\end{bmatrix}.
$$
Exactly the same idea applies for the $c$'s.  The system of equations is
$$
\left[ \begin{array}{rrrrr}
1&2&1&\vline&0\\
2&5&3&\vline&0\\
-1&-1&1&\vline&1
\end{array} \right].
$$
and this reduces to
$$
\begin{bmatrix}1&0&0&\vline&c_1\\0&1&0&\vline&c_2\\0&0&1&\vline&c_3\end{bmatrix}.
$$
Since the reduction process is identical in each case, we may as well reduce all three at once:
\begin{align*}
&\left[ \begin{array}{rrrrrrr}
1&2&1&\vline&1&0&0\\
2&5&3&\vline&0&1&0\\
-1&-1&1&\vline&0&0&1
\end{array} \right]
\quad\ \  \overset{\begin{smallmatrix}R_2-2R_1\\R_3+R_1\end{smallmatrix}}{\leadsto} \quad
&&\hspace{-10mm}\left[ \begin{array}{rrrrrrr}
1&2&1&\vline&1&0&0\\
0&1&1&\vline&-2&1&0\\
0&1&2&\vline&1&0&0
\end{array} \right] \\
\overset{\begin{smallmatrix}R_1-2R_2\\R_3-R_2\end{smallmatrix}}{\leadsto}
&\left[ \begin{array}{rrrrrrr}
1&0&-1&\vline&5&-2&0\\
0&1&1&\vline&-2&1&0\\
0&0&1&\vline&3&-1&1
\end{array} \right]
\quad \overset{\begin{smallmatrix}R_1+R_3\\R_2-R_3\end{smallmatrix}}{\leadsto} \quad
&&\hspace{-10mm}\left[ \begin{array}{rrrrrrr}
1&0&0&\vline&8&-3&1\\
0&1&0&\vline&-5&2&-1\\
0&0&1&\vline&3&-1&1
\end{array} \right]
\end{align*}
Hence the inverse is
$$
\left[ \begin{array}{rrr}
8&-3&1\\
-5&2&-1\\
3&-1&1
\end{array} \right]
$$
\end{example}
This is the process of finding the inverse of $n \times n$ matrices with $n \geq 3$.  Symbolically, we have
$$
[A|I] \leadsto [I|A^{-1}] \quad \mathrm{by\ elementary\ row\ operations}.
$$
The algorithm for finding the inverse will now be demonstrated by another example.
\begin{example}\label{exam3.findinv}
Let
$$
A = \left [\begin{array}{rrr}
      1 & 0 & 2 \\
      2 & -1 & 3 \\
      4 & 1 & 8 \end{array} \right ].
$$ Find $A^{-1}$.
\\
\noindent {\bf Step 1.}  Set up the matrix $[ \ A \ |\ I \ ]$.
$$
\left[ \begin{array}{rrrcrrr} 1 & 0 & 2 & \vline & 1 & 0 & 0
\\ 2 & -1 & 3 & \vline & 0 & 1 & 0 \\ 4 & 1 & 8 & \vline & 0 & 0 & 1
\end{array} \right ]
$$
\noindent {\bf Step 2.}   Use elementary row operations to reduce
$[ \ A \ | \ I \ ]$ to $[ \ I \ | \ A^{-1} \ ]$.
$$
\begin{array}{rl}
\overset{\begin{smallmatrix}R_2-2R_1\\R_3-4R_1\end{smallmatrix}}{\leadsto}
&\left[ \begin{array}{rrrcrrr} 1 & 0 & 2 & \vline &1 & 0 & 0 \\ 0 & -1 &
-1 & \vline & -2 & 1 & 0 \\ 0 & 1 & 0 & \vline & -4 & 0 & 1
\end{array} \right]\\ &\\
\overset{-R_2}{\leadsto}  & \left[ \begin{array}{rrrcrrr} 1 &
0 & 2 & \vline & 1 & 0 & 0 \\ 0 & 1 & 1 & \vline & 2 & -1 & 0 \\ 0
& 1 & 0 & \vline & -4 & 0 & 1 \end{array} \right]\\ &\\
\overset{R_3-R_2}{\leadsto} & \left[ \begin{array}{rrrcrrr} 1 & 0
& 2 & \vline & 1 & 0 & 0 \\ 0 & 1 & 1 & \vline & 2 & -1 & 0 \\ 0 &
0 & -1 & \vline & -6 & 1 & 1
\end{array} \right]\\ &\\
\overset{-R_3}{\leadsto} &\left[ \begin{array}{rrrcrrr} 1 & 0
& 2 & \vline & 1 & 0 & 0 \\ 0 & 1 & 1 & \vline & 2 & -1 & 0 \\ 0 &
0 & 1 & \vline & 6 & -1 & -1
\end{array} \right]\\
&\\ \overset{\begin{smallmatrix}R_1-2R_3\\R_2-R_3\end{smallmatrix}}{\leadsto} & \left[
\begin{array}{rrrcrrr} 1 & 0 & 0 & \vline & -11 & 2 & 2 \\ 0 & 1 & 0 & \vline & -4 &
0 & 1 \\ 0 & 0 & 1 & \vline & 6 & -1 & -1
\end{array} \right].
\end{array}
$$

\noindent {\bf Step 3.}  The matrix is now in the form $[ \ I \ |
\  A^{-1} \ ]$.  Therefore: $$ A^{-1}= \left [
\begin{array}{rrr}
           -11 & 2 & 2 \\
            -4 & 0 & 1 \\
             6 & -1 & -1 \end{array} \right ].
$$
Verify that this is true by finding $AA^{-1}.$
\end{example}
The latter procedure shows that the inverse $A^{-1}$ can be found precisely when $A$ can
be reduced to the identity matrix $I$.  If $A$ is an $n \times n$ matrix, then the following
statements are equivalent, \emph{i.e.}, if one of them is true they are all true, if one of them
is false they are all false.
\begin{enumerate}
\item $A$ is invertible.
\item $A$ is row equivalent to $I_n$
\item $A{\bf{x}} = 0 $ (the homogeneous system) has only the trivial solution.
\end{enumerate}

These results imply that a square matrix which is not
invertible cannot be reduced to $I$ using elementary row
operations.  This has a practical use, since if at some point
during the computation of an inverse, a row of zeroes occurs on
the left side, then the matrix is not invertible and the
computation can be stopped.

\begin{example} For the matrix given below, find its inverse, if it
exists.

$$A=\left [ \begin{array}{rrr}
                1&2&0\\
                1&1&-2\\
                0&2&4 \end{array} \right ]$$
Set up the matrix $[\ A \ \vline \ I \ ]$ and reduce:

$$
\begin{array}{rl}
& \left [ \begin{array}{rrrcrrr}
            1&2&0&\vline & 1&0&0\\
            1&1&-2& \vline & 0&1&0\\
            0&2&4& \vline & 0&0&1 \end{array} \right ]\\ &\\
\overset{R_2-R_1}{\leadsto}&\left [ \begin{array}{rrrcrrr}
            1&2&0&\vline & 1&0&0\\
            0&-1&-2& \vline & -1&1&0\\
            0&2&4& \vline & 0&0&1 \end{array} \right ]\\ &\\
\overset{R_3+2R_2}{\leadsto}&\left [ \begin{array}{rrrcrrr}
            1&2&0&\vline & 1&0&0\\
            0&-1&-2& \vline & -1&1&0\\
            0&0&0& \vline & -2&2&1 \end{array} \right ] \end{array}$$
A row of zeroes has occurred on the left hand side of the matrix,
therefore, this matrix is not invertible.
\end{example}

\noindent {\bf Activity \ref{ssec.findinv}}
Find the inverses of the following matrices, or determine that the matrix is not invertible.
\begin{itemize}
\item[(a)] $\left[ \begin{array}{rrr}
                        1&0&1\\
                        -1&1&2\\
                        2&2&9 \end{array} \right]$
\item[(b)] $\left [ \begin{array}{rrr} 1&0&1\\ 1&1&0 \\ -1&1&-2 \end{array} \right ]$
\item[(c)] $\left [ \begin{array}{rrrr}
                        1&0&0&0\\
                        0&0&0&1 \\
                        0&0&1&0\\
                        0&1&0&0\end{array} \right ]$
\end{itemize}
\noindent The answers can be found in section \ref{answers3}.

\section{Properties of the Inverse}
\label{ssec.propinv}\markright{\ref{ssec.propinv}
\titleref{ssec.propinv}}

We now state some important properties about the inverse.

If $A$, and $B$ are invertible matrices of the same size then:
\begin{enumerate}

\item $AB$ is invertible.  In fact, the product of any number of
invertible matrices will be invertible.
\item $(AB)^{-1}=B^{-1}A^{-1}$.  This property can be extended to
the inverse of any product of invertible matrices.
\item If $B$ is square and $BA = I$, then $B = A^{-1}$.
\item If $B$ is square and $AB = I$, then $B = A^{-1}$.
\item $(A^{-1})^{-1}=A$.
\item $(A^n)^{-1}=(A^{-1})^n=A^{-n}$.
\item $(kA)^{-1}= \frac{1}{k}A^{-1},\ k \neq 0$.
\item $(A^t)^{-1}=(A^{-1})^t$.
\end{enumerate}

It is easy to verify that some of these properties are in fact true. For
example, we have stated that $(AB)^{-1}=B^{-1}A^{-1}$. We know
that the inverse of $(AB)^{-1}$ is $AB$ and that the inverse is
unique. Now
$$ (AB)(B^{-1}A^{-1})=I \ {\rm and} \ (B^{-1}A^{-1})(AB)=I$$
so $B^{-1}A^{-1}$  must be the inverse to $AB$, which we have denoted
by $(AB)^{-1}$!

Note that by properties 3 and 4, if you want to check that two matrices $A$ and $B$
are inverses, it is only necessary to check either $AB = I$ or $BA = I$ (and not both).
The fact is that
$$
AB = I \Leftrightarrow BA = I
$$
and then $A$ and $B$ are inverses.

The following example uses property 6 to find the required matrix.
\begin{example} Given,
\label{exam3.invprop}
 $$A= \left [
\begin{array}{rr} 2 & -1 \\ -3 & 2
\end{array}
 \right], \quad A^{-1}=\left [
\begin{array}{rr} 2 & 1 \\ 3 & 2
\end{array}
 \right ].$$
 Find $A^{-3}$. $$ A^{-3}=(A^{-1})^3=\left [ \begin{array}{rr}
2&1
\\ 3&2 \end{array} \right ]\left [ \begin{array}{rr} 2&1 \\ 3&2 \end{array}
\right ]\left [ \begin{array}{rr} 2&1 \\ 3&2 \end{array} \right
]=\left[ \begin{array}{rr} 26 & 15 \\ 45 & 26
\end{array} \right] . $$
\end{example}

\begin{example} For $A$, defined below, show that
$(A^t)^{-1}=(A^{-1})^t$.

$$A=\left [ \begin{array}{rrr} 1&0&1\\2&1&-1\\-1&1&1 \end{array}
\right ].$$

\begin{align*}
(A^t)^{-1} &= \left [ \begin{array}{rrr} 1&2&-1\\0&1&1\\1&-1&1
\end{array} \right ]^{-1}
= \left [  \begin{array}{rrr}\vspace{1mm}
\frac{2}{5}&-\frac{1}{5} & \frac{3}{5}\\ \vspace{1mm}
\frac{1}{5}& \frac{2}{5}&  -\frac{1}{5}\\ \vspace{1mm}
 -\frac{1}{5}& \frac{3}{5} & \frac{1}{5}
\end{array} \right ].
\\
(A^{-1})^t&=\left[  \begin{array}{rrr}\vspace{1mm} \frac{2}{5}&
\frac{1}{5}& -\frac{1}{5}\\ \vspace{1mm} -\frac{1}{5}& \frac{2}{5}&
\frac{3}{5}\\\vspace{1mm} \frac{3}{5}&  -\frac{1}{5} & \frac{1}{5}
\end{array} \right ]^t
=\left[  \begin{array}{rrr}\vspace{1mm}
\frac{2}{5}&-\frac{1}{5} & \frac{3}{5}\\\vspace{1mm}
\frac{1}{5}& \frac{2}{5}&  -\frac{1}{5}\\\vspace{1mm}
 -\frac{1}{5}& \frac{3}{5} & \frac{1}{5}
\end{array} \right ].
\end{align*}
\end{example}

\noindent {\bf Activity \ref{ssec.propinv}}

Using the method introduced to show that property 2 was true, show
that properties 5 and 7 are true. Hints are provided in the
section \ref{answers3}.

\section{Systems of Equations and Invertibility}
\label{ssec.syseinv}\markright{\ref{ssec.syseinv}
\titleref{ssec.syseinv}}

\noindent Recall the linear system (\ref{generalsystem}).  In section
\ref{sec.matrix}, this
system was written symbolically as {\textit A{\bf x}={\bf b}}. If
$A$ is an invertible $n \times n$ matrix, then
\begin{align*}
A^{-1}A{\bf{x}} &= A^{-1}{\bf{b}},\\
\mathrm{i.e}\ {\bf{x}} &= A^{-1}{\bf{b}}.
\end{align*}
Hence the system $A{\bf {x}}={\bf {b}}$ has
exactly one solution namely ${\bf {x}} = A^{-1}{\bf {b}}$.

\begin{example}
\label{exam3.findsol} Find the solution to the following system.
$$
\begin{array}{ccccc}
x_1 & & +  2x_3 & = & 3 \\ 2 x_1 & -  x_2 & + 3 x_3 & = & 7
\\ 4x_1 & +x_2& + 8 x_3 & = & 2 \end{array}. $$ Written in matrix
form: $$ \left [
\begin{array}{rrr}
    1 & 0 & 2 \\
    2 & -1 & 3 \\
    4 & 1 & 8 \end{array} \right ] \left [ \begin{array}{r}
              x_1 \\
              x_2 \\
              x_3 \end{array} \right ] \;=\; \left [ \begin{array}{r}
                         3 \\
                         7\\
                         2 \end{array} \right ].
$$ Therefore $$A= \left [ \begin{array}{rrr}
    1 & 0 & 2 \\
    2 & -1 & 3 \\
    4 & 1 & 8 \end{array} \right ].
 $$
Recall from example \ref{exam3.findinv}:$$  A^{-1}=\left [
\begin{array}{rrr}
                -11 & 2 & 2 \\
                -4 & 0 & 1 \\
                 6 & -1 & -1 \end{array} \right ].
$$ A is invertible, thus there is exactly one solution {\textit
{\bf x}=$A^{-1}${\bf b}}
$$ \left [ \begin{array}{r}
   x_1 \\
   x_2 \\
   x_3 \end{array} \right ] \;=\;
        \left [ \begin{array}{rrr}
        -11 & 2 & 2 \\
        -4 & 0 & 1 \\
        6 & -1 & -1 \end{array} \right ] \left [ \begin{array}{r}
              3 \\
              7 \\
              2 \end{array} \right ] \;=\; \left [ \begin{array}{r}
                         -15 \\
                         -10 \\
                         9 \end{array} \right ].
$$ The solution is $x_1=-15, x_2=-10, x_3=9$.
\end{example}

Consider the following systems: $A{\bf x} = {\bf b}_1, A{\bf x} =
{\bf b}_2, \ldots, A{\bf x} = {\bf b}_k$. $A$ is fixed; \ ${\bf
b}_1, {\bf b}_2, \ldots, {\bf b}_k$ differ. If $A$ is invertible
then $$ {\bf x}_1 = A^{-1} {\bf b}_1 , \quad {\bf x}_2 = A^{-1}
{\bf b}_2 , \quad \cdots \quad , {\bf x}_k = A^{-1} {\bf b}_k .$$
Alternatively we can solve this system using Gauss-Jordan
elimination defined in section \ref{ssec.gausse}, by setting up
the augmented matrix as follows: $$[ \ A \;\; \vline \;\; {\bf
b}_1 \;\;\vline \;\;{\bf b}_2 \;\; \vline \;\; \cdots \;\;
 \vline \;\; {\bf b}_k ].$$
Notice that by using the second method, $A$ need not be
invertible. The use of either of the two methods will largely depend on the
problem.

\begin{example}
\label{exam3.gausse}
Find the solution to the following system by
using Gaussian elimination and by finding ${\bf x}=A^{-1}{\bf b}$.
$$A = \left[\begin{array}{rrr}
1 & 0 & 2\\
2& -1 & 3 \\
4 & 1 & 8 \end{array}\right ]
\quad {\bf b}_1 =
\left[ \begin{array}{r}
1\\
1\\
0\end{array}\right ]
\quad {\bf b}_2 =
\left[ \begin{array}{r}
3\\
2\\
9\end{array}\right ]$$
$$A{\bf x}_1={\bf b}_1,\ A{\bf x}_2={\bf b}_2.$$
To solve using Gaussian elimination,
set up the
augmented matrix:
$$\left [ \begin{array}{rrrcrcr}
    1 & 0 & 2 & \vline & 1 & \vline & 3 \\
    2 & -1 & 3 & \vline & 1 & \vline & 2 \\
    4 & 1 & 8 & \vline & 0 & \vline & 9 \end{array} \right ].$$
Elementary row operations yield the following:
$$\left [ \begin{array}{rrrcrcr}
           1 & 0 & 0 & \vline & -9 & \vline & -11 \\
           0 & 1 & 0 & \vline & -4& \vline & -3 \\
           0 & 0 & 1 & \vline & 5 & \vline & 7 \end{array} \right
           ].$$
Therefore the solution is: $${\bf x}_1= \left [\begin{array}{r} -9\\-4\\5
\end{array} \right ] \quad {\bf x}_2=\left [ \begin{array}{r}
-11\\-3\\7
\end{array} \right ].$$

To solve using the second method, we must first find the inverse
of $A$, by setting up the matrix $[ A \ \vline \ I ]$ and
reducing to $[ I \ \vline \ A^{-1}]$. This was done in example
\ref{exam3.findinv}. Therefore,
$${\bf x}_1=
\left
 [\begin{array}{rrr}
           -11 & 2 & 2 \\
            -4 & 0 & 1 \\
             6 & -1 & -1 \end{array} \right ]\left
          [ \begin{array}{r}
                1 \\
                1 \\
                0 \end{array} \right ]=\left
         [ \begin{array}{r}-9\\-4\\5
\end{array} \right ]$$
$$ \quad {\bf x}_2=\left
 [\begin{array}{rrr}
           -11 & 2 & 2 \\
            -4 & 0 & 1 \\
             6 & -1 & -1 \end{array} \right ]\left
          [ \begin{array}{r}
                          3 \\
                          2 \\
                         9 \end{array} \right ]=\left
              [ \begin{array}{r}
-11\\-3\\7
\end{array} \right ].$$
\end{example}

We have already given some equivalent statements regarding
invertibility and row equivalency.  We will return to these
statements and their implications relatively often, so it is
worthwhile to state them as a {\bf theorem}.  They are given here
without proof; it is really the results which will be important to
solving problems.

\begin{theorem}
\label{eq2invert}Let $A$ be an $n \times n$ matrix. The following
statements are equivalent, i.e., if one of them is true, they are
all true, if one of them is false they are all false.

\begin{enumerate}

\item $A$ is invertible.
\item {\textit A{\bf x}} = {\bf 0} (the homogeneous
system) has only the trivial solution.
\item $A$ is row equivalent
to $I_n$.
\item {\textit A{\bf x}={\bf b}} is consistent (and has
a unique solution) for every $n \times 1$ matrix ${\bf b}$.
\end{enumerate}
\end{theorem}

The following example makes use of theorem \ref{eq2invert}.

\begin{example}
\label{exam3.findall} Let $A$ be an $m \times n$ fixed matrix
representing the system below. Find all $m \times 1$ matrices ${\bf b}$
such that \mbox{{\textit A{\bf x} = {\bf b}}} is consistent.

\noindent {\bf Note :} If $m = n$ and $A$ is invertible we know
the answer. Theorem \ref{eq2invert} states that a unique solution is
guaranteed, therefore the system will be consistent for all {\bf
b}. However if a) $A$ is non invertible or  b) $m \neq n$ how do
we find the solution?

$$
\begin{array}{ccccc}
x_1 & + x_2 & + 2 x_3 & = & b_1 \\ x_1 &  & + x_3 & = & b_2 \\ 2
x_1 &+ x_2  & + 3 x_3 & = & b_3 \end{array} . $$ What value must
$b_1, b_2, b_3$ have for this system to be consistent?  Set up the
augmented matrix
 $$ \left [ \begin{array}{rrrcr}
    1 & 1 & 2 & \vline & b_1 \\
    1 & 0 & 1 & \vline & b_2 \\
    2 & 1 & 3 & \vline & b_3 \end{array} \right ].$$
Elementary row operations yield:
$$\left [
            \begin{array}{rrrcc}
            1 & 1 & 1 & \vline & b_2 \\
            0 & 1 & 1 & \vline & b_1 - b_2 \\
            0 & 0 & 0 & \vline & b_3 - b_2 - b_1 \end{array} \right
            ].
$$ Notice that the third row states that
$0x_1+0x_2+0x_3=b_3-b_2-b_1$. Thus, $b_3 - b_2 - b_1$  must be
zero for a consistent system. So,  $b_3=b_2+b_1$, and $$ {\bf b} =
\left [
\begin{array}{c}
            b_1 \\
            b_2 \\
            b_1 + b_2 \end{array} \right ].
$$
\end{example}

\noindent {\bf Activity \ref{ssec.syseinv}}

For the following system:

$$A=\left [ \begin{array}{rrr}
                    1&0&2,\\-1&1&3,\\ 0&0&-2 \end{array} \right ],
\quad {\bf b}_1=\left [ \begin{array}{r} 1\\4\\-3 \end{array} \right ],
\quad {\bf b}_2=\left [ \begin{array}{r} -10\\6\\-1 \end{array} \right ], $$
\begin{enumerate}
\item Find the inverse of $A$, then solve the system for ${\bf x}_1$ and ${\bf
x}_2$ using ${\bf x}=A^{-1}{\bf b}$.
\item Solve the system by setting up the augmented matrix.
\item Find the solution to $A{\bf x}={\bf 0}$ in any way you would
like.
\item Two matrices {\bf b} are given to be solved in this
question. What can be said about the solutions if any other {\bf b} matrices are introduced?
\end{enumerate}

\noindent Answers can be found in section \ref{answers3}

\section{The Determinant Function}
\label{sec.det} \markright{\ref{sec.det} \titleref{sec.det}}
The determinant\index{determinant} is a number (scalar) that can be calculated for
any square matrix.  The reason for calculating the determinant of
a matrix $A$ (denoted by $\det{(A)}$ or $|A|$) is because
it gives an easy way of determining whether or not a matrix is invertible.
The fact is that
\begin{align*}
\det(A) &\neq 0 \Leftrightarrow A\ \mathrm{is\ invertible},\\
\det(A) &= 0 \Leftrightarrow A\ \mathrm{is\ not\ invertible}.
\end{align*}
\subsection{Definition of the Determinant}\label{subsec.defdet}
The definition given here is inductive, in the sense that we
\begin{itemize}
\item Give explicit formulas for the determinant of $1 \times 1$ and
$2 \times 2$ matrices,
\item Give a formula for finding the determinant of a $n \times n$ matrix
in terms of determinants of $(n-1) \times (n-1)$ matrices.
\end{itemize}
We have: \newline For a $1 \times 1$ matrix $A = (a_{11})$,
$$
\det{(A)} = |A| = a_{11}.
$$
For a $2 \times 2$ matrix $A = \begin{pmatrix} a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix}$,
$$
\det{(A)} = \begin{vmatrix} a_{11} & a_{12} \\ a_{21} & a_{22}
\end{vmatrix} = a_{11}\,a_{22} - a_{21}\,a_{12}.
$$
\noindent For $n\times n$ matrices ($n\geq 3$), we have to
consider $\bf{cofactors}\index{cofactors}$. If $A$ is $n\times n$, then the $(i,
j)$ cofactor $A^{ij}$ is the $(n-1) \times (n-1)$ matrix obtained
from $A$ by deleting the $i^{th}$ row and the $j^{th}$ column
(``cross out through the (i, j) place"):
$$ A^{ij}= \left [ \begin{array}{rrrrrrr}
    a_{11} & a_{12} & \cdots & a_{1j-1} & a_{1j+1} &\cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2j-1} & a_{2j+1} &\cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
    a_{i-1,1} &a_{i-1,2} &\cdots &a_{i-1,j-1} &a_{i-1,j+1} &\cdots &a_{i-1,n}\\
    a_{i+1,1} &a_{i+1,2} &\cdots &a_{i+1,j-1} &a_{i+1,j+1} &\cdots &a_{i+1,n}\\
    \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
    a_{n,1} &a_{n,2} &\cdots &a_{n,j+1} &a_{n,j+1} &\cdots &a_{n,n}
\end{array} \right ].$$
\noindent For an $n\times n$ matrix there are $n^{2}$ places that
you can cross out through.  Hence there are $n^{2}$ different
cofactors.

\noindent For a $3\times3$ matrix $A=\left[ a_{ij}
\right]_{3\times 3}$~, we define
\begin{align}
\det{(A)}&= a_{11}\,\det{(A^{11})} - a_{12}\,\det{(A^{12})} + a_{13}\,\det{(A^{13})} \notag \\
&=a_{11}\begin{vmatrix} a_{22} &a_{23} \\
                         a_{32} &a_{33} \end{vmatrix}
- a_{12}\begin{vmatrix}  a_{21} &a_{23} \\
                        a_{31} &a_{33} \end{vmatrix}
+ a_{13}\begin{vmatrix} a_{21} &a_{22} \\
                        a_{31} &a_{32} \end{vmatrix} \notag \\
&=
a_{11}(a_{22}\,a_{33}-a_{32}\,a_{23})-a_{12}(a_{21}\,a_{33}-a_{31}\,a_{23})+
a_{13}(a_{21}\,a_{32}-a_{31}\,a_{22}). \label{eq.find-det}
\end{align}
\noindent Don't try to memorize this formula. Instead, remember
the method. We refer to this method as ``expanding along the top
row", because you proceed along the top row of the matrix. As you go,
multiply each entry by its cofactor and alternate the sign.

\begin{example}
\label{exam4.det}
\begin{align*}
\begin{vmatrix} 1&4&0 \\ 2&2&\!\!\!\!-1 \\ 1&3&6 \end{vmatrix}
&= 1 \begin{vmatrix} 2 &\!\!\!-1 \\ 3&6 \end{vmatrix} -4
\begin{vmatrix}  2 &\!\!\!-1 \\ 1&6 \end{vmatrix}
+0 \begin{vmatrix}  2 &2 \\ 1&3 \end{vmatrix} \\
&= 1(2\times 6-3(-1))-4(2\times 6-1(-1))+ 0(2\times 3-1\times 2) \\
&=12+3-4(13)=-37.
\end{align*}
\end{example}
There is a geometric way of remembering how to find the
determinant of a $3\times3$ matrix. Write the matrix with two
repeat column as follows:
\[
\left[
\begin{array}{ccccccccc}
  a_{11} &          & a_{12} &          & a_{13}  &          &   a_{11}    &          & a_{12}\\
         & \searrow &        & \searrow &         & \searrow &             &          &       \\
  a_{21} &          & a_{22} &          & a_{23}  &          &   a_{21}    &          & a_{22}\\
         &          &        & \searrow &         & \searrow &             & \searrow &\\
  a_{31} &          & a_{32} &          & a_{33}  &          &   a_{31}    &          & a_{32}\\
\end{array}
\right]
\]

Now multiply down the three diagonals, and sum the products positive. This yields
$$(a_{11}\,a_{22}\,a_{33}+a_{12}\,a_{23}\,a_{31}+a_{13}\,a_{21}\,a_{32}).$$
Finally, multiply up the three diagonals given below, and sum the
products negative.
\[
\left[
\begin{array}{ccccccccc}
  a_{11} &          & a_{12} &          & a_{13}  &          &   a_{11}    &          & a_{12}\\
         &          &        & \nearrow &         & \nearrow &             & \nearrow &       \\
  a_{21} &          & a_{22} &          & a_{23}  &          &   a_{21}    &          & a_{22}\\
         & \nearrow &        & \nearrow &         & \nearrow &             &          &\\
  a_{31} &          & a_{32} &          & a_{33}  &          &   a_{31}    &          & a_{32}\\
\end{array}
\right]
\]
This gives
$$-(a_{31}\,a_{22}\,a_{13}+a_{32}\,a_{23}\,a_{11}+a_{33}\,a_{21}\,a_{12}).$$
Combining the two yields
$$(a_{11}\,a_{22}\,a_{33}+a_{12}\,a_{23}\,a_{31}+a_{13}\,a_{21}\,a_{32})-
(a_{31}\,a_{22}\,a_{13}+a_{32}\,a_{23}\,a_{11}+a_{33}\,a_{21}\,a_{12}),$$
which is the expanded formula for the determinant given in
equation~\ref{eq.find-det}.

\iffalse
\begin{example}
\label{exam4.deter} Find the determinant of
$$
\left[ \begin{array}{rrr} 2&4&1 \\-3&2&7 \\ 1&1&2 \end{array}
\right].
$$
We have$$ \begin{array}{rrrrr}
            &&2&14&-24\\
            2&4&1&2&4\\
            -3&2&7&-3&2\\
            1&1&2&1&2\\
            &&8&28&-3 \end{array} $$
The determinant is $(8+28-3)-(2+14-24)=33-(-8)=41$.
\end{example}
\fi
For $4\times 4$ (and larger) matrices, the determinant can be
found by expanding along the top row. Specifically, if
$A=[a_{ij}]_{4\times 4}$ then
$$
\begin{vmatrix}
            a_{11}&a_{12}&a_{13}&a_{14}\\
            a_{21}&a_{22}&a_{23}&a_{24}\\
            a_{31}&a_{32}&a_{33}&a_{34}\\
            a_{41}&a_{42}&a_{43}&a_{44}
\end{vmatrix} =
a_{11}\det(A^{11})-a_{12}\det(A^{12})+a_{13}\det(A^{13})-a_{14}\det(A^{14}).$$
Since $A^{11}$, $A^{12}$, $A^{13}$ and $A^{14}$ are each
$3\times3$ matrices, calculating a $4\times4$ determinant requires
the calculation of $4$ separate $3\times3$ determinants.

\begin{example}
\label{exam4.determinant}
Find
$$\left|\begin{array}{rrrr}1&2&0&4 \\ 2&0&1&-1 \\-2&1&3&0\\
0&-3&-4&1\end{array}\right|$$
Using the above formula, we must calculate
$$1 \left|\begin{array}{rrr} 0&1&-1\\1&3&0\\-3&-4&1\end{array}\right|-2
\left|\begin{array}{rrr}2&1&-1\\-2&3&0\\0&-4&1\end{array}\right|
+0\left|\begin{array}{rrr}2&0&-1\\-2&1&0\\0&-3&1\end{array}\right|
-4\left|\begin{array}{rrr}2&0&1\\-2&1&3\\0&-3&-4\end{array}\right| \\$$
Calculating the $4$ separate $3\times3$ determinants we find
$$\left|\begin{array}{rrr}0&1&-1\\1&3&0\\-3&-4&1\end{array}\right|
=0(3-0)-1(1-0)-1(-4+9)=-6$$
$$\left|\begin{array}{rrr}2&1&-1\\-2&3&0\\0&-4&1\end{array}\right|
=2(3-0)+2(1-4)+0(0+3)=0$$
$$\left|\begin{array}{rrr}2&0&-1\\-2&1&0\\0&-3&1\end{array}\right|
=2(1-0)-0(-2+0)-1(6-0)=-4$$
$$\left|\begin{array}{rrr}2&0&1\\-2&1&3\\0&-3&-4\end{array}\right|
=2(-4+9)-0(8-0)+1(6-0)=16$$

Hence
$$\left|\begin{array}{rrrr} 1&2&0&4 \\ 2&0&1&-1 \\ -2&1&3&0\\0&-3&-4&1\end{array}\right|
=1(-6)-2(0)+0(-4)-4(16)=-6-0+0-64=-70$$
Notice that we needn't have calculated
$$\left|\begin{array}{rrr}2&0&-1\\-2&1&0\\0&-3&1\end{array}\right|,$$
since it was multiplied by zero.
\end{example}

If $A=[a_{ij}]_{5\times5}$ (a $5\times5$ matrix), then
$$\det(A)=
a_{11}\det(A^{11})-a_{12}\det(A^{12})+a_{13}\det(A^{13})-a_{14}\det(A^{14})
+a_{15}\det(A^{15}).$$
Thus 5 distinct $4\times4$ determinants must be calculated. The
definition proceeds in this way for larger matrices. Obviously the
arithmetic becomes more involved as the matrices get larger. Later
in the chapter we will see how the calculation of determinants can
sometimes be simplified.
\subsection{Alternative Expansions for the Determinant}
The definition given above uses expansion along the top row. In
fact, you can expand along \emph{any} row or down \emph{any}
column provided that you adjust the signs appropriately. The sign
associated with $a_{ij}$ has to be $(-1)^{i+j}$. The easy way to
deal with this is to start with a $+$ sign for $a_{11}$, then
alternate between $+$ and $-$ as you go side to side or up and
down. This can be illustrated with the following example.
\begin{example} \label{exam4.alter} Find
$$
\begin{vmatrix}1&4&0 \\  2&2&\!\!\!-1 \\  1&3&6 \end{vmatrix}.
$$
First, calculate the signs to be associated with each entry. These
are shown in superscript
$$
\left| \begin{array}{rrr}
  1^+&4^-&0^+ \\
  2^-&2^+&-1^- \\
  1^+&3^-&6^+
\end{array} \right|.
$$
Expanding along the second row gives
\begin{align*}
\left| \begin{array}{rrr}1^+&4^-&0^+ \\2^-&2^+&-1^- \\
1^+&3^-&6^+ \end{array} \right| &= -2\begin{vmatrix}4&0\\3&6
\end{vmatrix}+ 2\begin{vmatrix}1&0\\1&6\end{vmatrix}-(-1)
\begin{vmatrix}1&4\\1&3\end{vmatrix}\\
&= -2(24)+2(6)+1(-1) =-49+12=-37,
\end{align*}
which agrees with the previous answer.
Expanding down the second column gives
\begin{align*}
\left| \begin{array}{rrr}1^+&4^-&0^+ \\2^-&2^+&-1^- \\
1^+&3^-&6^+ \end{array} \right|
&= -4 \begin{vmatrix}2&\!\!\!-1 \\1&6 \end{vmatrix} + 2 \begin{vmatrix} 1&0 \\
1&6 \end{vmatrix} - 3 \begin{vmatrix}1&0 \\2&\!\!\!-1 \end{vmatrix} \\
&= -4(13)+2(6)-3(-1) =-52+15=-37.
\end{align*}
\end{example}
\vspace{-8mm}
\begin{example}
\label{exam4.alter2}
Find
$$\left|\begin{array}{rrrr}1&-2&5&3\\
-3&-1&4&2\\
1&3&2&-4\\
2&1&0&1\end{array}\right|$$
The sign associated with each entry is shown in superscript:
$$\left|\begin{array}{rrrr}1^{+}&-2^{-}&5^{+}&3^{-}\\
-3^{-}&-1^{+}&4^{-}&2^{+}\\
1^{+}&3^{-}&2^{+}&-4^{-}\\
2^{-}&1^{+}&0^{-}&1^{+}\end{array}\right|$$
Expanding along the third row gives
$$1\left|\begin{array}{rrr}-2&5&3\\-1&4&2\\1&0&1\end{array}\right|-3
\left|\begin{array}{rrr}1&5&3\\-3&4&2\\2&0&1\end{array}\right|
+2\left|\begin{array}{rrr}1&-2&3\\-3&-1&2\\2&1&1\end{array}\right|
-(-4)\left|\begin{array}{rrr}1&-2&5\\-3&-1&4\\2&1&0\end{array}\right|$$
Now expand each determinant along the top row.

\begin{eqnarray*}
\left|\begin{array}{rrr}-2&5&3\\-1&4&2\\1&0&1\end{array}\right|
&=&-2(4-0)-5(-1-2)+3(0-4)
\\&=&-2(4)-5(-3)+3(-4)\\&=&-8+15-12\\&=&-5
\\ \smallskip
\left|\begin{array}{rrr}1&5&3\\-3&4&2\\2&0&1\end{array}\right|
&=&1(4-0)-5(-3-4)+3(0-8)\\&=&1(4)-5(-7)+3(-8)\\&=&4+35-24\\&=&15
\\ \smallskip
\left|\begin{array}{rrr}1&-2&3\\-3&-1&2\\2&1&1\end{array}\right|
&=&1(-1-2)+2(-3-4)+3(-3+2)\\&=&1(-3)+2(-7)+3(-1)\\&=&-3-14-3\\&=&-20
\end{eqnarray*}
\begin{eqnarray*}
\left|\begin{array}{rrr}1&-2&5\\-3&-1&4\\2&1&0\end{array}\right|
&=&1(0-4)+2(0-8)+5(-3+2)\\&=&1(-4)+2(-8)+5(-1)\\&=&-4-16-5\\&=&-25
\end{eqnarray*}
Hence,
\begin{eqnarray*}
\left|\begin{array}{rrrr}1&-2&5&3\\
-3&-1&4&2\\
1&3&2&-4\\
2&1&0&1\end{array}\right|&=&1(-5)-3(15)+2(-20)-(-4)(-25)\\&=&-5-45-40-100\\&=&-190
\end{eqnarray*}
\end{example}
We can use the fact that a determinant can be expanded via any row
or column to our advantage, by identifying rows or columns that
have a lot of zeros. For example, expanding down the fourth column
gives
$$
\left| \begin{array}{rrrr}
  2&1&3&0 \\
  1&-1&4&0 \\
  5&-2&1&0 \\
  -6&3&2&1 \end{array} \right| = \left| \begin{array}{rrr}
  2&1&3 \\
  1&-1&4 \\
  5&-2&1 \end{array} \right |.
$$
Expanding along the top row, this is
\begin{align*}
\left| \begin{array}{rrr} 2&1&3 \\ 1&-1&4 \\ 5&-2&1 \end{array}
\right | &= 2 \begin{vmatrix}-1&4\\-2&1\end{vmatrix} - 1
\begin{vmatrix}1&4 \\ 5&1 \end{vmatrix} + 3
\begin{vmatrix}1&-1\\5&-2\end{vmatrix} \\
&= 2(7)-1(-19)+3(3) =42.
\end{align*}
The point is that we only had to calculate one $3\times3$
determinant because we identified a column with only one non-zero
entry.

For a lower triangular matrix,
$$
\begin{vmatrix}
  a_{11}&0&0 \\
  a_{21}&a_{22}&0 \\
  a_{31}&a_{32}&a_{33}
\end{vmatrix} = a_{11} \begin{vmatrix} a_{22}&0 \\ a_{32}&a_{33}
\end{vmatrix} = a_{11}\,a_{22}\,a_{33}.
$$
The same reasoning applies to upper triangular matrices, and for
any size of triangular matrix. Consequently the determinant of a
triangular matrix is equal to the product of the diagonal
elements.


\noindent {\bf Activity\ref{sec.det}}
\begin{enumerate}
\item Find the determinant of the matrices by expanding
\begin{enumerate}
\item along the top row
\item down the middle column
\end{enumerate}
\begin{equation*}
\begin{aligned}
\mathrm{(i)}\ \ \left[ \begin{array}{rrr} 2&-1&3\\3&2&-2\\0&0&1 \end{array} \right]
\end{aligned} \qquad
\begin{aligned}
\mathrm{(ii)}\ \ \left[ \begin{array}{rrr} 2&3&2\\0&1&2\\0&8&3 \end{array} \right].
\end{aligned}
\end{equation*}
\item For the above matrices, which row (or column) would you expand along (or down) to simplify
the arithmetic?
\end{enumerate}
Calculating a determinant is a straightforward, but possibly tedious
matter.  What is not obvious is why the definition of determinant
(which must seem obtuse to say the least) is what it is.  The
answer to this question is that mathematicians sought a function
{\it f} of a square matrix A with the properties
\begin{enumerate}
\item
{\it f} is linear in the rows of A.  That is to say if
$$\left[\begin{array}{c}{\bf r_1}\\{\bf r_2}\\\vdots\\
{\bf r_n}\end{array}\right]$$
represents the rows of a matrix then
$${\it f}\left[\begin{array}{c}{\bf r_1}\\\vdots\\k{\bf r_i}+{\bf r_i'}\\
\vdots\\{\bf r_n}\end{array}\right]=
k{\it f}\left[\begin{array}{c}{\bf r_1}\\\vdots\\{\bf r_i}\\\vdots\\{\bf r_n}
\end{array}\right]+{\it f}\left[\begin{array}{c}{\bf r_1}\\\vdots\\{\bf r_i'}
\\\vdots\\{\bf r_n}\end{array}\right].$$
\item
{\it f} is linear in the columns of A, i.e.
if $({\bf c_1}:{\bf c_2}:\cdots:{\bf c_n})$
represents the columns of a matrix then
\begin{eqnarray*}
{\it f}({\bf c_1}:\cdots:k{\bf c_i}+{\bf c_i'}:\cdots:{\bf
c_n})&=& k{\it f}({\bf c_1}:\cdots:{\bf c_i}:\cdots:{\bf c_n}) \\&
&+ {\it f}({\bf c_1}:\cdots:{\bf c_i'}:\cdots:{\bf c_n})
\end{eqnarray*}
\item
If you switch two rows or columns of A then {\it f}(A) changes sign.
\end{enumerate}
It turns out that the determinant function fills the bill.
From the perspective of this course, however, all you need to know
about determinants is how to calculate them, their properties and
the fact that $det(A)\neq 0 \leftrightarrow A $ is invertible.

\section{Properties of Determinants}
\label{ssec.propdet} \markright{\ref{ssec.propdet}
\titleref{ssec.propdet}}
In section \ref{ssec.rreduction}, elementary row operations on
matrices were introduced.  Recall that the three elementary row
operations did not change the solution to the system of equations
represented by the matrix. However, some of these operations will
change the determinant and these changes are given here.
\begin{properties}
\label{propdet} Let $A$ and $B$ be two matrices, such that $B$ is
the result of one elementary row operation on $A$.
\begin{enumerate}
\item If $B$ is the matrix resulting from multiplying
one row or one column of $A$ by a scalar $k$, then
$\det(B)=k\,\det(A)$.
\item If $B$ is the matrix resulting from interchanging two rows
or columns in $A$, then \ $\det(B)=-\det(A)$.
\item If $B$ is the matrix resulting from adding a multiple of one
row or column to another row or column, then $\det(A) =\det(B).$
\end{enumerate}
\end{properties}
\begin{example} Let
$$
A= \left [ \begin{array}{rrr} -1&0&2\\1&1&-1\\3&2&1 \end{array}
\right ],\ B=\left [
\begin{array}{rrr} 1&2&0\\ 1&1&-1\\ 3&2&1 \end{array} \right ],$$
so that $B$ is obtained from $A$ by adding $2\times$ row 2 to row
1. Then
\begin{align*}
\det(A)&= \left| \begin{array}{rrr}-1&0&2 \\ 1&1&-1 \\ 3&2&1 \end{array} \right| \\
& = -1(1+2)+2(2-3)=-3-2 =-5, \intertext{and}
\det(B)&= \left| \begin{array}{rrr}1&2&0\\1&1&-1\\3&2&1 \end{array} \right| \\
&= 1(1+2)-2(1+3)=3-8 =-5.
\end{align*}
\end{example}
These properties (particularily property 3) can be useful when
calculating determinants, especially when combined with the idea
of expanding along any row or column.
\begin{example} If $A = \left[ \begin{array}{rrr} -8&7&-2\\2&2&3\\2&1&3 \end{array} \right]$, find $\det(A)$. \newline
We have
$$
\left| \begin{array}{rrr} -8&7&-2\\2&2&3\\2&1&3 \end{array}
\right| \overset{R_2-R_3}{\leadsto} \left| \begin{array}{rrr}
-8&7&-2\\0&1&0\\2&1&3 \end{array} \right| = 1 \left|
\begin{array}{rrr} -8&-2\\2&3 \end{array} \right| = -24+4 = -20.
$$
\end{example}
Notice that property 2 implies that if a matrix has two identical
rows, then its determinant must be zero. This is because if we
switch the identical rows, the matrix stays the same but the
determinant changes sign. The determinant must therefore be zero.

We would like to know what effect the following operations have on
the determinant of $A$: $\det(kA)$, $\det(AB)$ and $\det(A+B)$.
Assuming that $A$ and $B$ are both $n \times n$ square matrices
then:
\begin{enumerate}
\item $\det(A)=\det(A^t)$.
\item $\det(kA)=(k^n)\,\det(A)$.
\item $\det(AB)=\det(A)\det(B)$.
\item If $A$ has two equal rows or columns then its determinant is
zero.
\end{enumerate}

Notice that nothing was said about $\det(A+B)$. Does
$\det(A+B)=\det(A)+\det(B)$?  The following example shows that
this need not be the case.

\begin{example}
Find $\det(A+B)$ and $\det(A)+ \det(B)$.  Given that
$$A=\begin{bmatrix} 1&3\\2&5 \end{bmatrix},\ B=\begin{bmatrix}2&1\\1&6 \end{bmatrix}
\ \mathrm{and}\ A+B = \begin{bmatrix}3&4\\3&11\end{bmatrix}.
$$
Solving the various determinants we find
\begin{align*}
\det(A)&=-1& \det(B)&=11\\
\det(A)+\det(B)&=9& \det(A+B)&=21,
\end{align*}
thus $$\det(A+B) \neq \det(A)+\det(B).$$
\end{example}

\vspace{0.1\baselineskip} \noindent {\bf Activity
\ref{ssec.propdet}}
\begin{enumerate}
\item Calculate the determinants of the following matrices.  Use properties of determinants to simplify
the arithmetic.
$$
\mathrm{(a)}\ \ \begin{bmatrix}2&2&1\\2&1&1\\1&2&1\end{bmatrix} \qquad
\mathrm{(b)}\ \ \left[ \begin{array}{rrrr} 1&2&7&5\\2&1&3&10\\4&-1&-3&20\\8&3&9&39 \end{array} \right]
$$
\item If $\det(B)=10$, $\det(A)=-3$, $\det(C)=2$, and all matrices are $3 \times 3$, find
\begin{itemize}
\item[a.] $\det(A^t)$
\item[b.] $\det(3B)$
\item[c.] $\det(ABC)$
\item[d.] $\det(A+C)$.
\end{itemize}
\end{enumerate}
\noindent The answers can be found in section \ref{answers3}.

\section{Invertibility and the Determinant} \label{ssec.adjoint}\markright{\ref{ssec.adjoint} \titleref{ssec.adjoint}}
Perhaps the most important reason for studying the determinant is because it gives a simple
way of finding whether or not a matrix is invertible.  Recall that a matrix is invertible if and
only if it can be reduced to the identity matrix.  Recall also that the three row reduction operations
\begin{itemize}
\item interchange rows
\item multiply a row by a non--zero constant
\item add a multiple of one row to another
\end{itemize}
have the following effect on the determinant.
\begin{itemize}
\item multiply by -1
\item multiply by a non--zero constant
\item no effect
\end{itemize}
It follows then that if a matrix has a zero determinant, then the reduced form will also have a zero determinant and so the
reduced form cannot be the identity matrix (which has determinant 1).  Hence a matrix with zero determinant is not invertible.
On the other hand if a matrix has non--zero determinant then so does the reduced form.  This means the reduced form cannot have a row of zeros
(otherwise its determinant would be 0).  Hence the reduced form is the identity matrix $I$, and the original matrix is invertible.  Hence we have
the following very useful extension of Theorem~\ref{eq2invert}
\begin{theorem}
\label{eq3invert}
 Let $A$ be an $n \times n$ matrix.  The following statements are equivalent, i.e., if one of them is
true, they are all true, if one of them is false they are all
false.
\begin{enumerate}
\item $A$ is invertible.
\item {\textit A{\bf x}} = {\bf 0}
(the homogeneous system) has only the trivial solution.
\item $A$ is row equivalent to $I_n$.
\item $A{\bf{x}}={\bf {b}}$ is consistent (and has a unique solution) for every $n \times 1$
matrix {\bf b}.\item  $\det(A) \neq 0$. \end{enumerate}
\end{theorem}
If we take determinants in the equation $A^{-1}A = I$, we obtain $|A^{-1}A| = |I|$, or $|A^{-1}|\,|A| = 1$. Hence
$$
|A^{-1}| = \frac{1}{|A|}
$$

\noindent {\bf Activity \ref{ssec.adjoint}}

Is $A$ invertible? If so, what is the determinant of the inverse.

$$A=\left [ \begin{array}{rrrr}
                            1&-1&0&0\\
                            2&1&3&1\\
                            4&1&0&2\\
                            0&1&4&-1 \end{array}\right ] $$
\noindent The answers can be found in section \ref{answers3}.

\section{Summary}\label{ssec.summ3}\markright{\ref{ssec.summ3} \titleref{ssec.summ3}}

{\bf Keywords: inverse;
invertible; singular; determinant; cofactors.}
\\ \\
In this chapter we have introduced the idea of the inverse of a
matrix.  The $n \times n$ matrix $A$ is invertible if there is a $B$
such that $AB = BA = I$, and $B$ is the (unique) inverse of $A$.
If there is no such $B$, then $A$ is not invertible and is referred to as a singular
matrix.
An invertible matrix can be cancelled from matrix equations, so if
$A$ is invertible then $A{\bf{x}} = {\bf{b}}$ has a solution
${\bf{x}} = A^{-1}{\bf {b}}$.

For any square matrix $A$, a scalar called the determinant of $A$ ($\det{(A)}$) can be calculated.  The determinant
is defined in terms of cofactors.  It has many interesting properties that simplify its calculation.  The principal use
of the determinant is to determine invertibility: $A$ is invertible if and only if $\det{(A)}\neq0$.

\input{exercise3a.tex}
\fi

%\end{document}
