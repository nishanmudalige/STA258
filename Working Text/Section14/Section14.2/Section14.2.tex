%\pagenumbering{arabic}
%\setcounter{page}{1}
\setcounter{chapter}{13}
\chapter{Two Sample Hypothesis Test}
%\index{Introduction}
%\label{sec.matrix}
%start relabeling as 2.1 etc
%\pagestyle{myheadings}  \markboth{\ref{sec.matrix}.
%\titleref{sec.matrix}}{}
%\setcounter{equation}{0}

\section{Two Sample Hypothesis Test on Paired Data}

When observations in sample 1 matches with an observation in sample 2. Observations in sample 1 are, usually, highly, correlated with observations in sample 2, these data are often called matched pairs. For each pair (the same cases), we form: Difference = observation in sample 2 - observation in sample 1. Thus, we have one single sample of differences scores. For example, in longitudinal studies: Pre- and post-survey of attitudes towards statistics (Same student is measured twice: Time 1 (pre) and Time 2 (post). We measure change in the attitudes: Post - Pre (for each student). Often these types of studies are called, repeated measures.\\

Paired Data Condition: the data must be quantitative and paired.\\

Independence Assumption:
\begin{itemize}
	\item If the data are p aired, the groups are not independent. For this methods, it is the differences that must be independent of each other.
	\item The pairs may be a random sample.
	\item In experimental design, the order of the two treatments may be randomly assigned, or the treatments ma y be randomly assigned to one member of each pair.
	\item In a before-and-after study, we may believe that the observed differences are representative sample of a population of interest. If there is any doubt, we need to include a control group to be able to draw conclusions.
	\item If samples are bigger than 10 \% of the target population, we need to acknowledge this and note in our report. When we sample from a ﬁnite population, we should be careful not to sample more than 10 \% of that population. Sampling too large a fraction of the population calls the independence assumption into question.
\end{itemize}

Recall chapter 10 when we first introduced paired data, now we are going to use it again:
\begin{center}
\begin{figure}[H]
\centering
\begin{tabular}{ c c c c }
 Sample Units & Measurement 1 ($M_1$) & Measurement 2 ($M_2$) & Difference ($M_2 - M_1$ or $M_1 - M_2$)\\ 
 1 & $x_{11}$ & $x_{12}$ & $x_{d1} = x_{12} -  x_{11}$\\  
 2 & $x_{21}$ & $x_{22}$ & $x_{d2} = x_{22} -  x_{21}$\\
 3 & $x_{31}$ & $x_{32}$ & $x_{d2} = x_{32} -  x_{31}$\\
 ......\\
 n & $x_{n1}$ & $x_{n2}$ & $x_{dn} = x_{n2} -  x_{n1}$\\
\end{tabular}
\caption{A table of paired data}
\end{figure}
\end{center}
\vspace{-0.75cm}

From that table, we can get $\bar{X_d}$, which is the mean, variance and standard deviation of the difference. We need these values to continue our analysis.\\

\textbf{Step 1: Stating the Structure of Testing Hypothesis}

The idea of two sample hypothesis test on pair data is transforming it to one sample hypothesis data, so that our analysis based on $\mu_d$.

\begin{center}
\begin{figure}[H]
\centering
\begin{tabular}{ c c c }
Cases & Null Hypothesis & Alternative Hypothesis \\
     1	   & $\mu_d = 0$ & $\mu_d > 0$ \\
     2	   & $\mu_d = 0$ & $\mu_d < 0$ \\
     3    & $\mu_d = 0$ & $\mu_d \neq 0$ \\
\end{tabular}
\caption{All possible cases of two sample hypothesis test on paired data}
\end{figure}
\end{center}
\vspace{-0.75cm}

\textbf{Step 2: Computing Test Statistics}

\begin{definition}[Test statistics of two sample hypothesis test on paired data]
The test statistics of two sample hypothesis test on paired data is given by: \[ t_* = \frac{\bar{x_d}-0}{s_d / \sqrt{n}} \sim t_{n-1;\alpha/2}.\]
$\bar{x_d}$ is the mean of sample difference and $s_d$ is the standard deviation of difference data.
\end{definition}

\textbf{Step 3: Finding the P-value}

Two sample hypothesis test is quite similar to one sample hypothesis test on a mean. Notes that we are working within 2 measurement. You can refer one sample hypothesis test to get the idea about find the p-value in this case.\\

\textbf{Step 4: Comparing P-value with $\alpha$-level}

If p-value is less than $\alpha$-level, then we reject the null hypothesis ($H_0$) and accept the alternative hypothesis ($H_a$). Otherwise, If p-value is greater than $\alpha$-level, then we do not reject the null hypothesis ($H_0$) and reject the alternative hypothesis ($H_a$).\\

\textbf{Step 5: Final Conclusion}
If we reject the null hypothesis, then we conclude that: there is sufficient evidence to reject the null hypothesis. If we do not reject the null hypothesis, then we conclude that: there is insufficient evidence to reject the null hypothesis.\\

Note that your final conclusion is based on the value of $\mu_d$, but we still need more. We also can know which group has larger means from the result ($\mu_d$).

\begin{itemize}
	\item If $\mu_d > 0$: from the table above we get $M_2-M_1 >0$, then $M_2 > M_1$.
	\item If $\mu_d < 0$: from the table above we get $M_2-M_1 <0$, then $M_2 < M_1$.
	\item If $\mu_d \neq 0$: from the table above we get $M_2-M_1 \neq 0$, then $M_2 \neq M_1$.
\end{itemize}

\begin{example}
In an effort to determine whether a new type of fertilizer is more effective than the type currently in use, researchers took 12 two-acre plots of land scattered throughout the county. Each plot was divided into two equal-size sub plots, one of which was treated with the new fertilizer. Wheat was planted, and the crop yields were measured.

\begin{center}
\begin{figure}[H]
\centering
\begin{tabular}{ c c c c c c c c c c c c c }
Plot & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$ & $11$ & $12$\\
Current & $56$ & $45$ & $68$ & $72$ & $61$ & $69$ & $57$ & $55$ & $60$ & $72$ & $75$ & $66$\\
New & $60$ & $49$ & $66$ & $73$ & $59$ & $67$ & $61$ & $60$ & $58$ & $75$ & $72$ & $68$\\
\end{tabular}
\caption{Data of example 14.1}
\end{figure}
\end{center}
\vspace{-0.75cm}

Can we conclude at the 5\% signiﬁcance level that the new fertilizer is more eﬀective than the current one?

\textbf{Solution:}

You can verify that the mean and standard deviation of the twelve difference measurements are $\bar{d} = new - current = 1$ and $s_d = 3.0151$.

\textbf{Step 1: State Hypothesis}

$H_0: \mu_d = 0$ and $H_a: \mu_d>0$

\textbf{Step 2: Find test statistics}

$t* = \frac{\bar{d} - 0}{s_d/\sqrt{n}} = \frac{1}{3.0151/\sqrt{12}} = 1.1489$

\textbf{Step 3: Compute p-value}

Using t-distribution table with df $=11$, then $0.10 < p-value < 0.15$.

\textbf{Step 4: Conclusion}

Since p-value $> \alpha = 0.05$, we can’t reject $H_0$. There is not enough evidence to infer that the new fertilizer is better.

\end{example}

\section{Two Sample Hypothesis Test on Proportions}

Moreover, we can use hypothesis test to compare proportions from two independent groups as well. We are going to start all these steps again.\\

\textbf{Step 1: Stating the Structure of Testing Hypothesis}

\begin{center}
\begin{figure}[H]
\centering
\begin{tabular}{ c c c }
Cases & Null Hypothesis & Alternative Hypothesis \\
     1	   & $p_1 - p_2 = 0$ & $p_1 - p_2 > 0$ \\
     2	   & $p_1 - p_2 = 0$ & $p_1 - p_2 < 0$ \\
     3    & $p_1 - p_2 = 0$ & $p_1 - p_2 \neq 0$ \\
\end{tabular}
\caption{All possible cases of two sample hypothesis test on proportions}
\end{figure}
\end{center}
\vspace{-0.75cm}

In this case, we are interested in the difference of proportions.

\textbf{Step 2: Computing Test Statistics}

\begin{definition}[Test statistics of two sample hypothesis test on proportions]
To test the hypothesis $H_0 : p_1 = p_2$ first find the pooled proportion $\hat{p}$ of successes in both samples combined. Then compute the $Z*$ statistic: \[ z_* = \frac{\hat{p_1}-\hat{p_2}}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_1}+ \frac{1}{n_2})}} \sim N(0,1).\]
In this case, $\hat{p} = \frac{x_1 + x_2}{n_1+n_2}$, which is the number of success with two groups combined. $n_1 \text{ and } n_2$ represent sample size of each group respectively.
\end{definition}

\textbf{Step 3: Finding the P-value}

In terms of a variable $Z$ having the standard Normal distribution, the approximate P-value for a test of $H_0$ against:

\begin{itemize}
	\item $H_a: p_1 - p_2 > 0$ is $P(Z > Z_*)$;
	\item $H_a: p_1 - p_2 < 0$ is $P(Z < Z_*)$;
	\item $H_a: p_1 - p_2 \neq 0$ is $2 \cdot P(Z > |Z_*|)$.
\end{itemize}

\textbf{Step 4: Comparing P-value with $\alpha$-level}

If p-value is less than $\alpha$-level, then we reject the null hypothesis ($H_0$) and accept the alternative hypothesis ($H_a$). Otherwise, If p-value is greater than $\alpha$-level, then we do not reject the null hypothesis ($H_0$) and reject the alternative hypothesis ($H_a$).\\

\textbf{Step 5: Final Conclusion}

If we reject the null hypothesis, then we conclude that: there is sufficient evidence to reject the null hypothesis. If we do not reject the null hypothesis, then we conclude that: there is insufficient evidence to reject the null hypothesis. Moreover: 

\begin{itemize}
	\item If $p_1 - p_2 > 0$: if we reject the null hypothesis under this alternative test, then $p_1 > p_2$.
	\item If $p_1- p_2 < 0$: if we reject the null hypothesis under this alternative test, then $p_1 < p_2$.
	\item If $p_1 - p_2 \neq 0$: if we reject the null hypothesis under this alternative test, then $p_1 \neq p_2$.
\end{itemize}

\textbf{Conditions of Two Sample Hypothesis Test on Proportions}

i. Independent Response Assumption:
Within each group , we need independent responses from the cases. We cannot check that for certain, but randomization provides evidence of independence. So, we need to check the following:
\begin{itemize}
	\item Randomization Condition: The data in each group should be drawn independently and at random from a population or generated by a completely randomized designed experiment.
	\item The 10 \% Condition: If the data are sampled without replacement, the sample should not exceed 10 \% of the population. If samples are bigger than 10 \% of the target population, random draws are no longer approximately independent.
	\item Independent Groups Assumption: The two groups we are comparing must be independent from each other.
\end{itemize}

ii. Sample Size Condition Each of the groups must be big enough. As with individual proportions, we need larger group s to estimate proportions that are near 0\% and 100\%. We check the success / failure condition for each group.
\begin{itemize}
	\item Success / Failure Condition: Both groups are big enough that at least 10 successes and at least 10 failures have been observed in each group or will be expected in each (when testing hypothesis).
\end{itemize}

Note: Two-sided significance tests (later we will discuss this concept) are robust against violations of this condition. In this case, we can conduct significance tests with smaller sample sizes. In practice, the two-sided significance test works well if there are at least five successes and five failures in each sample.

\begin{example}
Nicotine patches are often used to help smokers quit. Does giving medicine to fight depression help? A randomized double-blind experiment assigned 244 smokers who wanted to stop to receive nicotine patches and another 245 to receive both a patch and the anti-depression drug bupropion. Results: After a y ear, 40 subjects in the nicotine patch group had abstained from smoking, as had 87 in the patch-plus-drug group. How significant is the evidence that the medicine increases the success rate? State hypotheses, calculate a test statistic, use Table 6 to give its P-value, and state y our conclusion. (Use $\alpha = 0.01$)\\
Solution:

\textbf{Step 1: State Hypothesis}

$H_0: p_1 = p_2$ and $H_a: p_1 < p_2$

\textbf{Step 2: Find test statistics}

$\hat{p_1} = \frac{40}{244} = 0.1639$ and $\hat{p_2} = \frac{87}{245} = 0.3551$. Then $\hat{p} = \frac{40+87}{244+245} = 0.2597$.

Now, $z_* = \frac{\hat{p_1} - \hat{p_2}}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_1} + \frac{1}{n_2})}} = -4.82$

\textbf{Step 3: Compute p-value}

P-value $= P(Z < z_*) = P(Z < -4.82) < 0.0003$

\textbf{Step 4: Conclusion}

Since p-value $ < 0.0003 < \alpha = 0.01$, we reject the null hypothesis that $p_1 = p_2$. The data provide very strong evidence that bupropion increases success rate.

\textbf{R-code:}

Input
\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
successes=c(87, 40);

totals=c(245, 244);

prop.test(successes, totals, alternative="greater",correct=FALSE);
\end{verbatim}
\end{tcolorbox}

Output
\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}

## 
## 2-sample test for equality of proportions without 
## continuity correction 
## 
## data: x and n
## X-squared = 23.237, df = 1, p-value = 7.161e-07
## alternative hypothesis: greater 
## 95 percent confidence interval: 
## 0.1275385 1.0000000 
## sample estimates:
##    prop 1    prop 2 
## 0.3551020 0.1639344

\end{verbatim}
\end{tcolorbox}

\end{example}

\section{Two Sample Hypothesis Test on Variances}

Let's begin this type of hypothesis test with a case. The question is: how do you know whether the homogeneity of variance assumption is satisﬁed? One simple method involves just looking at two sample variances. Logically, if two population variances are equal, then the two sample variances should be very similar. When the two sample variances are reasonably close, you can be reasonably conﬁdent that the homogeneity assumption is satisﬁed and proceed with, for example, Student t-interval. However, when one sample variance is three or four times larger than the other, then there is reason for a concern. The common statistical procedure for comparing population variances $\sigma_1^2$ and $\sigma_2^2$ makes an inference about the ratio of $\sigma_1^2$/$\sigma_2^2$.\\

\textbf{Step 1: Stating the Structure of Testing Hypothesis}

\begin{center}
\begin{figure}[H]
\centering
\begin{tabular}{ c c c }
Cases & Null Hypothesis & Alternative Hypothesis \\
     1	   & $\sigma_1^2 = \sigma_2^2$ & $\sigma_1^2 > \sigma_2^2$ \\
     2	   & $\sigma_1^2 = \sigma_2^2$ & $\sigma_1^2 < \sigma_2^2$ \\
     3    & $\sigma_1^2 = \sigma_2^2$ & $\sigma_1^2 \neq \sigma_2^2$ \\
\end{tabular}
\caption{All possible cases of two sample hypothesis test on variances}
\end{figure}
\end{center}
\vspace{-0.75cm}

\textbf{Step 2: Computing Test Statistics}

\begin{definition}[Test statistics of two sample hypothesis test on variances]
The test statistics of two sample hypothesis test on variances is given by: \[ F_* = \frac{s_1^2}{s_2^2} \sim F_{n_1-1,n_2-1}.\]
\end{definition}

\textbf{Decision Rules}

\begin{itemize}
	\item $H_0: \sigma_1^2 = \sigma_2^2$ and $H_a:\sigma_1^2 \neq \sigma_2^2$. If $F^* > F_{n_1-1,n_2-1,\alpha/2}$ or $F^* < F_{n_1-1,n_2-1,1 - \alpha/2}$, then we reject $H_0$. Otherwise, we do not reject it.
	\item $H_0: \sigma_1^2 = \sigma_2^2$ and $H_a: \sigma_1^2 > \sigma_2^2$. If $F^*> F_{n_1-1,n_2-1,\alpha}$ or $P(F_{n_1-1,n_2-1} > F^*)$ is too small, then we reject $H_0$. Otherwise, we do not reject it.
	\item $H_0: \sigma_1^2 = \sigma_2^2$ and $H_a: \sigma_1^2 < \sigma_2^2$. if $F^* < F_{n_1-1,n_2-1,1-\alpha}$ or $P(F_{n_1-1,n_2-1} < F^*)$ is too small, then we reject $H_0$. Otherwise, we do not reject it.
\end{itemize}






