\setcounter{chapter}{3} % Makes this Chapter 4
\chapter{Normal Approximation to the Binomial Distribution}

\section{Introduction}

\begin{definition}[Statistic]
A \textit{statistic} is a function of the observable random variables in a sample and known constants.

Since statistics are functions of the random variables observed in a sample, they themselves are random variables. As such, all statistics have a corresponding probability distribution, which we refer to as their \textit{sampling distribution}.
\end{definition}
\begin{tcolorbox}[title=Review from STA256,
  colback=gray!5, 
  colframe=gray!40!black, 
  coltitle=black, 
  colbacktitle=gray!20,  % <- softer title bar
  fonttitle=\bfseries,
  sharp corners=south,
  breakable]


\textbf{Bernoulli Distribution:}

A Bernoulli trial is a single experiment with two outcomes:
\begin{itemize}
  \item Success: \( X = 1 \) with probability \( p \)
  \item Failure: \( X = 0 \) with probability \( 1 - p \)
\end{itemize}

\medskip

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|c|c|c|}
\hline
\( X = x \) & 0 & 1 \\
\hline
\( P(X = x) \) & \( 1 - p \) & \( p \) \\
\hline
\end{tabular}
\end{center}

The probability mass function (PMF) is:
\[
f(x) = p^x (1 - p)^{1 - x}, \quad x \in \{0, 1\}
\]

\medskip

\textbf{Binomial Distribution:}

A binomial distribution arises from \( n \) independent Bernoulli trials. Let:
\[
X = \text{number of successes in } n \text{ trials}
\]
Then:
\[
X \sim \text{Binomial}(n, p)
\]

where:
\begin{itemize}
  \item Each trial results in either success (with probability \( p \)) or failure (with probability \( 1 - p \))
  \item \( X \in \{0, 1, \dots, n\} \)
\end{itemize}

The PMF is:
\[
P(X = x) = \binom{n}{x} p^x (1 - p)^{n - x}
\]

\medskip

\textbf{Moment Generating Function (MGF):}

The moment generating function (MGF) of a random variable \( X \) is defined as:
\[
M_X(t) = \mathbb{E}[e^{tX}]
\]
The MGF uniquely characterizes the distribution of \( X \) (if it exists in an open interval around 0), and it can be used to compute moments such as the mean and variance.


\end{tcolorbox}

\section{Bernoulli Distribution}
Bernoulli random variable is a discrete random variable that has exactly two possible outcomes which are either a \textbf{success} or a \textbf{failure}. An experiment in which there are exactly 2 outcomes (which are success or failure) is called a \textbf{Bernoulli trial}. \\
\vspace{1,0em} \\
When \( x = 1 \) we have a success and when \( x = 0 \) we have a failure. The term success and failure are relative to the problem being studied.

\begin{tcolorbox}[title=\textbf{TIP: ``success'' need not be something positive}, 
  colback=cyan!5, 
  colframe=cyan!40!black, 
  coltitle=black, 
  colbacktitle=cyan!10,
  fonttitle=\bfseries,
  sharp corners=south,
  breakable]

We chose to label a person who refuses to administer the worst shock a ``success'' and all others as ``failures''. However, we could just as easily have reversed these labels. The mathematical framework we will build does not depend on which outcome is labeled a success and which a failure, as long as we are consistent.

\end{tcolorbox}

\vspace{1,0em} 

Consider the random experiment of rolling a die once. Define the random variable:

\[
X_i =
\begin{cases}
1 & \text{if the } i\text{-th roll is a six}, \\
0 & \text{otherwise}
\end{cases}
\]

Then \( X_i \sim \text{Bernoulli}(p) \), where \( p = P(\text{rolling a six}) \). 
\begin{tcolorbox}[
  colback=yellow!10,
  colframe=yellow!10,
  boxrule=0pt,
  sharp corners=south,
  enhanced,
  breakable]

\textit{Let \( X \sim \text{Bernoulli}(p) \). The mass function of \( X \) is}

\[
P(X = x) = p^x (1 - p)^{1 - x}, \quad x = 0, 1
\]

\textit{where \( p \) represents the probability of success.}

\end{tcolorbox}




\begin{definition}[Mean and Variance of a Bernoulli Random Variable]
Let \( X \sim \text{Bernoulli}(p) \). The mean of \( X \) is
\[
E(X) = \mu = p
\]
and the variance of \( X \) is
\[
\text{Var}(X) = \sigma^2 = p(1 - p)
\]
\end{definition} 
\textit{To support the earlier result, we now provide a derivation of the mean, variance, and standard deviation of a Bernoulli random variable.}
\vspace{1,0em}  \\
Let \( X \) be a Bernoulli random variable with the probability of a success as \( p \). Then

\[
E[X] = \mu = \sum_{i=1}^{n} x_i \cdot P(X = x_i)
\]
\[
= 0 \cdot P(X = 0) + 1 \cdot P(X = 1)
\]
\[
= 0 \cdot (1 - p) + 1 \cdot p
\]
\[
= p
\]

Similarly, the variance of \( X \) can be computed:

\[
V(X) = \sigma^2 = \sum_{i=1}^{k} (x_i - \mu)^2 \cdot P(X = x_i)
\]
\[
= (0 - p)^2 \cdot P(X = 0) + (1 - p)^2 \cdot P(X = 1)
\]
\[
= p^2 (1 - p) + (1 - p)^2 p
\]
\[
= p(1 - p)
\]

The standard deviation is

\[
\sigma = \sqrt{\sigma^2}
\]
\[
= \sqrt{p(1 - p)}
\]




\section{Sampling Distribution of the Sum and MGF Derivation}

Consider determining the sampling distribution of the sample total:
\[
T_n = X_1 + X_2 + \dots + X_n
\]
Suppose \( X_i \overset{iid}{\sim} \text{Bernoulli}(p) \). Then the moment-generating function of \( T_n \) is:

\begin{align*}
M_{T_n}(t) &= \mathbb{E}[e^{t T_n}] \\
           &= \mathbb{E}\left[e^{t(X_1 + X_2 + \dots + X_n)}\right] \\
           &= \mathbb{E}\left[e^{tX_1} e^{tX_2} \dots e^{tX_n} \right] \quad \text{(independence)} \\
           &= \mathbb{E}[e^{tX_1}] \cdot \mathbb{E}[e^{tX_2}] \cdots \mathbb{E}[e^{tX_n}] \\
           &= M_{X_1}(t) \cdot M_{X_2}(t) \cdots M_{X_n}(t) \\
           &= \left[pe^t + (1 - p)\right]^n
\end{align*}


Since this is the MGF of a binomial random variable with parameters \( n \) and \( p \), we conclude:

\[
T_n \sim \text{Binomial}(n, p)
\]
\medskip
\begin{tcolorbox}[title=Example: Binomial Distribution from Die Rolls,
  colback=blue!5, colframe=blue!40!black, coltitle=black,
  colbacktitle=blue!10!white, fonttitle=\bfseries, breakable]

We can think of rolling a die \( n \) times as an example of the binomial setting. Each roll gives either a six (a â€œsuccessâ€) or a number different from six (a â€œfailureâ€).

Knowing the outcome of one roll doesnâ€™t tell us anything about the others, so the \( n \) rolls are independent.

If we call a six a success, then:
\begin{itemize}
  \item The probability of success on each trial is \( p = P(\text{rolling a six}) = \frac{1}{6} \)
  \item The probability of failure is \( 1 - p = \frac{5}{6} \)
\end{itemize}

Let \( Y \) be the number of sixes rolled in \( n \) trials. Then \( Y \sim \text{Binomial}(n, p) \), and the distribution of \( Y \) is called a \textbf{binomial distribution}.
\end{tcolorbox}


\section{Binomial Distribution} % or \section{4.4 Binomial Distribution} if numbering manually

In section 4.2 we learnt about Bernoulli random variables in which we were interested in the outcome of just a single trial. A \textbf{binomial random variable} is a generalization of several independent Bernoulli trials. Instead of performing just a single Bernoulli trial and observing whether we have a success or not, we are now performing several Bernoulli trials and observing whether we have a certain number of successes and failures. The \textbf{binomial distribution} describes the probability of having exactly \( k \) successes in \( n \) independent Bernoulli trials with probability of a success \( p \). \\
\begin{tcolorbox}[
  colback=yellow!10,
  colframe=yellow!10,
  boxrule=0pt,
  sharp corners=south,
  enhanced,
  breakable]

\textit{Let \( X \sim \text{Bin}(n, p) \). The probability of observing \( x \) successes in these \( n \) independent trials is given by}

\[
P(X = x) = \binom{n}{x} p^x (1 - p)^{n - x}
\]

\textit{where}
\begin{itemize}
  \item \( n \) represents the number of trials,
  \item \( x \) represents the number of successes,
  \item \( p \) represents the probability of success on any given trial,
\end{itemize}

\[
\binom{n}{x} = \frac{n!}{x!(n - x)!} \quad \text{is the binomial coefficient.}
\]

\end{tcolorbox}

\begin{definition}[Mean and Variance of a Binomial Random Variable]
Let \( X \sim \text{Bin}(n, p) \). The mean of \( X \) is
\[
E(X) = \mu = np
\]
\textit{and the variance of \( X \) is}
\[
\text{Var}(X) = \sigma^2 = np(1 - p)
\]
\end{definition}



\subsection{Visualizing the PMF of Binomial Distributions}

\noindent\textcolor{gray!40}{\textbf{R code:}}

\vspace{0.5em}

{\ttfamily
\textcolor{violet!70!black}{\#\# Pmf of Binomial with n=10 and p=1/6.} \\

x \textcolor{red}{$\leftarrow$} \textcolor{red}{seq}(\textcolor{magenta}{0}, \textcolor{magenta}{10}, \textcolor{green!50!black}{by}=\textcolor{magenta}{1}); \\
y \textcolor{red}{$\leftarrow$} \textcolor{red}{dbinom}(x, \textcolor{magenta}{10}, \textcolor{magenta}{1}\textcolor{black}{/}\textcolor{magenta}{6}); \\

\textcolor{red}{plot}(x, y, \textcolor{green!50!black}{type}= \textcolor{blue}{"p"}, \textcolor{green!50!black}{col}=\textcolor{blue}{"blue"}, \textcolor{green!50!black}{pch}=\textcolor{magenta}{19});
}\\


\textbf{Probability Mass Functions (PMFs) for increasing \(n\):}

\vspace{0.5em}

The following plots display the probability mass functions (PMFs) for a binomial distribution with \( p = \frac{1}{6} \) and increasing values of \( n \). As \( n \) increases, the binomial distribution begins to resemble a normal distribution.


\begin{figure}[h!]
  \centering
 \includegraphics[width=0.8\textwidth]{Section4/pmf_plot.pdf}
 \caption{PMF of Binomial distribution with \(n = 10\) and \(p = \frac{1}{6}\).}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{Section4/pmf_n50.pdf}
  \caption{PMF of Binomial distribution with \(n = 50\) and \(p = \frac{1}{6}\).}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{Section4/pmf_n100.pdf}
  \caption{PMF of Binomial distribution with \(n = 100\) and \(p = \frac{1}{6}\).}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{Section4/pmf_n300.pdf}
  \caption{PMF of Binomial distribution with \(n = 300\) and \(p = \frac{1}{6}\).}
\end{figure}
% ðŸ” Force figures to be placed before moving on
\clearpage


\section{Sampling Distribution of a Sample Proportion and the Normal Approximation}
When studying categorical data, we are often interested not just in individual outcomes, but in the proportion of successes observed in a sample. Understanding how this proportion behaves across repeated samples is crucial for making inferences about a population. In this section, we explore the sampling distribution of a sample proportion and how it can be approximated by a normal distribution under certain conditions.\\
\vspace{0.5em}

Draw a \textit{Simple Random Sample (SRS)} of size \( n \) from a large population that contains proportion \( p \) of â€œsuccessesâ€. Let \( \hat{p} \) be the \textbf{\textit{sample proportion}} of successes:

\[
\hat{p} = \frac{\text{number of successes in the sample}}{n}
\]

Then:

\begin{itemize}
  \item The \textbf{mean} of the sampling distribution of \( \hat{p} \) is \( p \).
  \item The \textbf{standard deviation} of the sampling distribution is \( \sqrt{ \frac{p(1 - p)}{n} } \).
\end{itemize}


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{Section4/binomial_normal.pdf}
  \caption{Binomial distribution with \(n = 300\), \(p = \frac{1}{6}\) and its Normal approximation.}
\end{figure}


According to the Central Limit Theorem (CLT), the sampling distribution of a sample proportion becomes approximately normal as the sample size increases.

That is:
\[
\hat{p} \sim \mathcal{N}\left(p, \sqrt{\frac{p(1 - p)}{n}}\right)
\]

This approximation is most accurate when both \( np \geq 10 \) and \( n(1 - p) \geq 10 \).

These are called the \textbf{success-failure conditions}.

\medskip

\textit{Key Point:} When the success-failure conditions are met, the normal approximation to the sampling distribution of \( \hat{p} \) can be used for probability calculations.
\subsection*{Conditions for Using the Normal Approximation}

\vspace{0.5em}

Suppose \( X \sim \text{Binomial}(n, p) \). Then:

\[
\mu = np, \quad \sigma^2 = np(1 - p)
\]

\medskip

\textbf{Binomial probabilities can be approximated by the normal distribution:}
\[
X \approx \mathcal{N}(np, \, np(1 - p))
\]

This approximation is \textit{useful for large \( n \)} and valid under the following conditions:

\begin{tcolorbox}[title=Standard Conditions,
  colback=blue!5, 
  colframe=blue!50!black, 
  coltitle=black,
  colbacktitle=blue!20, % â† LIGHTER blue for the title bar
  fonttitle=\bfseries,
  sharp corners=south,
  boxrule=0.5pt,
  enhanced,
  width=\textwidth,
  breakable]
The binomial setting holds (i.e., independent trials, fixed \( n \), same probability \( p \)) and

\[
np \geq 10 \quad \text{and} \quad np(1 - p) \geq 10
\]
\end{tcolorbox}

\vspace{0.5em}

Alternatively, a more conservative criterion for using the normal approximation is:
\[
n > 9 \cdot \left( \frac{\max(p, \, 1 - p)}{\min(p, \, 1 - p)} \right)
\]

\medskip

These ensure that the binomial distribution is sufficiently symmetric and smooth to approximate with the normal distribution.

\medskip
We derive the sampling distribution of \( \hat{p} \) using properties of the Bernoulli distribution.

\subsection*{Bernoulli Distribution (Binomial with $n = 1$)}

\[
X_i =
\begin{cases}
1 & \text{if the $i$-th roll is a six} \\
0 & \text{otherwise}
\end{cases}
\]

\[
\mu = \mathbb{E}(X_i) = p, \quad \sigma^2 = \mathrm{Var}(X_i) = p(1 - p)
\]

Let $\hat{p}$ be our estimate of $p$. Note that $\hat{p} = \frac{1}{n} \sum_{i=1}^{n} X_i = \bar{X}$.
Let $\hat{p} = \frac{\text{\# successes } (X)}{\text{sample size } (n)}$

Recall that for $X \sim \text{Binomial}(n, p)$:
\[
X \overset{\cdot}{\sim} \mathcal{N}(np, np(1 - p))
\]

Let $\hat{p} = \frac{X}{n}$

\paragraph*{Mean of $\hat{p}$:}

\[
\mathbb{E}(\hat{p}) = \mathbb{E} \left( \frac{X}{n} \right) = \frac{1}{n} \cdot \mathbb{E}(X) = \frac{1}{n} \cdot np = p
\]

\paragraph*{Variance of $\hat{p}$:}

\[
\mathrm{Var}(\hat{p}) = \mathrm{Var} \left( \frac{X}{n} \right) = \frac{1}{n^2} \cdot \mathrm{Var}(X) = \frac{1}{n^2} \cdot np(1 - p) = \frac{p(1 - p)}{n}
\]

By the Central Limit Theorem (CLT), for sufficiently large $n$:
\[
\hat{p} \sim \mathcal{N} \left( p, \frac{p(1 - p)}{n} \right)
\]

\paragraph*{Standardization of $\hat{p}$:}
\[
Z = \frac{\hat{p} - p}{\sqrt{ \frac{p(1 - p)}{n} }}
\]

If $n$ is large, then by the Central Limit Theorem:
\[
\bar{X} \approx \mathcal{N} \left( \mu, \frac{\sigma}{\sqrt{n}} \right)
\quad \Rightarrow \quad
\hat{p} \sim \mathcal{N} \left( p, \sqrt{\frac{p(1 - p)}{n}} \right)
\]
\begin{example}[Normal Approximation for Proportions]
In the last election, a state representative received 52\% of the votes cast. One year after the election, the representative organized a survey that asked a random sample of 300 people whether they would vote for him in the next election. If we assume that his popularity has not changed, what is the probability that more than half the sample would vote for him?


\vspace{1em}
\subsubsection*{Solution 1 (using Normal Approximation)}

We want to determine the probability that the sample proportion is greater than 50\%. In other words, we want to find $P(\hat{p} > 0.50)$.

We know that the sample proportion $\hat{p}$ is roughly Normally distributed with mean $p = 0.52$ and standard deviation
\[
\sqrt{p(1 - p)/n} = \sqrt{(0.52)(0.48)/300} = 0.0288.
\]
Thus, we calculate
\[
P(\hat{p} > 0.50) = P\left( \frac{\hat{p} - p}{\sqrt{p(1-p)/n}} > \frac{0.50 - 0.52}{0.0288} \right)
\]
\[
= P(Z > -0.69) = 1 - P(Z < -0.69) \quad \text{(Z is symmetric)}
\]
\[
= P(Z > -0.69) = 1 - P(Z > 0.69)
\]
\[
= 1 - 0.2451 = 0.7549.
\]

If we assume that the level of support remains at 52\%, the probability that more than half the sample of 300 people would vote for the representative is 0.7549.
\paragraph*{R code (Normal approximation)}

Just type in the following:

\begin{verbatim}
1 - pnorm(0.50, mean = 0.52, sd = 0.0288)
## [1] 0.7562982
\end{verbatim}

Recall that, \texttt{pnorm} will give you the area to the left of 0.50, for a Normal distribution with mean 0.52 and standard deviation 0.0288.
\subsubsection*{Solution 2 (using Binomial)}

We want to determine the probability that the sample proportion is greater than 50\%. In other words, we want to find $P(\hat{p} > 0.50)$. We know that 
$n = 300$ and $p = 0.52$. \\
Thus, we calculate
\begin{align*}
P(\hat{p} > 0.50) &= P\left(\frac{\sum_{i=1}^{n} x_i}{n} > 0.50\right) \\
&= P\left(\sum_{i=1}^{300} x_i > 150\right) \\
&= 1 - P\left(\sum_{i=1}^{300} x_i \leq 150\right) \\
&\text{(it can be shown that } Y = \sum_{i=1}^{300} x_i \text{ has a Binomial distribution with} \\
&n = 300 \text{ and } p = 0.52\text{)} \\
&= 1 - F_Y(150)
\end{align*}

\paragraph*{R code (using Binomial distribution )}

Just type in the following:

\begin{verbatim}
1- pbinom(150, size = 300, prob = 0.52);
## [1] 0.7375949
\end{verbatim}

Recall that, \texttt{pbinom} will give you the CDF at 150, for a Binomial distribution with $n = 300$ and $p = 0.52$.

\subsection*{Solution 3 (using continuity correction)}


We have that $n = 300$ and $p = 0.52$.
Thus, we calculate
\begin{align*}
P(\hat{p} > 0.50) &= P\left( \frac{\sum_{i=1}^{n} x_i}{n} > 0.50 \right) \\
&= P\left( \sum_{i=1}^{300} x_i > 150 \right) \\
&= 1 - P\left( \sum_{i=1}^{300} x_i \leq 150 \right) \\
&\text{(it can be shown that } Y = \sum_{i=1}^{300} x_i \text{ has a Binomial distribution with} \\
&n = 300 \text{ and } p = 0.52\text{)}. \\
&\approx 1 - P\left( \sum_{i=1}^{300} x_i \leq 150.5 \right) \quad \text{(continuity correction)} \\
&= 1 - P\left( \frac{\sum_{i=1}^{300} x_i}{n} \leq \frac{150.5}{300} \right) \\
&= 1 - P(\hat{p} \leq 0.5017) \\
&= 1 - P\left( Z \leq -0.6354 \right) \quad \text{(Why?)}
\end{align*}
\paragraph*{R code (Normal approximation with continuity correction)}

Just type in the following:

\begin{verbatim}
1 - pnorm(0.5017, mean = 0.52, sd = 0.0288)
## [1] 0.7374216
\end{verbatim}

Recall that, \texttt{pnorm} will give you the area to the left of 0.5017, for a Normal distribution with mean 0.52 and standard deviation 0.0288.
\end{example}
\section{Continuity Correction}
The normal distribution is continuous, while the binomial distribution is discrete. When we approximate a binomial probability using the normal distribution, this mismatch can lead to inaccuracyâ€”especially near the boundaries of discrete values. A continuity correction improves the approximation by adjusting for this difference. In this section, we explore how and why this correction is applied. \\
\vspace{0.5em} \\
Suppose that $Y$ has a Binomial distribution with $n = 20$ and $p = 0.4$. We will find the exact probabilities that $Y \leq y$ and compare these to the corresponding values found by using two Normal approximations. One of them, when $X$ is Normally distributed with $\mu_X = np$ and $\sigma_X = \sqrt{np(1 - p)}$.
\vspace{1em}
The other one, $W$, a shifted version of $X$.

\vspace{1em}

For example,
\[
P(Y \leq 8) = 0.5955987
\]

As previously stated, we can think of $Y$ as having approximately the same distribution as $X$.
\[
P(Y \leq 8) \approx P(X \leq 8)
= P\left[ \frac{X - np}{\sqrt{np(1 - p)}} \leq \frac{8 - 8}{\sqrt{20(0.4)(0.6)}} \right]
= P(Z \leq 0) = 0.5
\]

\vspace{1em}

\[
P(Y \leq 8) \approx P(W \leq 8.5)
= P\left[ \frac{W - np}{\sqrt{np(1 - p)}} \leq \frac{8.5 - 8}{\sqrt{20(0.4)(0.6)}} \right]
= P(Z \leq 0.2282) = 0.5902615
\]

% Images from slides 37 and 38 will be inserted here.
\begin{example}


Fifty-one percent of adults in the U. S. whose New Yearâ€™s resolution was to exercise more achieved their resolution. You randomly select 65 adults in the U. S. whose resolution was to exercise more and ask each if he or she achieved that resolution. What is the probability that exactly forty of them respond yes?\\

We are given that $p = 0.51$, $n = 65$, and we want to find $P(X = 40)$ where $X \sim Binomial(n = 65, p = 0.51)$.\\

\textbf{Use Normal Approximation}
\vspace{0.5 em}
We use normal approximation to the binomial. First, compute the mean and standard deviation:
\begin{align*}
\mu &= np = 65 \times 0.51 = 33.15 \\
\sigma^2 &= np(1-p) = 65 \times 0.51 \times 0.49 = 16.485 \\
\sigma &= \sqrt{16.485} \approx 4.06
\end{align*}


We apply continuity correction:
\[
P(X = 40) = P(39.5 \leq X \leq 40.5)
\]

\[
= P\left(\frac{39.5 - 33.15}{4.06} \leq Z \leq \frac{40.5 - 33.15}{4.06}\right) = P(1.56 \leq Z \leq 1.81)
\]

From the standard normal table:
\[
= P(Z \leq 1.81) - P(Z \leq 1.56) = 0.0594 - 0.0352 = 0.0242
\]

So the approximate probability is:
\[
P(X = 40) \approx 0.0242
\]
\end{example}
\vspace{1em}
\begin{frame}{Normal Approximation to Binomial}

Let $X = \sum_{i=1}^{n} Y_i$ where $Y_1, Y_2, \ldots, Y_n$ are iid Bernoulli random variables. Note that $X = n\hat{p}$.

\begin{enumerate}
    \item $n\hat{p}$ is approximately Normally distributed provided that $np \geq 10$ and $n(1 - p) \geq 10$.

    \item Another criterion is that the Normal approximation is adequate if
    \[
    n > 9 \left( \frac{\text{larger of $p$ and $q$}}{\text{smaller of $p$ and $q$}} \right)
    \]

    \item The expected value: $E(\hat{p}) = np$.

    \item The variance: $V(\hat{p}) = np(1 - p) = npq$.
\end{enumerate}

\end{frame}

\vspace{1\baselineskip}  % Adds vertical space after the frame





