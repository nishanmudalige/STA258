\pagenumbering{arabic}
%\setcounter{page}{1}
\chapter{Sampling Distributions Related to a Normal Population}
\index{Introduction}
\label{sec.matrix}
%start relabeling as 2.1 etc
\pagestyle{myheadings}  \markboth{\ref{sec.matrix}.
\titleref{sec.matrix}}{}
%\setcounter{equation}{0}

Previously, we have introduced lots of definitions and given you a rough idea about what really statistics it and what people do in statistics. Now, we are going to proceed statistical distributions. 

\section{Normal Distribution}

In probability theory and statistics, normal distribution also called Gaussian distribution which is discovered by a famous German mathematician Johann Carl Friedrich Gauss in 1809. It is one of the most important distribution that used to approximate other types of probability distribution, such as binomial, hypergeometric, inverse (or negative) hypergeometric, negative binomial and Poisson distribution. Generally, it is denote as $N(\mu, \sigma^2)$ with probability density function as the following: \[ f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}}.\]
Formally, let's begin with its definition:

\begin{definition}[Normal Distribution]
Suppose a random variable $X \sim N(\mu, \sigma^2)$, then $E(X) = \mu \text{ and } Var(X) = \sigma^2$. And $-\infty < \mu < \infty, \sigma^2 > 0.$ Moreover, $X$ has probability density function as: \[ f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}}, \text{ for $-\infty < x < \infty$ (same as above).}\]
The only special case of normal distribution is standard normal distribution, such that a random variable $Y \sim N( \mu = E(Y) = 0, \sigma^2 = Var(Y) = 1)$, then $Y$ has probability density function as: \[ f(y) = \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{y^2}{2}}.\]
\end{definition}

\section{Gamma and Chi-square Distribution}

The Chi-square and Gamma distributions are two fundamental probability distributions widely used in statistical theory and applications. The Gamma distribution is a continuous distribution characterized by its shape and scale parameters, making it versatile for modeling waiting times and various positively skewed data. The Chi-square distribution, a special case of the Gamma distribution, arises naturally in the context of hypothesis testing and confidence interval estimation, especially in tests involving variance and categorical data.\\

\textbf{Gamma Distribution}

\begin{definition}[Gamma Distribution]
Suppose a random variable $X$ is Gamma distributed with $\alpha > 0$ (shape parameter) and $\beta > 0$ (scale parameter) if and only if the probability density function of $X$ is \[ f(x) = \frac{x^{\alpha - 1} e^{\frac{-x}{\beta}}}{\beta^{\alpha} \Gamma(\alpha)}, \text{ for $0 < x < \infty.$}\]
Then, $E(X) = \alpha \beta \text{, } Var(X) = \alpha \beta^2 \text{ and its moment generating function is } M_{X}(t) = \frac{1}{(1 - \beta t)^{\alpha}},$ for $t < \frac{1}{\beta}.$
\end{definition}

Now, let's introduce some properties of Gamma function:

\begin{itemize}
	\item Gamma function (\textbf{not a distribution}): \[ \Gamma(x) = \int_{0}^{\infty}t^{x-1}e^{-t}\,dt, \text{ for $x > 0$.} \]
	\item Properties
		\begin{itemize}
			\item 1. $\Gamma(x) = x \cdot \Gamma(x-1)$;
			\item 2. For all $n \in \mathbb{N} \text{, } \Gamma(n) = (n - 1)!$; 
			\item 3. $\Gamma(\frac{1}{2}) = \sqrt{\pi}$.
		\end{itemize}
\end{itemize}

\textbf{Chi-square Distribution}

\begin{definition}[Chi-square Distribution]
A random variable $X$ has a Chi-squared distribution with $n$ degrees of freedom $(\chi_{n}^{2})$ if and only if $X$ is a random variable with a Gamma distribution with parameters $\alpha = \frac{n}{2} \text{ and } \beta = 2.$ Then, the probability density function of $X$ is given by \[ f(x) = \frac{1}{2^{\frac{n}{2}} \Gamma(\frac{n}{2})} x^{\frac{k}{2} - 1} e^{\frac{-x}{2}}.\] Moreover, $E(X) = n \text{, } Var(X) = 2n \text{ and moment generating function of $X$ is } M_{X}(t) = (1 - 2t)^{\frac{-n}{2}}, \text{ for $t < \frac{1}{2}$}.$ 
\end{definition}

We claim that Chi-square distribution is a special case of Gamma distribution with $\alpha = \frac{n}{2} \text{ and } \beta = 2$. Now, let's prove it by using moment generating function.\\

The proof is quite straightforward as the following shows:

\begin{proof} Suppose $X \sim Gamma( \alpha = \frac{n}{2} \text{, } \beta = 2).$ \\
Then the following moment generating function holds for $X$: \[ M_{X}(t) = (1- 2t)^{\frac{-n}{2}}, \text{ for $t < \frac{1}{2}$}.\]
Compare the moment generating function of $X$ under Gamma distribution with Chi-square distribution, we can conclude that $X \sim \chi_{n}^{2}$.
\end{proof}

\textbf{Obtaining Chi-square Distribution by Normal Distribution}

Previously, we showed how to use Gamma distribution to get Chi-square distribution by moment generating function method. Now, let's do something interestingly, to use normal distribution to get Chi-square distribution. We will begin with a theorem, then prove it.

\begin{theorem}[$Z^{2} \sim \chi_{1}^{2}$]
Suppose a random variable $Z$ is standard normally distributed, such that $Z \sim N(0 \text{, }1).$ Then, $Z^2$ is Chi-square distributed with $1$ degree of freedom, so that $Z^2 \sim \chi_{1}^{2}$.
\end{theorem}

The proof of Theorem $2.1$ isn't that trivial to see. We still need moment generating function, but in a different way. Before we get into the proper proof, let's grab everything we need:

\begin{itemize}
	\item 1. Recall STA256 about how to get moment generating function for a given continuous random variable that: \[ M_{Z}(t) = \int_{-\infty}^{\infty} e^{tx}f_{X}(x)\,dx.\]
	\item 2. We also need Gaussian integral:
	\begin{itemize}
		\item (i) \[ \int_{-\infty}^{\infty} e^{-x^2}\,dx = \sqrt{\pi};\]
		\item (ii) \[ \int_{-\infty}^{\infty} e^{-kx^2}\,dx = \sqrt{\frac{\pi}{k}}, \text{ for $k > 0$};\]
		\item (iii) \[ \int_{-\infty}^{\infty} e^{kx^2}\,dx = \sqrt{\frac{\pi}{-k}}, \text{ for $k < 0$}.\]
	\end{itemize}
\end{itemize}

\begin{proof}
Suppose that $Z \sim N(0, 1)$, then \[ f_{Z}(z) = \frac{1}{\sqrt{2\pi}} \cdot e^{\frac{-z^2}{2}}.\] 
\hspace*{2.7em} Next, computing M.G.F for $Z^2$: \[ M_{Z^2}(t) = E(e^{tz^2}) = \int_{-\infty}^{\infty} e^{tz^2} \cdot \frac{1}{\sqrt{2\pi}} \cdot e^{\frac{-z^2}{2}}\, dz.\]
\hspace*{2.7em} After rearranging: \[  M_{Z^2}(t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-(\frac{1}{2} - t)z^2}\, dz. \]
\hspace*{2.7em} Let $u^2 = (\frac{1}{2} - t)z^2, \text{ then } u = z\sqrt{\frac{1}{2}-t}, \text{ so that } du = \sqrt{\frac{1}{2}-t} \cdot dz, \text{ and } dz = \frac{1}{\sqrt{\frac{1}{2}-t}}du.$
\hspace*{2.7em} By substitution: \[ M_{Z^2}(t) = \frac{1}{\sqrt{2\pi}} \int_{u = -\infty}^{u = \infty} e^{-u^2} \cdot \frac{1}{\sqrt{\frac{1}{2}-t}}du\]
\hspace*{2.7em} Rearranging and using 2(i) from above: \[ M_{Z^2}(t) = \frac{1}{\sqrt{2\pi}} \cdot \frac{1}{\sqrt{\frac{1}{2}-t}} \int_{u = -\infty}^{u = \infty} e^{-u^2} = \frac{1}{\sqrt{2\pi}} \cdot \frac{1}{\sqrt{\frac{1}{2}-t}} \cdot \sqrt{\pi}.\]
\hspace*{2.7em} After all simplification, we get: \[ M_{Z^2}(t) = (1-2t)^{-\frac{1}{2}}, \text{ which is the M.G.F of $\chi_{n = 1}^{2}$, as required.}\]
\end{proof}

Now, we can do another proof by using Theorem $2.1$.

\begin{theorem}[$ \sum_{i = 1}^{n}Z_{i}^{2} \sim \chi_{n}^{2}$]
Suppose $Z_1, Z_2, ..., Z_n \overset{\text{i.i.d.}}{\sim} N(0,1)$, then the sum of $n$ independent $Z^2$ is going to be Chi-square distributed with $n$ degrees of freedom, as the following: \[ \sum_{i=1}^{n}Z_{i}^{2} \sim \chi_{n}^{2}.\]
\end{theorem}

We need Theorem $2.1$ to prove this, but it going to be easier. 

\begin{proof}
Assume that $Z_1, Z_2, ..., Z_n \overset{\text{i.i.d.}}{\sim} N(0,1)$, then we compute the M.G.F for the entire summation as well, and we let $\delta = \sum_{i = 1}^{n} Z_{i}^{2}$, which as the following shows: \[ M_{\delta}(t) = E[e^{t\delta}] = E[e^{t(Z_1^2 + \cdots + Z_n^2)}] = E[e^{tZ_1^2} \cdots e^{tZ_n^2}].\]
\hspace*{2.7em} Since $Z_1, Z_2, ..., Z_n$ are independent and identically distributed, so that: \[ M_{\delta}(t) = E[e^{tZ_1^2}] \cdots E[e^{tZ_n^2}].\]
\hspace*{2.7em} By Theorem $2.1$, we have: $Z^2 \sim \chi_{1}^{2}$, then: \[ M_{\delta}(t) = E[e^{t\delta}] = E[e^{tZ_1^2} \cdots e^{tZ_n^2}] = [(1-2t)^{-\frac{1}{2}}] \cdots [(1-2t)^{-\frac{1}{2}}].\]
\hspace*{2.7em} Since there are $n$ individuals of $Z^2$, hence: \[ M_{\delta}(t) = (1-2t)^{-\frac{n}{2}}, \text{ which is the M.G.F of $\chi_{n}^{2}$, as required}.\]
\end{proof}

Here is the last theorem for Chi-square and normal distribution, but we won't show you the proof due to its complexity. For people who are interested in that, please see STA260 lecture notes or power point slide to figure out.

\begin{theorem}[$ \frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^{2}$]
Let $n$ be sample size, $s^2$ be sample variance and $\sigma^2$ be population variance, then $\frac{(n-1)s^2}{\sigma^2}$ is Chi-square distributed with $n - 1$ degrees of freedom. As the following: \[ \frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^{2}.\]
\end{theorem}

\section{Student's t-Distribution and F-Distribution}

The t-distribution and F-distribution are essential tools in inferential statistics, particularly in the context of hypothesis testing and variance analysis. The t-distribution, which resembles the normal distribution but with heavier tails, is primarily used when estimating population means in situations where the sample size is small and the population standard deviation is unknown. On the other hand, the F-distribution is used to compare variances between two populations and plays a central role in analysis of variance (ANOVA) and regression analysis.\\

\textbf{Student's t-Distribution}

\begin{definition}[Student's t-Distribution]
Suppose $X$ is t-distributed with $n$ degrees of freedom, then the probability density function of $X$ is given by: \[ f_{X}(x) = \frac{\Gamma(\frac{n+1}{2})}{\sqrt{\pi n} \Gamma(\frac{n}{2})} (1+\frac{x^2}{n})^{\frac{-n+1}{2}}.\]
Alternatively, define a new variable $T$ is the following: \[ T = \frac{W}{\sqrt{\frac{V}{r}}}, \text{ for $W \sim N(0, 1)  \text{ and } V \sim \chi_{r}^{2}$.}\]
Or suppose $X_1, ..., X_n \overset{\text{i.i.d.}}{\sim} N(\mu, \text{ } \sigma^2)$, then $\bar(X) \sim N(\mu, \text{ } \frac{\sigma^2}{n})$. Thus, \[ T = \frac{ \bar{x} - \mu}{\frac{s}{(\sqrt{n}})}.\]
\end{definition}

Same as normal distribution, student's t-distribution is also symmetric. Also, as the degrees of freedom of t-distribution getting larger, the curve of student's t-distribution getting closer to standard normal distribution.\\

\textbf{F-Distribution}

\begin{definition}
We define a new variable $F$ as the following shows: \[ F = \frac{ (\frac{W_1}{v_1}) }{ (\frac{W_2}{v_2})} \sim F_{v_1, \text{ } v_2}; \text{ for $W_1 \sim \chi_{v_1}^{2}$ and $W_2 \sim \chi_{v_2}^{2}$; also both $W_1$ and $W_2$ are independent.}\]
Alternatively, we select two samples (with same population variance) with size $n$ and $m$, and also sample variance $s_x$ and $s_y$ respectively. Then, F-distribution is: \[ F = \frac{ [\frac{ (\frac{(n-1)}{\sigma^2}) s_x^2}{n-1}] }{ [\frac{ (\frac{(m-1)}{\sigma^2}) s_y^2}{m-1}] } \sim F_{n-1, \text{ } m-1 }.\]
\end{definition}

Both student's t-distribution and F-distribution are highly used in inferential statistics, until confidence interval, testing hypothesis and ANOVA analysis, these two distributions will come to play a lot. At this point, just guarantee that you know how to obtain those distribution from random given information is sufficient.


