%\pagenumbering{arabic}
%\setcounter{page}{1}
\setcounter{chapter}{14}
\chapter{Introduction to Simple Linear Regression}
%\index{Introduction}
%\label{sec.matrix}
%start relabeling as 2.1 etc
%\pagestyle{myheadings}  \markboth{\ref{sec.matrix}.
%\titleref{sec.matrix}}{}
%\setcounter{equation}{0}

In statistics, simple linear regression (SLR) is a linear regression model with a single explanatory variable. In other words, we use linear functions to illustrate the relationship of variables (ie. time and one's height). The goal of simple linear regression is to find the best-fitting straight line, known as the regression line, that predicts the dependent variable based on the independent variable. For example we are interested a people's height within 10 months. Then, we use coordinate system to draw each data point and use simple linear regression to find a function which perfectly describes the relationship between height and time.

\begin{figure}[h]
\centering
\begin{tikzpicture}[->, thick]
  % Draw x and y axes
  \draw[->] (-1,0) -- (7,0) node[right] {Time (month)};
  \draw[->] (0,-1) -- (0,5) node[above] {Height (m)};
  
  \filldraw[blue] (0.5, 0.75) circle (2pt);
  \filldraw[blue] (1.5, 1.75) circle (2pt);
  \filldraw[blue] (1, 1.25) circle (2pt);
  \filldraw[blue] (2, 2.1) circle (2pt);
  \filldraw[blue] (2.5, 2.4) circle (2pt);
  \filldraw[blue] (3, 2.8) circle (2pt);
  \filldraw[blue] (3.5, 3.1) circle (2pt);
  \filldraw[blue] (4, 3.7) circle (2pt);
  \filldraw[blue] (4.5, 3.95) circle (2pt);
  \filldraw[blue] (5, 4.1) circle (2pt);
  \draw[red, thick, domain=0:6] plot (\x, {0.8*\x + 0.35}) node[right] {$y = mx+ b$};
\end{tikzpicture}
\caption{An illustration of simple linear regression. The blue points are measures of height monthly, and the red line is our SLR model. In this case $m$ is the slope which tells you the rate of change, $b$ is the intercept which may have a special meaning depending on the case.}
\end{figure}

Now, you may wonder the accuracy of this model. In statistics, we do have parameters that approximate the slope and intercept of the function $y = mx + b$. The model we are going to use is: $\hat{y} = \hat{\beta_1}x + \hat{\beta_0} + \epsilon$. From this model, the slope and the intercept are $\hat{\beta_1}$, $\hat{\beta_{0}}$ respectively ($\hat{\beta_1}$ and $\hat{\beta_{0}}$ are unbiased estimators). Moreover, the $\epsilon$-term is called error term, which we will discuss it later.

\section{Measures of Linear Relationship}

Before we formally introduce simple linear regression, there are some measures of SLR that should be discussed.\\

\textbf{Covariance (Sample Covariance)}

In probability theory and statistics, covariance is a measure of the joint variability of two random variables. The covariance sign shows the direction of the linear relationship between two variables. If higher values of one variable tend to occur with higher values of the other (and lower with lower), the covariance is positive, meaning the variables move in the same direction. If higher values of one variable tend to occur with lower values of the other, the covariance is negative, meaning they move in opposite directions. The size (magnitude) of the covariance reflects how much the two variables vary together, based on the variances they share.

\begin{definition}[Covariance (sample covariance)]
The formula of sample covariance is given by: \[ S_{xy} = \frac{1}{n-1} \cdot \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) = \frac{\sum_{i = 1}^{n}x_i \cdot y_i}{n -1} - \frac{n\bar{x}\bar{y}}{n-1}.\]
These are the two ways to compute covariance. Both will give you the same answer.
\end{definition}

Basically, covariance indicates that how two variables move together.

\begin{itemize}
	\item If covariance of two random variables is greater than $0$ ($cov(x,y) > 0$), then the two random variables show the same trend. That is: if one random variable is increasing, then the other one is also increasing; while if one random variable is decreasing, then the other one is also decreasing.
	\item If covariance of two random variables is less than $0$ ($cov(x,y) < 0$), then the two random variables show the opposite trend. That is: if one random variable is increasing, then the other one is decreasing; while if one random variable is decreasing, then the other one is increasing.
	\item If covariance of two random variables is equal to $0$ ($cov(x,y) = 0$), then we say that there is no relationship (systematically linear) between the two random variables.
\end{itemize}

Note that covariance is not standardized, so it can be difficult to interpret directly.\\

\textbf{Coefficient of Correlation}

In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. It helps us understand whether and how changes in one variable are associated with changes in another. A positive correlation means that as one variable increases, the other tends to increase as well, while a negative correlation means that one variable tends to decrease as the other increases. The degree of correlation is usually expressed with a correlation coefficient, which ranges from $-1$ to $+1$.

\begin{definition}[Coefficient of correlation]
The coefficient of correlation is given by: \[ r_{xy} = \frac{S_{xy}}{S_x \cdot S_y}.\]
Now, $r_{xy} = $ sample correlation coefficient, $S_{xy} = $ sample covariance, $S_{x} = $ sample standard deviation of $x$, $S_{y} = $ sample standard deviation of $y$. Also, remember that the range of coefficient of correlation is between $-1$ and $+1$: $r_{xy} \in [-1, +1]$.
\end{definition}

The correlation r measures the strength and direction of the linear association between two quantitative variables $x$ and $y$. Although you calculate a correlation for any scatter plot, $r$ measures only straight-line relationships. In short, coefficient of correlation is a measure of the strength of the linear relationship between two random variables.

\begin{itemize}
	\item If $r_{xy} \approx +1$, then we say that the two random variables have a strong positive correlation.	(See figure 15.2)
\begin{figure}[h]
\centering
\begin{tikzpicture}[->, thick]
  \draw[->] (-1,0) -- (7,0) node[right] {$x$};
  \draw[->] (0,-1) -- (0,5) node[above] {$y$};
  \filldraw[blue] (0.5, 0.5) circle (2pt);
  \filldraw[blue] (1, 1) circle (2pt);
  \filldraw[blue] (1.5, 1.5) circle (2pt);
  \filldraw[blue] (2, 2) circle (2pt);
  \filldraw[blue] (2.5, 2.5) circle (2pt);
  \filldraw[blue] (3, 3) circle (2pt);
  \filldraw[blue] (3.5, 3.5) circle (2pt);
  \filldraw[blue] (4, 4) circle (2pt);
  \filldraw[blue] (4.5, 4.5) circle (2pt);
  \filldraw[blue] (5, 5) circle (2pt);
  \filldraw[red] (0.5,1) circle (2pt);
  \filldraw[red] (1,1.5) circle (2pt);
  \filldraw[red] (1.5,2) circle (2pt);
  \filldraw[red] (2,2.5) circle (2pt);
  \filldraw[red] (2.5,3) circle (2pt);
  \filldraw[red] (3,3.5) circle (2pt);
  \filldraw[red] (3.5,4) circle (2pt);
  \filldraw[red] (4,4.5) circle (2pt);
  \filldraw[red] (4.5,5) circle (2pt);
  \filldraw[red] (5,5.5) circle (2pt);
\end{tikzpicture}
\caption{An illustration of strong positive correlation ($r_{xy} \approx +1$).}
\end{figure}

	\item If $r_{xy} \approx -1$, then we say that the two random variables have a strong negative correlation.	 (See figure 15.3)
\begin{figure}[H]
\centering
\begin{tikzpicture}[->, thick]
  \draw[->] (-1,0) -- (7,0) node[right] {$x$};
  \draw[->] (0,-1) -- (0,5) node[above] {$y$};
  \filldraw[blue] (0.5,5) circle (2pt);
  \filldraw[blue] (1,4.5) circle (2pt);
  \filldraw[blue] (1.5,4) circle (2pt);
  \filldraw[blue] (2,3.5) circle (2pt);
  \filldraw[blue] (2.5,3) circle (2pt);
  \filldraw[blue] (3,2.5) circle (2pt);
  \filldraw[blue] (3.5,2) circle (2pt);
  \filldraw[blue] (4,1.5) circle (2pt);
  \filldraw[blue] (4.5,1) circle (2pt);
  \filldraw[blue] (5,0.5) circle (2pt);
  \filldraw[red] (0.5,4.5) circle (2pt);
  \filldraw[red] (1,4) circle (2pt);
  \filldraw[red] (1.5,3.5) circle (2pt);
  \filldraw[red] (2,3) circle (2pt);
  \filldraw[red] (2.5,2.5) circle (2pt);
  \filldraw[red] (3,2) circle (2pt);
  \filldraw[red] (3.5,1.5) circle (2pt);
  \filldraw[red] (4,1) circle (2pt);
  \filldraw[red] (4.5,0.5) circle (2pt);
  \filldraw[red] (5,0) circle (2pt);
\end{tikzpicture}
\caption{An illustration of strong negative correlation ($r_{xy} \approx -1$).}
\end{figure}

	\item If $r_{xy} \approx 0$, then we say that there is essentially no correlation between the two random variables. Note that if $r \approx 0$, then it suggests a linear relationship doesn't exist but other relationship may exist. (See figure 15.4)
\begin{figure}[h!]
\centering
\begin{tikzpicture}[->, thick]
  \draw[->] (-1,0) -- (7,0) node[right] {$x$};
  \draw[->] (0,-1) -- (0,5) node[above] {$y$};
  \draw[black, thick, domain=0:6] plot (\x, {3});
  \filldraw[blue] (0.5, 2.7) circle (2pt);
  \filldraw[red]  (1,   1.5) circle (2pt);
  \filldraw[blue] (1.5, 4.1) circle (2pt);
  \filldraw[red]  (2.3, 2.0) circle (2pt);
  \filldraw[blue] (3.1, 0.8) circle (2pt);
  \filldraw[red]  (4.0, 3.5) circle (2pt);
  \filldraw[blue] (4.5, 1.2) circle (2pt);
  \filldraw[red]  (5.2, 4.3) circle (2pt);
  \filldraw[blue] (6.0, 2.5) circle (2pt);
\end{tikzpicture}
\caption{An illustration of no correlation ($r_{xy} \approx 0$).}
\end{figure}
\end{itemize}

Note that correlation doesn't imply causation:

\begin{itemize}
	\item $cor(x,y) \approx +1$ doesn't necessarily imply on increase in $x$ causes increase in $y$.
	\item $cor(x,y) \approx -1$ doesn't necessarily imply on increase in $x$ causes decrease in $y$.
\end{itemize}

\textbf{Properties of Covariance and Correlation}

These two values are symmetric:

\begin{itemize}
	\item $cov(x,y) = cov(y,x)$;
	\item $cor(x,y) = cor(y,x)$.
\end{itemize}

\begin{example}
Five observations taken for two variables follow.
\begin{center}
\begin{figure}[H]
\centering
\begin{tabular}{ c c c }
$x_i$ & $y_i$\\
$4$ & $50$\\
$6$ & $50$\\
$11$ & $40$\\
$3$ & $60$\\
$16$ & $30$\\
\end{tabular}
\caption{Data of example 15.1}
\end{figure}
\end{center}
\vspace{-0.75cm}
(a) Compute the sample covariance.

(b) Compute and interpret the sample correlation coefficient.

\textbf{Solution:}

Step 1: Compute $\bar{x}$ and $\bar{y}$; $\bar{x} = 8$ and $\bar{y} = 46$ (check this by yourself).

Step 2: Find $s_x$ and $s_y$.
\vspace{-0.5cm}

$$s_x^2 = \frac{1}{5-1} \cdot \sum_{i = 1}^{5}(x_i - \bar{x})^2 = 29.5 \text{ and } s_y^2 = \frac{1}{5-1} \cdot \sum_{i=1}^{5}(y_i - \bar{y})^2 = 130$$
\hspace*{3.5em}Then: $s_x = 5.4313$ and $s_y = 11.4017$.

Step 3: Find $s_{xy}$ and $r$.
\vspace{-0.75cm}

$$\sum_{i=1}^{5}x_i \cdot y_i = 1600, \text{ then } s_{xy} = \frac{1600}{5-1} - \frac{5\cdot8\cdot46}{5-1} = -60 \text{ and } r_{xy} = \frac{s_{xy}}{s_x \cdot s_y} = 0.9688.$$

\textbf{R code}

Step 1: Entering data;

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
X=c(4,6,11,3,16); 

Y=c(50,50,40,60,30);
\end{verbatim}
\end{tcolorbox}

Step 2: Finding means;

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
mean(X);

mean(Y);
\end{verbatim}
\end{tcolorbox}

Step 3: Finding variances;

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
var(X);

var(Y);
\end{verbatim}
\end{tcolorbox}

Step 4: Finding standard deviations;

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
sd(X);

sd(Y):
\end{verbatim}
\end{tcolorbox}

Step 5: Finding covariance and correlation;

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
cov(X,Y);

cor(X,Y);
\end{verbatim}
\end{tcolorbox}
\end{example}

\section{Least Squares Method}

The method of least squares is a mathematical optimization technique used to find the best-fitting function by minimizing the sum of the squared differences between the observed data points and the values predicted by the model. It interested in a linear model of the form: $y  = \beta_0 + \beta_1 \cdot x_1 + \cdots + \beta_p \cdot x_p + \epsilon$, where $\epsilon \sim N(0, \sigma^2)$, $x_i$'s ($i = 1,..., p$) are independent predictors, $\beta_j$'s ($j = 0,..., p$) are coefficients, $y$ is dependent variable.\\

Using sample data, we get estimates of this model of the form: $\hat{y} = \hat{\beta_0} + \hat{\beta_1} \cdot x_1 + \cdots + \hat{\beta_p} \cdot x_p$. Now, $\hat{y}$ is predicated $y$, $x_i$'s ($i = 1,...,p$) are independent predicators, $\hat{\beta_j}$'s ($j = 0,...,p$) are estimated coefficients. Moreover, $\hat{\beta_0}$ is intercept; $\hat{\beta_1},...,\hat{\beta_p}$ are quantifiers how much $y$ changes with a unit increase in $x_i$'s.\\

In this course, we focus on the following model a bit more: $\hat{y} = b_0 + b_1\cdot x$, where $b_0$ is the y-intercept, and $b_1$ is the slope, and $\hat{y}$ is the value of $y$ determined by the line. The coefficients $b_0$ and $b_1$ are derived using Calculus so that we minimize the sum of squared deviations: $\sum_{i=1}^{n}(y_i - \hat{y_i})^2$. Then the least squares line coefficients are $b_1 = r \cdot \frac{s_y}{s_x}$ and $b_0 = \bar{y} - b_1\cdot \bar{x}$.\\

\textbf{Facts about Least Squares Method}

\begin{itemize}
	\item The distinction between explanatory and response variables is essential in Least Squares Method.
	\item The least-squares line (trendline) always passes through the point ($\bar{x}$, $\bar{y}$) on the graph of $y$ against $x$.
	\item The square of the correlation, $r^2$, is the fraction of the variation in the values of $y$ that is explained by the variation in $x$.
\end{itemize}

\begin{example}
A tool die maker operates out of a small shop making specialized tools. He is considering increasing the size of his business and needs to know more about his costs. One such cost is electricity, which he needs to operate his machines and lights. He keeps track of his daily electricity costs and the number of tools that he made that day. These data are listed next. Determine the fixed and variable electricity costs using the Least Squares Method.

\begin{center}
\begin{figure}[H]
\centering
\begin{tabular}{ c c c }
Day & Number of tools ($X$) & Electricity costs ($Y$)\\
$1$ & $7$ & $23.80$\\
$2$ & $3$ & $11.89$\\
$3$ & $2$ & $15.89$\\
$4$ & $5$ & $26.11$\\
$5$ & $8$ & $31.79$\\
$6$ & $11$ & $39.93$\\
$7$ & $5$ & $12.27$\\
$8$ & $15$ & $40.06$\\
$9$ & $3$ & $21.38$\\
$10$ & $6$ & $18.65$\\
\end{tabular}
\caption{Data of example 15.2}
\end{figure}
\end{center}
\vspace{-0.75cm}

Solution:

Step 1: Entering Data;

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
tools=c(7,3,2,5,8,11,5,15,3,6);

cost=c(23.80,11.89,15.98,26.11,31.79, 39.93,12.27,40.06,21.38,18.65);
\end{verbatim}
\end{tcolorbox}

Step 2: Finding Slope;

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
Sx=sd(tools); 

Sy=sd(cost); 

r=cor(tools,cost); 

b1=r*(Sy/Sx); 

b1;
 
## [1] 2.245882
\end{verbatim}
\end{tcolorbox}

Step 3: Finding $y$-intercept;

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
x.bar=mean(tools); 

y.bar=mean(cost);

b0=y.bar - b1*x.bar;

b0; 

## [1] 9.587765
\end{verbatim}
\end{tcolorbox}

We can also use R-code to draw a graph:

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
plot(tools,cost,pch=19);

abline(least.squares$coeff,col="red");

# pch=19 tells R to draw solid circles; 

# abline tells R to add trendline;
\end{verbatim}
\end{tcolorbox}

Interpretation: 

The slope measures the marginal rate of change in the dependent variable. In this example, the slope is $2.25$, which means that in this sample, for each one-unit increase in the number of tools, the marginal increase in the electricity cost is $\$ 2.25$ per tool.

The $y$-intercept is $9.57$; that is, the line strikes the $y$-axis at $9.57$. However, when $x = 0$, we are producing no tools and hence the estimated ﬁxed cost of electricity is $ \$9.57$ per day .

\end{example}

\section{Simple Linear Regression}

\textbf{Estimating Regression Model Parameters}

The regression line which we are going to use is: $E(Y) = \beta_0 + \beta_1 \cdot x$. This is fitted to the data points $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$ by finding the line that is closest to the data in some sense. There are many ways in which closeness can be defined, but the method most generally used is to consider the vertical deviations between the line and the data points: $y_i - (\beta_0 + \beta_1 \cdot x_i), 1 \leq i \leq n$.\\

The fitted line is chosen to be the line that minimizes the sum of the squares of these vertical deviations $$Q = \sum_{i=1}^{n} [y_i - (\beta_0 + \beta_1 \cdot x_i)]^2$$ and this is referred to as the least squares fit. (The quantity $Q$ is also called the \textbf{sum of squares for error}, SSE.)\\

The parameter estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ are therefore the values that minimize the quantity $Q$. They are found taking partial derivatives of $Q$ with respect to $\hat{\beta_0}$ and $\hat{\beta_1}$ and setting the resulting expressions equal to $0$.\\

Now, you know the method to get the regression model $E(Y) = \beta_0 + \beta_1 \cdot x$, the following lines are the derivation:

\begin{figure}[h]
\centering
\begin{tikzpicture}[thick]
  \draw[->] (-1,0) -- (7,0) node[right] {$x$};
  \draw[->] (0,-1) -- (0,5) node[above] {$y$};
  \filldraw[blue] (0.5, 1.0) circle (2pt);
  \filldraw[blue] (1.5, 1.4) circle (2pt);
  \filldraw[blue] (1, 1.5) circle (2pt);
  \filldraw[blue] (2, 2.4) circle (2pt);
  \filldraw[blue] (2.5, 2.1) circle (2pt);
  \filldraw[blue] (3, 2.7) circle (2pt);
  \filldraw[blue] (3.5, 3.3) circle (2pt);
  \filldraw[blue] (4, 3.4) circle (2pt);
  \filldraw[blue] (4.5, 4.2) circle (2pt);
  \filldraw[blue] (5, 3.7) circle (2pt);
  \draw[red, thick, domain=0:6] plot (\x, {0.8*\x + 0.35}) node[right] {$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \cdot x$};
  \foreach \x/\y in {
    0.5/1.0,
    1.5/1.4,
    1/1.5,
    2/2.4,
    2.5/2.1,
    3/2.7,
    3.5/3.3,
    4/3.4,
    4.5/4.2,
    5/3.7
  } {
    \draw[black] (\x,\y) -- (\x,{0.8*\x + 0.35});
  }
\end{tikzpicture}
\caption{An illustration of a simple linear regression model. The red line is the fitted regression line, and the full black vertical lines represent the residuals (SSE). The data points deviate from the line to show errors clearly. Note that the sum of residuals is necessarily $0$.}
\end{figure}

The following proof is the derive of $\hat{\beta_0}$ and $\hat{\beta_1}$ for simple linear regression model: $\hat{y} = \hat{\beta_0} + \hat{\beta_1} \cdot x + \epsilon$.

\begin{proof}
\begin{align*}
\intertext{Firstly, we examine sum of residual squared:}\\
\sum_{i=1}^{n}e_i^2 &= \sum_{i=1}^{n}(y_i - \hat{y_i})^2 = 0\\
&= \sum_{i=1}^{n}[y_i - (\hat{\beta_0} + \hat{\beta_1} \cdot x_i)]^2 = 0\\
&= \sum_{i=1}^{n}(y_i - \hat{\beta_0} - \hat{\beta_1} \cdot x_i)^2 = 0\\
\end{align*}
To find $\hat{\beta_0}$ and $\hat{\beta_1}$ which minimizes $\sum_{i=1}^{n}e_i^2$, we need partial derivative with respect to $\hat{\beta_0}$ and $\hat{\beta_1}$:
\begin{align*}
\frac{\partial}{\partial \hat{\beta_0}}\sum_{i=1}^{n}e_i^2 &= \frac{\partial}{\partial \hat{\beta_0}}\sum_{i=1}^{n}(y_i - \hat{\beta_0} - \hat{\beta_1} \cdot x_i)^2 = 0\\
&= \sum_{i=1}^{n}2(\hat{\beta_0} + \hat{\beta_1}x_i - y_i) = 0\\
&= \sum_{i=1}^{n}\hat{\beta_0} + \hat{\beta_1}\sum_{i=1}^{n}x_i - \sum_{i=1}^{n}y_i = 0 \text{; } (\text{consider:} \sum_{i=1}^{n}x_i = n\bar{x})\\
&= n\cdot\hat{\beta_0} + \hat{\beta_1}\cdot n \bar{x} - n\bar{y} = 0\\
\text{Hence: } \hat{\beta_0} &= \bar{y} - \hat{\beta_1}\cdot \bar{x}. \text{ (Equ 1)}\\
\frac{\partial}{\partial \hat{\beta_1}}\sum_{i=1}^{n}e_i^2 &= \frac{\partial}{\partial \hat{\beta_1}}\sum_{i=1}^{n}(y_i - \hat{\beta_0} - \hat{\beta_1} \cdot x_i)^2 = 0\\
&= \sum_{i=1}^{n}2(y_i - \hat{\beta_0} - \hat{\beta_1}x_i)(-x_i)=0\\
&= \sum_{i=1}^{n}2(\hat{\beta_0}x_i + \hat{\beta_1}x_i^2 - y_ix_i)=0\\
&= \hat{\beta_0}\sum_{i=1}^{n}x_i + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 - \sum_{i=1}^{n}x_iy_i = 0\\
&= \hat{\beta_1}\sum_{i=1}^{n}x_i^2 + \sum_{i=1}^{n}(\bar{y} - \hat{\beta_1}\bar{x})x_i - \sum_{i=1}^{n}x_iy_i = 0 \text{; } (\text{sub Equ 1 into this line})\\
&= \hat{\beta_1}\sum_{i=1}^{n}x_i^2 + \bar{y}\sum_{i=1}^{n}x_i - \hat{\beta_1}\bar{x}\sum_{i=1}^{n}x_i - \sum_{i=1}^{n}x_iy_i = 0\\
&= \hat{\beta_1}\sum_{i=1}^{n}x_i^2 + n\bar{x}\bar{y} - n\hat{\beta_1}(\bar{x})^2 - \sum_{i=1}^{n}x_iy_i = 0\\
&= \hat{\beta_1}[\sum_{i=1}^{n}x_i^2 - n(\bar{x})^2] + n\bar{x}\bar{y} - \sum_{i=1}^{n}x_iy_i = 0\\
\text{Hence: } \hat{\beta_1} &= \frac{\sum_{i=1}^{n}x_iy_i - n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_i^2 - n(\bar{x})^2} = \frac{S_{xy}}{S_{xx}}.
\end{align*}
\end{proof}

\textbf{Introduction to Simple Linear Regression}

At this point, we are going to provide the definition of simple linear regression model as the following:

\begin{definition}[Simple Linear Regression]
Let $x$ be independent variable and $y$ be dependent variable, then the model of simple linear regression is: $\hat{y} = \hat{\beta_0} + \hat{\beta_1} \cdot x + \epsilon$, where $\hat{\beta_0}$ represents the $y$-intercept, $\hat{\beta_1}$ represents the slope and $\epsilon$ is the error term that $\epsilon \sim N(0, \sigma^2)$. Moreover, $\hat{\beta_0} = \bar{y} - \hat{\beta_1}\cdot \bar{x}$ and $\hat{\beta_1} = \frac{S_{xy}}{S_{xx}}$, which is also equal to: \[ \frac{\sum_{i=1}^{n}x_iy_i - n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_i^2 - n(\bar{x})^2}.\]
\end{definition}

\begin{example}
Suppose an appliance store conducts a 5-month experiment to determine the effect of advertising on sales revenue. The results are shown in a table below. The relationship between sales revenue, $y$, and advertising expenditure, $x$, is hypothesized to follow a ﬁrst-order linear model, that is, $y = \beta_0 + \beta_1\cdot x + \epsilon$, where $y =$ dependent variable, $x =$ independent variable, $\beta_0$ y-intercept, $\beta_1 =$ slope of the line and $\epsilon =$ error variable.

\begin{center}
\begin{figure}[H]
\centering
\begin{tabular}{ c c c }
Month & Advertising Expenditure $x$ (\$ hundreds) & Sales Revenue $y$ (\$ thousands)\\
$1$ & $1$ & $1$\\
$2$ & $2$ & $1$\\
$3$ & $3$ & $2$\\
$4$ & $4$ & $2$\\
$5$ & $5$ & $4$\\
\end{tabular}
\caption{Data of example 15.3}
\end{figure}
\end{center}
\vspace{-0.75cm}

a) Obtain the least squares estimates of $\beta_0$ and $\beta_1$, and state the estimated regression function.

b) Plot the estimated regression function and the data.

\textbf{Solution: }

a) $\bar{x} = 3$, $\bar{y} = 2$, $S_{xx} = 10$, $S_{xy} = 7$

Then, the slope of the least squares line is $\hat{\beta_1} = \frac{S_{xy}}{S_{xx}} = 0.7$ and $\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x} = -0.1$. Thus, the least squares line is $\hat{y} = -0.1 + 0.7x$.

b) R-code

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, arc=2mm]
\begin{verbatim}
plot(x, y, main="Scatterplot: Simple Linear Regression",
xlab="x", ylab="y", pch=19,col="blue");

abline(coef(linear.reg), col="red",lty=2);
\end{verbatim}
\end{tcolorbox}
\end{example}

\section{SST, SSE and SSR}

Early in Section $15.3$, we introduced a value called the sum of residual squared (SSE). There are two more values that are important in simple linear regression, which are total sum of squares (SST) and sum of squares for regression (SSR). We will introduce all these three values with figures, so that you may have a better understanding of what they measure.\\

\textbf{SST (Total Sum of Squares)}

It is defined as the sum over all squared differences between the observations and their overall mean $\bar{y}$.

\begin{figure}[h]
\centering
\begin{tikzpicture}[thick]
  \draw[->] (-1,0) -- (7,0) node[right] {$x$};
  \draw[->] (0,-1) -- (0,5) node[above] {$y$};
  \filldraw[blue] (0.5, 1.3) circle (2pt);
  \filldraw[blue] (1.5, 2.1) circle (2pt);
  \filldraw[blue] (1, 1.6) circle (2pt);
  \filldraw[blue] (2, 2.7) circle (2pt);
  \filldraw[blue] (2.5, 2.2) circle (2pt);
  \filldraw[blue] (3, 2.5) circle (2pt);
  \filldraw[blue] (3.5, 3.7) circle (2pt);
  \filldraw[blue] (4, 3.9) circle (2pt);
  \filldraw[blue] (4.5, 3.4) circle (2pt);
  \filldraw[blue] (5, 4.2) circle (2pt);
  \draw[red, thick, domain=0:6] plot (\x, {0.8*\x + 0.35}) node[right] {$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \cdot x$};
  \draw[dashed] (-0.5,2.86) -- (6.5,2.86);
  \node at (6.8, 2.86) [right] {$\bar{y}$};
  \foreach \x/\y in {
    0.5/1.3,
    1.5/2.1,
    1/1.6,
    2/2.7,
    2.5/2.2,
    3/2.5,
    3.5/3.7,
    4/3.9,
    4.5/3.4,
    5/4.2
  } {
    \draw[black] (\x,\y) -- (\x,2.86);
  }
\end{tikzpicture}
\caption{An illustration of Total Sum of Squares (SST). The blue points represent height measurements over time, the dashed line is the mean height $\bar{y}$, and the solid black vertical lines represent the squared deviations from the mean (SST components).}
\end{figure}

\begin{definition}[SST (Toal sum of squares)]
For any simple linear regression model, SST (Toal sum of squares) measures the sum over all squared differences between the observations and their overall mean $\bar{y}$ is given by: \[ SST = \sum_{i=1}^{n}(y_i - \bar{y})^2.\]
\end{definition}

\textbf{SSE (Sum of Residual Squared)}

It is the sum of the squares of residuals (deviations predicted from actual empirical values of data). It is a measure of the discrepancy between the data and an estimation model, such as a linear regression. A small SSE indicates a tight fit of the model to the data.

\begin{figure}[h]
\centering
\begin{tikzpicture}[->, thick]
  \draw[->] (-1,0) -- (7,0) node[right] {$x$};
  \draw[->] (0,-1) -- (0,5) node[above] {$y$};
  \draw[red, thick, domain=0:6] plot (\x, {0.8*\x + 0.35}) node[right] {$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \cdot x$};
  \filldraw[blue] (0.5, 1.4) circle (2pt);
  \filldraw[blue] (1.5, 1.1) circle (2pt);
  \filldraw[blue] (1, 2.1) circle (2pt);
  \filldraw[blue] (2, 1.9) circle (2pt);
  \filldraw[blue] (2.5, 3.1) circle (2pt);
  \filldraw[blue] (3, 2.2) circle (2pt);
  \filldraw[blue] (3.5, 3.8) circle (2pt);
  \filldraw[blue] (4, 3.4) circle (2pt);
  \filldraw[blue] (4.5, 4.5) circle (2pt);
  \filldraw[blue] (5, 3.6) circle (2pt);
  \foreach \x/\y in {
    0.5/1.4, 
    1.5/1.1,
    1/2.1,
    2/1.9,
    2.5/3.1,
    3/2.2,
    3.5/3.8,
    4/3.4,
    4.5/4.5,
    5/3.6
  }{
    \draw[black] (\x,\y) -- (\x,{0.8*\x + 0.35});
  }
  \draw[dashed] (-0.5,2.7) -- (6.5,2.7);
  \node at (6.7,2.7) {$\bar{y}$};
\end{tikzpicture}
\caption{A simple linear regression illustration with residuals shown. Blue points are observed data, red line is the model, dashed line is the average of $y$, and black lines represent residuals $y_i - \hat{y_i}$.}
\end{figure}

\begin{definition}[SSE (Sum of residual squared)]
For any simple linear regression model, SSE (Sum of residual squared) measures the distance between observed data and estimated data, which is given by: \[ SSE = \sum_{i=1}^{n} (y_i - \hat{y_i})^2.\]
\end{definition}

Note that SSE (sum of residual squared) is an explained variation.\\

\textbf{SSR (Sum Square Regression)}

It measures the distance between estimated value (estimated dependent data) and the mean of dependent data ($\bar{y}$).

\begin{figure}[H]
\centering
\begin{tikzpicture}[->, thick]
  \draw[->] (-1,0) -- (7,0) node[right] {$x$};
  \draw[->] (0,-1) -- (0,5) node[above] {$y$};
  \filldraw[blue] (0.5, 1.2) circle (2pt);
  \filldraw[blue] (1.5, 2.1) circle (2pt);
  \filldraw[blue] (1, 1.6) circle (2pt);
  \filldraw[blue] (2, 2.5) circle (2pt);
  \filldraw[blue] (2.5, 2.9) circle (2pt);
  \filldraw[blue] (3, 3.3) circle (2pt);
  \filldraw[blue] (3.5, 3.7) circle (2pt);
  \filldraw[blue] (4, 4.2) circle (2pt);
  \filldraw[blue] (4.5, 4.4) circle (2pt);
  \filldraw[blue] (5, 4.7) circle (2pt);
  \draw[red, thick, domain=0:6] plot (\x, {0.8*\x + 0.35}) node[right] {$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \cdot x$};
  \draw[dashed] (0,3) -- (6,3) node[right] {$\bar{y}$};
  \foreach \x in {0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5} {
    \draw[black] (\x, {0.8*\x + 0.35}) -- (\x, 3);
  }
\end{tikzpicture}
\caption{An illustration of simple linear regression. The blue points represent monthly height measurements with added variability. The red line is the fitted simple linear regression model. The dashed line shows the mean of the dependent variable, $\bar{y}$, and the solid black lines illustrate the deviation of the model's predictions from this mean.}
\end{figure}

\begin{definition}[SSR (Sum square regression)]
For any simple linear regression, the distance between the mean of dependent value and estimated dependent value is called sum square regression (SSR), which is given by $$SSR = \sum_{i=1}^{n}(\hat{y_i} - \bar{y})^2.$$
\end{definition}

Note that sum square regression (SSR) is an explained variation.\\

\textbf{Summery}

Now, let's zoom in to see SST, SSE and SSR. Note that the relationship of these measures is that total deviation is equal to unexplained deviation (error) plus explained deviation (regression), that is: $(y_i - \bar{y}) = (y_i - \hat{y_i}) + (\hat{y_i} - \bar{y})$.\\

We square all three deviations for each one of our data points, and sum over all $n$ points. Here, cross terms drop out, and we are left with the following equation: \[ \sum_{i=1}^{n}(y_i - \bar{y})^2 = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 + \sum_{i=1}^{n}(\hat{y_i} - \bar{y})^2,\] SST = SSE + SSR.

Total sum of squares = Sum of squares for error + Sum of squares for regression.

\begin{figure}[H]
\centering
\begin{tikzpicture}[->, thick]
  \draw[->] (-1,0) -- (7,0) node[right] {$x$};
  \draw[->] (0,-1) -- (0,5) node[above] {$y$};
  \filldraw[blue] (2, 4) circle (2pt) node[above right] {Observed $y_i$};
  \draw[gray, thick, domain=0:6] plot (\x, {0.8*\x + 0.35}) node[right] {$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \cdot x$};
  \draw[dashed] (0,3) -- (6,3) node[right] {$\bar{y}$};
  \coordinate (xhat) at (2, 1.95);
  \filldraw[red] (xhat) circle (2pt) node[below right] {$\hat{y}_i$};
  \draw[pink, thick] (xhat) -- (2, 3) node[midway, right] {\footnotesize SSR};
  \draw[green, thick] (2, 4) -- (2, 3) node[midway, right] {\footnotesize SST};
  \draw[thick, orange, |-|] (1.9, 1.95) -- (1.9, 4) node[midway, left=4pt, yshift=6pt] {\footnotesize SSE};

\end{tikzpicture}
\caption{
Visual representation of regression variability. \textbf{SST} (green) is total variability from the mean, \textbf{SSR} (pink) is explained variability, and \textbf{SSE} (orange curly brace) is the residual (unexplained) variability between the observed value $y_i$ and the prediction $\hat{y}_i$.}
\end{figure}

\textbf{Coefficient of Determination ($r^2$)}

Moreover, we can use SST, SSE and SSR to calculate another value which is important in simple linear regression, that is coefficient of determination. It is proportion of variability in $y$ which is explained by $x$.

\begin{definition}[Coefficient of determination]
We define the coefficient of determination as the sum of squares due to the regression divided by the total sum of squares. $$r^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}.$$
The coefficient of determination can be interpreted as the proportion of the variation in $Y$ that is explained by the regression relationship of $Y$ with $X$ (or the proportion of the total corrected sum of squares explained by the regression). Note that: $0 \leq r^2 \leq 1.$

\end{definition}




