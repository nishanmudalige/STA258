# Introduction to Hypothesis Testing

## Test of Hypothesis for One Mean

::: definition
An inferential procedure to determine whether there is sufficient
evidence to suggest a condition for a population parameter using
statistics from a sample.
:::

[Attach a probability to the conclusion of a hypothesis
test.]{style="color: blue"}

#### Steps {#steps .unnumbered}

1.  Decide on a level of significance ($\alpha$)

2.  State the null hypothesis ($H_0$) and the alternative hypothesis
    ($H_a$) [($H_1$)]{style="color: blue"}

3.  Calculate the appropriate test statistic.

4.  Use the test statistic and a reference distribution to calculate a
    p-value.\
    [(Also refer back to $H_a$)]{style="color: blue"}

5.  Compare p-value to $\alpha$ to make a conclusion.

[Note:]{style="color: blue"}\
[The definition of a p-value can be confusing. We will define it
later.]{style="color: blue"}

### Step 1: Decide on a Level of Significance ($\alpha$) {#step-1-decide-on-a-level-of-significance-alpha .unnumbered}

-Threshold for decision making.

-Depends on tolerance for consequences of errors, sample size, nature of
the study, and variability.

-Common values: $0.10$, $0.01$, $0.05$ [(very common
default)]{style="color: blue"}

### Step 2: State the Null Hypothesis ($H_0$) and the Alternative Hypothesis ($H_a$) {#step-2-state-the-null-hypothesis-h_0-and-the-alternative-hypothesis-h_a .unnumbered}

**$\Theta$:** parameter of interest

**$\Theta_0$:** numerical value of the parameter of interest
hypothesized under the null hypothesis.

$$\begin{array}{lll}
H_0: \Theta = \Theta_0 & (\Theta \leq \Theta_0) & H_a: \Theta > \Theta_0 \quad \textcolor{blue}{\text{one-sided (one-tailed)}} \\
H_0: \Theta = \Theta_0 & (\Theta \geq \Theta_0) & H_a: \Theta < \Theta_0 \quad \textcolor{blue}{\text{one-sided (one-tailed)}} \\
H_0: \Theta = \Theta_0 &                         & H_a: \Theta \neq \Theta_0 \quad \textcolor{purple}{\text{two-sided (two-tailed)}}
\end{array}$$

**Null ($H_0$):** Represents the current belief ([status
quo]{style="color: blue"}) or the safe belief.

**Alternative ($H_a$):** Represents the research hypothesis (or what you
are asked to test)

### Step 3: Calculate an appropriate test statistic {#step-3-calculate-an-appropriate-test-statistic .unnumbered}

Depends on the hypothesis test conducted and the information available.

::: definition
$$\text{test statistic} = \frac{\text{(a statistic)} - \text{(hypothesized value of parameters under } H_0 \text{)}}{\text{standard error of statistic}}$$
:::

The test statistic follows a reference distribution ($Z$, $t$, $F$,
$\chi^2$).

### Step 4: Calculate the p-value {#step-4-calculate-the-p-value .unnumbered}

Use the test statistic, reference distribution, and refer back to $H_a$.

<figure>
<div class="minipage">
<embed src="section11/images/hypothesis_right_tail.pdf" />
</div>
<div class="minipage">
<embed src="section11/images/hypothesis_right_tail_wide.pdf" />
</div>
<figcaption>Right-tailed test: <em>p</em>-value is the area to the right
of the test statistic.</figcaption>
</figure>

<figure>
<div class="minipage">
<embed src="section11/images/hypothesis_left_tail.pdf" />
</div>
<div class="minipage">
<embed src="section11/images/hypothesis_left_tail_wide.pdf" />
</div>
<figcaption>Left-tailed test: <em>p</em>-value is the area to the left
of the test statistic.</figcaption>
</figure>

![Two-tailed test: p-value is the total area in both tails beyond $\pm$
test statistic.](section11/images/hypothesis_two_tail.pdf){width="52%"}

### Step 5: Compare *p*-value to level of significance $\alpha$ and make a conclusion {#step-5-compare-p-value-to-level-of-significance-alpha-and-make-a-conclusion .unnumbered}

- If *p*-value $< \alpha$:\
  Sufficient evidence against $H_0$. The hypothesis test rejects $H_0$
  in favor of $H_a$.

- If *p*-value $> \alpha$:\
  Insufficient evidence against $H_0$. Do not reject $H_0$ (fail to
  reject $H_0$).

**Note:** It is not good practice to give conclusions in the context of
stating we *accept $H_0$* or *accept $H_a$*.

:::::: example
Diet colas use artificial sweeteners to avoid sugar. These sweeteners
gradually lose their sweetness over time. Manufacturers therefore test
new colas for loss of sweetness before marketing them. Trained tasters
sip the cola along with drinks of standard sweetness and score the cola
on a "sweetness score" of 1 to 10. The cola is then stored for a month
at high temperature to imitate the effect of four months' storage at
room temperature. Each taster scores the cola again after storage. This
is a matched pairs experiment. Our data are the differences (score
before storage minus score after storage) in the tasters' scores. The
bigger these differences, the bigger the loss of sweetness.

Suppose we know that for any cola, the sweetness loss scores vary from
taster to taster according to a Normal distribution with standard
deviation $\sigma = 1$. The mean $\mu$ for all tasters measures loss of
sweetness.

The following are the sweetness losses for a new cola as measured by 10
trained tasters:

2.0, 0.4, 0.7, 2.0, -0.4, 2.2, -1.3, 1.2, 1.1, 2.3

Are these data good evidence that the cola lost sweetness in storage?\
**Solution**\
$\mu$ = mean sweetness loss for the population of **all** tasters.\
**Step 1:** State hypotheses. $$\begin{aligned}
H_0 &: \mu = 0 \\
H_a &: \mu > 0
\end{aligned}$$ **Step 2:** Test statistic:
$z_\star = \dfrac{\bar{x} - \mu_0}{\sigma / \sqrt{n}} = \dfrac{1.02 - 0}{1 / \sqrt{10}} = 3.23$\
**Step 3:** P-value. $P(Z > z_\star) = P(Z > 3.23) = 0.0006$\
**Step 4:** Conclusion. We would rarely observe a mean as large as 1.02
if $H_0$ were true. The small p-value provides strong evidence against
$H_0$, supporting $H_a: \mu > 0$. That is, the mean sweetness loss is
likely positive.

**R code (Simulation)**

::: tcolorbox
    # n = sample size;
    n<-10;
    mu.zero<-0;
    sigma<-1;
    sigma.xbar<-sigma/sqrt(n);

    # x bar = sample mean with 10 obs;
    x.bar<-rnorm(1,mean=mu.zero,sd=sigma.xbar);
    x.bar;

    ## [1] 0.3265859

    # z.star = test statistic;
    z.star<-(x.bar-mu.zero)/sigma.xbar;
    z.star;

    ## [1] 1.032755
:::

**R code (10,000 Simulations)**

::: tcolorbox
    n <- 10;
    mu.zero <- 0;
    sigma <- 1;
    sigma.xbar <- sigma / sqrt(n);
    # x bar = sample mean with 10 obs;
    # m = number of simulations;
    m <- 10000;
    x.bar <- rnorm(m, mean = mu.zero, sd = sigma.xbar);

    # z.star = test statistic;
    z.star <- (x.bar - mu.zero) / sigma.xbar;
    hist(z.star, xlab = "differences", col = "blue");
:::

**Histogram from simulation (see Section 7 for R code format)**

![Histogram of $z_\star$ values from 10,000 simulations under
$H_0$.](Section11/images/sweetening_10000_hist.pdf){width="60%"}

**R code (Empirical p-value)**

::: tcolorbox
    ## P-value

    p_value <- length(z.star[z.star > 3.23]) / m;

    p_value

    ## [1] 8e-04
:::
::::::

::: tcolorbox
**When $\sigma$ is known:**

  ------------------------------- ----------------------- ------------------------- --
  $\bullet\ H_0\!: \mu = \mu_0$   (or $\mu \leq \mu_0$)   $H_a\!: \mu > \mu_0$      
  $\bullet\ H_0\!: \mu = \mu_0$   (or $\mu \geq \mu_0$)   $H_a\!: \mu < \mu_0$      
  $\bullet\ H_0\!: \mu = \mu_0$                           $H_a\!: \mu \neq \mu_0$   
  ------------------------------- ----------------------- ------------------------- --

  --------------------- --------------------------------------------------
  **Test statistic:**   $z^* = \dfrac{\bar{X} - \mu_0}{\sigma/\sqrt{n}}$
  --------------------- --------------------------------------------------

**Reference distribution:** Standard normal ($Z$)
:::

::::: example
Deer are a common sight on the UTM campus. Suppose an ecologist is
interested in the average mass of adult white-tailed does (female deer)
around the Mississauga campus to determine whether they are healthy for
the upcoming winter. The ecologist captures a sample of 36 adult females
around the UTM and measures the average mass of this sample to be 42.53
kg.

From previous studies conducted in the area, the average mass of healthy
does was reported to be 45 kg. Conduct a hypothesis test at the 5%
significance level to determine whether the mass of does around UTM has
decreased. Assume the standard deviation is known to be 5.25 kg.

**1. Level of significance.** $\alpha = 0.05$

**2. State the null and alternative hypotheses.**
$$H_0: \mu = 45 \qquad H_a: \mu < 45$$

**3. Calculate appropriate test statistic.**

Given: $$n = 36, \quad \bar{x} = 42.53, \quad \sigma = 5.25$$

Since $\sigma$ is known, the test statistic is:
$$z^* = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}} = \frac{42.53 - 45}{5.25/\sqrt{36}} = -2.82$$

Reference distribution: standard normal

**4. Calculate p-value**

$$\text{p-value} = P(Z < -2.82) \approx 0.0024$$

![Left-tailed p-value for the test statistic
$z^* = -2.82$](section11/images/deer_pvalue.pdf){width="60%"}

**5. Compare p-value with level of significance $\alpha$ and make a
conclusion:**

$$0.0024 < 0.05 \Rightarrow \text{p-value} < \alpha$$

There is sufficient evidence at the 5% level of significance to reject
the null that does this winter weigh the same as in the past and to
conclude the alternative that does this winter weigh less than 45 kg.

**R code:**

::: tcolorbox
    # Find test stat
    z_test_stat = (42.53 - 45) / (5.25 / sqrt(36))
    z_test_stat
    [1] -2.822857

    # Find the p-value
    # Since the alternative is Ha : mu < 45
    p-value = pnorm(z_test_stat)
    [1] 0.00237989
:::

*Note:* The `pnorm()` function in R, by default, returns the cumulative
probability (area) to the left of the given value.\

**R code: Using BSDA package**

::: tcolorbox
    # Using the BSDA library. install BSDA if it is not already installed.
    # install.packages("BSDA")
    > library(BSDA)
    > # Conduct the z-test with the zsum.test function
    > zsum.test(mean.x = 42.53, sigma.x = 5.24, n.x = 36, mu = 45, alternative = "less")

            One-sample z-Test

    data:  Summarized x
    z = -2.8282, p-value = 0.00234
    alternative hypothesis: true mean is less than 45
    95 percent confidence interval:
     NA 43.96651
    sample estimates:
    mean of x 
        42.53 
:::

**Interpretation:**

There is sufficient evidence at the 5% level of significance to reject
the null hypothesis. We conclude that the average mass of does this
winter is significantly less than 45 kg.
:::::

::: example
The National Center for Health Statistics reports that the systolic
blood pressure for males 35 to 44 years of age has mean 128 and standard
deviation 15.

The medical director of a large company looks at the medical records of
72 executives in this age group and finds that the mean systolic blood
pressure in this sample is $\bar{x} = 126.07$. Is this evidence that the
company's executives have a different mean blood pressure from the
general population?

Suppose we know that executives' blood pressures follow a Normal
distribution with standard deviation $\sigma = 15$.

**Solution:** Let $\mu$ be the mean systolic blood pressure of the
executive population.

1.  **State hypotheses:**\
    $$\begin{aligned}
    H_0 &: \mu = 128 \\
    H_a &: \mu \ne 128
    \end{aligned}$$

2.  **Test statistic:**\
    $$z_\ast = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}} = \frac{126.07 - 128}{15 / \sqrt{72}} = -1.09$$

3.  **P-value:**\
    $$2P(Z > |z_\ast|) = 2P(Z > 1.09) = 2(1 - 0.8621) = 0.2758$$

4.  **Conclusion:**\
    More than 27% of the time, a simple random sample of size 72 from
    the general male population would have a mean blood pressure at
    least as far from 128 as that of the executive sample. The observed
    $\bar{x} = 126.07$ is therefore not good evidence that executives
    differ from other men.
:::

::: tcolorbox
There are four steps in carrying out a significance test:

1.  State the hypotheses.

2.  Calculate the test statistic.

3.  Find the P-value.

4.  State your conclusion in the context of your specific setting.

Once you have stated your hypotheses and identified the proper test, you
or your computer can do Steps 2 and 3 by following a recipe.
:::

::: tcolorbox
Here is the recipe for the test we have used in our examples.\
Draw a simple random sample of size $n$ from a Normal population that
has unknown mean $\mu$ and known standard deviation $\sigma$. To test
the null hypothesis that $\mu$ has a specified value,
$H_0 : \mu = \mu_0$, calculate the **one-sample $z$ statistic**:
$$z_\ast = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}}$$

In terms of a variable $Z$ having the standard Normal distribution, the
P-value for a test of $H_0$ against:

- $H_a: \mu > \mu_0$ is $P(Z > z_\ast)$

- $H_a: \mu < \mu_0$ is $P(Z < z_\ast)$

- $H_a: \mu \ne \mu_0$ is $2P(Z > |z_\ast|)$
:::

::: example
Consider the following hypothesis test:

$$\begin{aligned}
H_0 &: \mu = 20 \\
H_a &: \mu < 20
\end{aligned}$$

A sample of 50 provided a sample mean of 19.4. The population standard
deviation is 2.

1.  Compute the value of the test statistic.

2.  What is the p-value?

3.  Using $\alpha = 0.05$, what is your conclusion?

**Solution:**

1.  **Test statistic:**
    $$z_\ast = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}} = \frac{19.4 - 20}{2 / \sqrt{50}} = -2.1213$$

2.  **P-value:** $$P(Z < z_\ast) = P(Z < -2.1213) = 0.0169$$

3.  **Conclusion:**\
    Since the P-value $= 0.0169 < \alpha = 0.05$, we reject
    $H_0 : \mu = 20$. We conclude that $\mu < 20$.
:::

::: example
Consider the following hypothesis test:

$$\begin{aligned}
H_0 &: \mu = 25 \\
H_a &: \mu > 25
\end{aligned}$$

A sample of 40 provided a sample mean of 26.4. The population standard
deviation is 6.

1.  Compute the value of the test statistic.

2.  What is the p-value?

3.  Using $\alpha = 0.01$, what is your conclusion?

**Solution:**

1.  **Test statistic:**
    $$z_\ast = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}} = \frac{26.4 - 25}{6 / \sqrt{40}} = 1.4757$$

2.  **P-value:** $$P(Z > z_\ast) = P(Z > 1.4757) = 0.0700$$

3.  **Conclusion:**\
    Since P-value $= 0.0700 > \alpha = 0.01$, we **cannot reject**
    $H_0 : \mu = 25$.\
    We conclude that we don't have enough evidence to claim that
    $\mu > 25$.
:::

::: example
Consider the following hypothesis test:

$$\begin{aligned}
H_0 &: \mu = 15 \\ 
H_a &: \mu \ne 15
\end{aligned}$$

A sample of 50 provided a sample mean of 14.15. The population standard
deviation is 3.

1.  Compute the value of the test statistic.

2.  What is the p-value?

3.  Using $\alpha = 0.05$, what is your conclusion?

**Solution:**

1.  **Test statistic:**
    $$z_\ast = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}} = \frac{14.15 - 15}{3 / \sqrt{50}} = -2.0034$$

2.  **P-value:**
    $$2P(Z > |z_\ast|) = 2P(Z > |{-2.0034}|) = 2P(Z > 2.0034) = 0.0451$$

3.  **Conclusion:**\
    Since P-value $= 0.0451 < \alpha = 0.05$, we reject
    $H_0 : \mu = 15$.\
    We conclude that $\mu \ne 15$.

**Confidence Interval Interpretation:**

The 95% confidence interval for $\mu$ is:
$$\bar{x} \pm z_\ast \left( \frac{\sigma}{\sqrt{n}} \right)$$
$$14.15 \pm 1.96 \left( \frac{3}{\sqrt{50}} \right)
= (13.3184,\ 14.9815)$$

Since the hypothesized value $\mu_0 = 15$ falls **outside** this
interval, we again reject $H_0 : \mu = 15$.
:::

::: nt
*A level $\alpha$ two-sided significance test rejects a hypothesis
$H_0 : \mu = \mu_0$ exactly when the value $\mu_0$ falls outside a level
$1 - \alpha$ confidence interval for $\mu$.*
:::

::: tcolorbox
**When $\sigma$ is NOT known:**

  ----------------------------------- ----------------------- --------------------------- --
  $\bullet$ H$_0\!$ : $\mu = \mu_0$   (or $\mu \geq \mu_0$)   $H_a\!$ : $\mu < \mu_0$     
  $\bullet$ H$_0\!$ : $\mu = \mu_0$   (or $\mu \leq \mu_0$)   $H_a\!$ : $\mu > \mu_0$     
  $\bullet$ H$_0\!$ : $\mu = \mu_0$                           $H_a\!$ : $\mu \ne \mu_0$   
  ----------------------------------- ----------------------- --------------------------- --

  ----------------------------------------------------------------------
  **Test statistic:** $t^\ast = \dfrac{\bar{X} - \mu_0}{s / \sqrt{n}}$
  ----------------------------------------------------------------------

**Reference distribution:** $t$ distribution with $n - 1$ degrees of
freedom
:::

::: example
Researchers studied the physiological effects of laughter. They measured
heart rates (in beats per minute) of **$n = 25$** subjects (ages 18--34)
while they laughed. They obtained:
$$\bar{x} = 73.5, \quad s = 6, \quad \alpha = 0.05$$ It is well known
that the resting heart rate is 71 bpm. Is there evidence that the mean
heart rate during laughter exceeds 71 bpm?

**Step 1: State the hypotheses.** $$\begin{aligned}
H_0 &: \mu = 71 \\
H_a &: \mu > 71
\end{aligned}$$

**Step 2: Check assumptions.**

- The sample is an independent random sample of individuals aged 18--34.

- The population of heart rates during laughter is normally distributed.

**Step 3: Compute the test statistic.**

Since $\sigma$ is unknown, we use the $t$ statistic:
$$t^\ast = \frac{\bar{x} - \mu_0}{s / \sqrt{n}} = \frac{73.5 - 71}{6 / \sqrt{25}} = 2.083$$

Reference distribution: $t$ distribution with $n - 1 = 25 - 1 = 24$
degrees of freedom.

**Step 4: Determine the p-value.**

Using the $t$ distribution with 24 df: $$0.01 < \text{p-value} < 0.025$$

**Step 5: Make a conclusion.**

Since $\text{p-value} < \alpha = 0.05$, we reject $H_0$.

*There is sufficient evidence at the 5% level of significance to reject
the null that the mean is 71 bpm in favor of the alternative that the
mean is greater than 71 bpm for people who are laughing.*
:::

::::: example
A researcher is asked to test the hypothesis that the average price of a
2-star (CAA rating) motel room has decreased since last year. Last year,
a study showed that the prices were Normally distributed with a mean of
\$89.50.

A random sample of twelve 2-star motels produced the following room
prices:

$$\text{\$85.00, 92.50, 87.50, 89.90, 90.00, 82.50, 87.50, 90.00, 85.00, 89.00, 91.50, 87.50}$$

At the 5% level of significance, can we conclude that the mean price has
decreased?

**Solution:**

Let $\mu$ be the true average price of a 2-star motel room.

1.  **State hypotheses.** $$\begin{aligned}
    H_0 &: \mu = 89.5 \\
    H_a &: \mu < 89.5
    \end{aligned}$$

2.  **Compute test statistic.**
    $$t^\ast = \frac{\bar{x} - \mu_0}{s / \sqrt{n}} = \frac{88.1583 - 89.5}{2.9203 / \sqrt{12}} = -1.5915$$

3.  **Find the P-value.**\
    With $df = 11$, the P-value (from the $t$-distribution table) is
    between 0.05 and 0.10.

4.  **Conclusion.**\
    Since P-value $> 0.05$, we **fail to reject** $H_0$.\
    There is not sufficient evidence to conclude that the average price
    of 2-star motels has decreased this year.

**R code (One Sample t-test)**

::: tcolorbox
    # Step 1. Entering data;
    prices=c(85.00, 92.50, 87.50, 89.90, 90.00, 82.50,
             87.50, 90.00, 85.00, 89.00, 91.50, 87.50);

    # Step 2. Hypothesis test;
    t.test(prices, alternative="less", mu=89.5);
:::

**R output**

::: tcolorbox
    ## 
    ##  One Sample t-test
    ## 
    ## data:  prices
    ## t = -1.5915, df = 11, p-value = 0.0699
    ## alternative hypothesis: true mean is less than 89.5
    ## 95 percent confidence interval:
    ##  -Inf 89.67229
    ## sample estimates:
    ## mean of x 
    ##  88.15833 
:::
:::::

::: tcolorbox
Draw an SRS of size $n$ from a large population having unknown mean
$\mu$.

To *test the hypothesis* $H_0 : \mu = \mu_0$, compute the *one-sample
$t$ statistic* $$t^\ast = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}$$

In terms of a variable $T$ having the $t_{n - 1}$ distribution, the
P-value for a test of $H_0$ against

$$\begin{aligned}
H_a : \mu > \mu_0 &\quad \text{is} \quad P(T \ge t^\ast) \\
H_a : \mu < \mu_0 &\quad \text{is} \quad P(T \le t^\ast) \\
H_a : \mu \ne \mu_0 &\quad \text{is} \quad 2P(T \ge |t^\ast|)
\end{aligned}$$

These P-values are exact if the population distribution is Normal and
are approximately correct for large $n$ in other cases.
:::

:::: example
We are conducting a two-sided one-sample $t$-test for the hypotheses:
$$\begin{aligned}
H_0 &: \mu = 64 \\
H_a &: \mu \ne 64
\end{aligned}$$

based on a sample of $n = 15$ observations, with test statistic
$t^* = 2.12$.

**a) Degrees of freedom:**

$$df = n - 1 = 15 - 1 = 14$$

**b) Critical values and P-value bounds:**

From the $t$-distribution table for $df = 14$:

- $t = 1.761$ corresponds to a two-tailed probability of 0.10

- $t = 2.145$ corresponds to a two-tailed probability of 0.05

Since $t^* = 2.12$ falls between these values, the two-sided P-value
satisfies: $$0.05 < \text{P-value} < 0.10$$

**c) Significance:**

- At the 10% level: **Yes**, since P-value $< 0.10$

- At the 5% level: **No**, since P-value $> 0.05$

**d) Exact two-sided P-value using R:**

::: tcolorbox
    # Compute exact two-sided P-value for t* = 2.12 with df = 14
    2 * (1 - pt(2.12, df = 14))

    ## [1] 0.05235683
:::

Thus, the exact two-sided P-value is approximately **0.0524**,
confirming the bracketing result.
::::

## Test of Hypothesis for One Proportion

::: tcolorbox
**When sample size is large enough (np, n(1-p) $\ge$ 10):**

  ------------------------------- ------------------- ----------------------- --
  $\bullet$ $H_0\!$ : $p = p_0$   (or $p \geq p_0$)   $H_a\!$ : $p < p_0$     
  $\bullet$ $H_0\!$ : $p = p_0$   (or $p \leq p_0$)   $H_a\!$ : $p > p_0$     
  $\bullet$ $H_0\!$ : $p = p_0$                       $H_a\!$ : $p \ne p_0$   
  ------------------------------- ------------------- ----------------------- --

  -----------------------------------------------------------------------------------
  **Test statistic:** $z^* = \dfrac{\hat{p} - p_0}{\sqrt{\dfrac{p_0(1 - p_0)}{n}}}$
  -----------------------------------------------------------------------------------

**Reference distribution:** Standard normal ($\mathcal{Z}$)
:::

::: example
A YouTuber goes to her nearest Tim Hortons and buys 100 empty cups.
After rolling up the rims, she ends up with 12 winning cups out of the
100 she bought, all of them were food prizes.

If the probability of winning a food prize is supposed to be
$\frac{1}{6}$, does she have evidence to claim that the probability of
winning a food prize is less than $\frac{1}{6}$?
:::

::: definition
The **sample space** $\mathbf{S}$ of a random phenomenon is the set of
all possible outcomes.

An **event** is an outcome or a set of outcomes of a random phenomenon.
That is, an event is a subset of the sample space.

A **probability model** is a mathematical description of a random
phenomenon consisting of two parts: a sample space $S$ and a way of
assigning probabilities to events.
:::

Rolling a fair die (random phenomenon). There are 6 possible outcomes
when we roll a die.\
The sample space for rolling a die and counting the pips is

$$S = \{1,\, 2,\, 3,\, 4,\, 5,\, 6\}$$

"Roll a 6" is an event that contains one of these 6 outcomes.

::: definition
A random variable $X$ has a **discrete uniform distribution** if each of
the $n$ values in its range, say, $x_1, x_2, \ldots, x_n$, has equal
probability. Then, $$f(x_i) = \frac{1}{n}$$
:::

**R code:**

::: tcolorbox
    # Define a die with values 1 through 6
    die <- c(1, 2, 3, 4, 5, 6)

    # Roll the die once
    sample(die, 1, replace = TRUE)
    ## [1] 2

    # Roll the die six times
    sample(die, 6, replace = TRUE)
    ## [1] 1 3 2 6 3 1
:::

![Plot of frequencies from 60 simulations of a fair six-sided
die.](section11/images/simulation_60.pdf){width="60%"}

::: definition
A **random variable** is a variable whose value is a numerical outcome
of a random phenomenon.

The **probability distribution** of a random variable $X$ tells us what
values $X$ can take and how to assign probabilities to those values.
:::

::: nt
The Binomial setting

- There are a fixed number $n$ of observations.

- The $n$ observations are all **independent**. That is, knowing the
  result of one observation tells you nothing about the other
  observations.

- Each observation falls into one of just two categories, which for
  convenience we call "success" and "failure".

- The probability of a success, call it $p$, is the same for each
  observation.
:::

A random variable $Y$ is said to have a **binomial distribution** based
on $n$ trials with success probability $p$ if and only if
$$p(y) = \frac{n!}{y!(n - y)!} \, p^y (1 - p)^{n - y}, \quad y = 0, 1, 2, \ldots, n \quad \text{and} \quad 0 \leq p \leq 1.$$

::: example
Think of rolling a die $n$ times as an example of the binomial setting.
Each roll gives either a six or a number different from six. Knowing the
outcome of one roll doesn't tell us anything about other rolls, so the
$n$ rolls are independent.

If we call six a success, then $p$ is the probability of a six and
remains the same as long as we roll the same die. The number of sixes we
count is a random variable $X$. The distribution of $X$ is called a
**binomial distribution**.
:::

**R code (Binomial Simulations and PMF)**

::: tcolorbox
    ## Simulation: Binomial with n = 10 and p = 1/6.
    rbinom(1, size = 10, prob = 1/6);
    ## [1] 3

    rbinom(1, size = 10, prob = 1/6);
    ## [1] 1

    rbinom(1, size = 10, prob = 1/6);
    ## [1] 0

    ## Pmf: Binomial with n = 10 and p = 1/6.
    x <- seq(0, 10, by = 1);
    y <- dbinom(x, 10, 1/6);
    plot(x, y, type = "p", col = "blue", pch = 19);
:::

![PMF when $n=10$ and
$p=1/6$](section11/images/binomial_pmf.pdf){width="65%"}

![PMF when $n=100$ and
$p=1/6$](section11/images/binomial_pmf_100.pdf){width="65%"}

**R code (PMF values for selected $x$ values)**

::: tcolorbox
    dbinom(c(15, 16, 17, 18), size = 100, prob = 1/6);
    ## [1] 0.10023663 0.10650142 0.10524847 0.09706247
:::

![Simulation: 2000 YouTuber, $n = 100$, and
$p = 1/6$](section11/images/binomial_simulation_histogram.pdf){width="65%"}

**R code (A few values from our simulation)**

::: tcolorbox
    ## vec.prop
    ##  6  7  8  9 10 11 12 
    ##  7  3  8 24 46 72 106 
    ## [1] 266
    ## [1] 0.133
:::

It turns out that our P-value for this simulation is:\
0.133

![Simulation vs Theoretical
pmf](section11/images/sim_vs_theoretical_pmf.pdf){width="65%"}

::: tcolorbox
Draw an SRS of size $n$ from a large population that contains proportion
$p$ of "successes". Let $\hat{p}$ be the **sample proportion** of
successes,

$$\hat{p} = \frac{\text{number of successes in the sample}}{n}$$

Then:

- The **mean** of the sampling distribution of $\hat{p}$ is $p$.

- The **standard deviation** of the sampling distribution is
  $$\sqrt{\frac{p(1 - p)}{n}}.$$

<!-- -->

- As the sample size increases, the sampling distribution of $\hat{p}$
  becomes **approximately Normal**. That is, for large $n$, $\hat{p}$
  has approximately the $$N\left(p, \sqrt{\frac{p(1 - p)}{n}}\right)$$
  distribution.
:::

![Binomial with Normal
Approximation](section11/images/binomial_normal_approximation.pdf){width="65%"}

::: tcolorbox
To *test the hypothesis* $H_0 : p = p_0$, compute the $z_\ast$
statistic:
$$z_\ast = \frac{\hat{p} - p_0}{\sqrt{\dfrac{p_0(1 - p_0)}{n}}}$$

In terms of a variable $Z$ having the standard Normal distribution, the
approximate P-value for a test of $H_0$ against:

$$\begin{aligned}
H_a &: p > p_0 \quad \text{is} \quad P(Z > z_\ast) \\
H_a &: p < p_0 \quad \text{is} \quad P(Z < z_\ast) \\
H_a &: p \ne p_0 \quad \text{is} \quad 2P(Z > |z_\ast|) \\
\end{aligned}$$
:::

## Introduction to Hypothesis Testing (Significance Test) {#introduction-to-hypothesis-testing-significance-test .unnumbered}

Consider the following problem: In 1980s, it was generally believed that
congenital abnormalities affect 5% of the nation's children. Some people
believe that the increase in the number of chemicals in the environment
in recent years has led to an increase in the incidence of
abnormalities. A recent study examined 384 children and found that 46 of
them showed signs of abnormality. Is this strong evidence that the risk
has increased?

- The above statement serves as a hypothesis, moreover it is a Research
  Hypothesis.

A hypothesis is:

- a statement about a population.

- a prediction that a parameter describing some characteristics of a
  variable (e.g., true proportion, $p$) takes a particular numerical
  value or falls in a certain range of values.

For conducting a Significance Test:

- Researchers (you) use data to summarize the evidence about a
  hypothesis.

- With data, you can compare the point estimates of parameters to the
  values predicted by the hypothesis.

**Important Ideas about Hypothesis Testing**

- All the hypothesis tests boil down to the same question: "Is an
  observed difference or pattern too large to be attributed to chance?"

- We measure "how large" by putting our sample results in the context of
  a sampling distribution model (e.g., Normal model, $t$ distribution).

To plan a statistical hypothesis test, specify the model you will use to
test the null hypothesis and the parameter of interest.

- All models require assumptions, so you will need to state them and
  check any corresponding conditions.

- For example, if the conditions are satisfied, we can model the
  sampling distribution of the proportion with a Normal model.
  Otherwise, we cannot proceed with the test (we need to stop and
  reconsider).

## Steps in conducting Hypothesis Testing {#steps-in-conducting-hypothesis-testing .unnumbered}

1.  State the null and the alternative hypothesis.

2.  Check the necessary assumptions.

3.  Identify the test-statistic. Find the value of the test-statistic.

4.  Find the p-value of the test-statistic.

5.  State (if any) a conclusion.

:::: example
**Example of Hypothesis Testing for a Proportion**

In 1980s, it was generally believed that congenital abnormalities affect
5% of the nation's children. Some people believe that the increase in
the number of chemicals in the environment in recent years has led to an
increase in the incidence of abnormalities. A recent study examined 384
children and found that 46 of them showed signs of abnormality. Is this
strong evidence that the risk has increased?

**Step 1. Set up the null and alternative hypothesis:**

- The null hypothesis is the current belief: $H_0 : p = p_0$

In our example it would have a form: $H_0 : p = 0.05$

- The Alternative hypothesis is what the researcher(s) \[you\] want to
  prove: $H_a : p > p_0$

In our example it would have a form: $H_a : p > 0.05$

This means a one-sided test.

- The goal here is to provide evidence against $H_0$ (e.g., suggest
  $H_a$).

You want to conclude $H_a$.\
Try a Proof by Contradiction: Assume $H_0$ is true ...and hope your data
contradicts it.

**Step 2. Check the Necessary Assumptions:**

- **Independence Assumption:** There is no reason to think that one
  child having genetic abnormalities would affect the probability that
  other children have them.

- **Randomization Condition:** This sample may not be random, but
  genetic abnormalities are plausibly independent. The sample is
  probably representative of all children, with regards to genetic
  abnormalities.

- **10% Condition:** The sample of 384 children is less than 10% of all
  children.

- **Success/Failure Condition:** $np = (384)(0.05) = 19.2$ and\
  $n(1 - p) = (384)(0.95) = 364.8$ are both greater than 10, so the
  sample is large enough.

**Step 3. Identify the test-statistics. Find the value of the
test-statistic:**

Since the conditions are met, assume $H_0$ is true:\
The sampling distribution of $\hat{p}$ becomes **approximately Normal**.
That is, for large $n$, $\hat{p}$ has approximately the
$$N\left(p_0, \sqrt{\frac{p_0(1 - p_0)}{n}}\right)$$ distribution.

$$z_\ast = \frac{\hat{p} - p_0}{\sqrt{\dfrac{p_0(1 - p_0)}{n}}}
= \frac{0.1198 - 0.05}{\sqrt{\dfrac{(0.05)(0.95)}{384}}}
\approx 6.28$$

Recall that $$\hat{p} = \frac{46}{384} = 0.1198.$$

The value of $z^\ast$ is approximately 6.28, meaning that the observed
proportion of children with genetic abnormalities is over 6 standard
deviations above the hypothesized proportion ($p_0 = 0.05$).

**Step 4.** Find the p-value of the test-statistic.\
P-value = $P(Z > 6.28) \approx 0.000$ [(better to report
$p\text{-value} < 0.0001$)]{.nodecor}\
*Note:* We find the area above $Z = 6.28$ since $H_a : p > 0.05$.\
**Meaning of this p-value:**\
If 5% of children have genetic abnormalities, the chance of observing 46
children with genetic abnormalities in a random sample of 384 children
is almost 0.

**Step 5.** Give (if any) a conclusion.\
p-value is less than 0.0001, which is less than $\alpha = 0.05$; We
reject $H_0 : p = 0.05$, and conclude $H_a : p > 0.05$. Our result is
statistically significant at $\alpha = 0.05$.\
There is very strong evidence that more than 5% of children have genetic
abnormalities.

**R code (1-sample proportion test)**

::: tcolorbox
    prop.test(x=46, n = 384 ,p=0.05,alternative="greater", correct=FALSE);

    ##
    ## 1-sample proportions test without continuity correction
    ##
    ## data:  46 out of 384, null probability 0.05
    ## X-squared = 39.377, df = 1, p-value = 1.747e-10
    ## alternative hypothesis: true p is greater than 0.05
    ## 95 percent confidence interval:
    ##  0.09516097 1.00000000
    ## sample estimates:
    ##        p 
    ## 0.1197917 
:::
::::

::: nt
**About the P-value of the Test-statistics**

- P-value is a conditional probability.

- It is not the probability that $H_0$ (null hypothesis: current belief)
  is true.

- It is: P(observed statistic value \[or even more extreme\] --- $H_0$).
  Given $H_0$ (the null hypothesis), because $H_0$ gives the parameter
  values that we need to find required probability.

- P-value serves as a measure of the strength of the evidence against
  the null hypothesis (but it should not serve as a hard and fast rule
  for decision).

- If p-value = 0.03 (for example) all we can say is that there is 3%
  chance of observing the statistic value we actually observed (or one
  even more inconsistent with the null value).

- P-value is the chance (the proportion) of getting a, for instance,
  $\hat{p}$ as far as or further from $H_0$ than the value observed.

- P-value is the probability of getting at least something (e.g., sample
  proportion $\hat{p}$) more extreme (e.g., unusual, unlikely, or rare)
  than what we have already found (our observed value of $\hat{p}$) that
  provide even stronger evidence against $H_0$.

- The more extreme the z-score (large in absolute values) are the ones
  that denote farther departure of the observed value (e.g., our
  $\hat{p}$) from the parameter value ($p_0$) in $H_0$.

- In the one-sided test, e.g., $H_a : p > p_0$, p-value is one-tailed
  probability. This is the probability that sample proportion $\hat{p}$
  falls at least as far from $p_0$ in one direction as the observed
  value of $\hat{p}$.

- In the two-sided test, e.g., $H_a : p \ne p_0$, p-value is two-tailed
  probability. This is the probability that sample proportion $\hat{p}$
  falls at least as far from $p_0$ in either direction as the observed
  value of $\hat{p}$.
:::

The probability, computed assuming that $H_0$ is true, that the test
statistic would take a value as extreme or more extreme than that
actually observed is called the **P-value** of the test. The smaller the
P-value, the stronger the evidence against $H_0$ provided by the data.

Small P-values are evidence against $H_0$, because they say that the
observed result is unlikely to occur when $H_0$ is true. Large P-values
fail to give evidence against $H_0$.

**The P-value Scale**

- If P-value $<$ 0.001, we have very strong evidence against $H_0$.

- If 0.001 $\leq$ P-value $<$ 0.01, we have strong evidence against
  $H_0$.

- If 0.01 $\leq$ P-value $<$ 0.05, we have evidence against $H_0$.

- If 0.05 $\leq$ P-value $<$ 0.075, we have some evidence against $H_0$.

- If 0.075 $\leq$ P-value $<$ 0.10, we have slight evidence against
  $H_0$.

**Use p-value Method to Make a Decision (Reject or Fail to Reject
$H_0$)**

But how small is small p-value?\
We would need to choose an $\alpha$-level (significance-level): a number
such that if:

- $P\text{–value} \leq \alpha$-level, we reject $H_0$; We can conclude
  $H_a$ (we have evidence to support our claim). Often we phrase as a
  statistically significant result at that specified $\alpha$-level.

- $P\text{–value} > \alpha$-level, we fail to reject $H_0$; We cannot
  conclude $H_a$ (we have not enough evidence to support our claim;
  thus, $H_0$ is plausible - We do not accept $H_0$). Often we phrase as
  the result is not statistically significant at that specified
  $\alpha$-level.

- The default $\alpha$-level (significance-level) is typically
  $\alpha = 0.05$ (but it can be different based on the context of the
  study - it is usually not higher than 0.10).

The p-value in the previous example was extremely small (less than
0.0001). That is a strong evidence to suggest that more than 5% of
children have genetic abnormalities. However, it does not say that the
percentage of sampled children with genetic abnormalities was "a lot
more than 5%". That is, the p-value by itself says nothing about how
much greater the percentage might be. The confidence interval provides
that information.\
To assess the difference in practical terms, we should also construct a
confidence interval:

$$0.1198 \pm (1.96 \times 0.0166)$$ $$0.1198 \pm 0.0324$$
$$(0.0874,\ 0.1522)$$

Interpretation: We are 95% Confident that the true percentage of
children with genetic abnormalities is between 8.74% and 15.22%.\
95% CI for $p$: (9.1%, 15.6%) -- We are 95% confident that the true
percentage of all children that have genetic abnormalities is between
approximately 9.1% and 15.6%. Since both values of this CI are more than
the hypothesized value of $p = 0.05$ (5%), we can further infer that
this true percentage is more than 5%.\
**Do environmental chemicals cause congenital abnormalities?**

We do not know that environmental chemicals cause genetic abnormalities.
We merely have evidence that suggests that a greater percentage of
children are diagnosed with genetic abnormalities now, compared to the
1980s.

::: nt
**More About P-values**

- Big p-values just mean that what we have observed is not surprising.
  It means that the results are in line with our assumption that the
  null hypothesis models the world, so we have no reason to reject it.

- A big p-value does not prove that the null hypothesis is true.

- When we see a big p-value, all we can say is: we cannot reject $H_0$
  (we fail to reject $H_0$) -- we cannot conclude $H_a$ (We have no
  evidence to support $H_a$).
:::

### Some Additional Examples {#some-additional-examples .unnumbered}

::: example
Consider the following hypothesis test:

$$\begin{aligned}
H_0 &: p = 0.75 \\
H_a &: p < 0.75
\end{aligned}$$

A sample of 300 items was selected. Compute the p-value and state your
conclusion for each of the following sample results. Use
$\alpha = 0.05$.

- $\hat{p} = 0.68$

- $\hat{p} = 0.72$

- $\hat{p} = 0.70$

- $\hat{p} = 0.77$

**Solution a.**

$$z_* = \frac{\hat{p} - p_0}{\sqrt{p_0(1 - p_0)/n}} = \frac{0.68 - 0.75}{\sqrt{0.75(1 - 0.75)/300}} = -2.80$$

Using Normal table, P-value $= P(Z < z_*) = P(Z < -2.80) = 0.0026$\
P-value $< \alpha = 0.05$, reject $H_0$.

**Solution b.**

$$z_* = \frac{\hat{p} - p_0}{\sqrt{p_0(1 - p_0)/n}} = \frac{0.72 - 0.75}{\sqrt{0.75(1 - 0.75)/300}} = -1.20$$

Using Normal table, P-value $= P(Z < z_*) = P(Z < -1.20) = 0.1151$\
P-value $> \alpha = 0.05$, do not reject $H_0$.

**Solution c.**

$$z_* = \frac{\hat{p} - p_0}{\sqrt{p_0(1 - p_0)/n}} = \frac{0.70 - 0.75}{\sqrt{0.75(1 - 0.75)/300}} = -2.00$$

Using Normal table, P-value $= P(Z < z_*) = P(Z < -2.00) = 0.0228$\
P-value $< \alpha = 0.05$, reject $H_0$.
:::

::: example
Consider the following hypothesis test:

$$\begin{aligned}
H_0 &: p = 0.20 \\
H_a &: p \ne 0.20
\end{aligned}$$

A sample of 400 provided a sample proportion $\hat{p} = 0.175$.

- Compute the value of the test statistic.

- What is the p-value?

- At the $\alpha = 0.05$, what is your conclusion?

- What is the rejection rule using the critical value? What is your
  conclusion?

**Solution**

- $$z_* = \frac{\hat{p} - p_0}{\sqrt{p_0(1 - p_0)/n}} = \frac{0.175 - 0.20}{\sqrt{(0.20)(0.80)/400}} = -1.25$$

- Using Normal table, P-value =
  $$2P(Z > |z_*|) = 2P(Z > |-1.25|) = 2P(Z > 1.25) = 2(0.1056) = 0.2112$$

- P-value $> \alpha = 0.05$, we CAN'T reject $H_0$.
:::

:::: example
A study found that, in 2005, 12.5% of U.S. workers belonged to unions.
Suppose a sample of 400 U.S. workers is collected in 2006 to determine
whether union efforts to organize have increased union membership.

- Formulate the hypotheses that can be used to determine whether union
  membership increased in 2006.

- If the sample results show that 52 of the workers belonged to unions,
  what is the p-value for your hypothesis test?

- At $\alpha = 0.05$, what is your conclusion?

**Solution**

- $$\begin{aligned}
  H_0 &: p = 0.125 \\
  H_a &: p > 0.125
  \end{aligned}$$

- $$\hat{p} = \frac{52}{400} = 0.13$$
  $$z_* = \frac{\hat{p} - p_0}{\sqrt{p_0(1 - p_0)/n}} = \frac{0.13 - 0.125}{\sqrt{(0.125)(0.875)/400}} = 0.30$$
  Using Normal table, P-value =
  $$P(Z > z_*) = P(Z > 0.30) = 1 - 0.6179 = 0.3821$$

- P-value $> 0.05$, do not reject $H_0$. We cannot conclude that there
  has been an increase in union membership.

**R code**

::: tcolorbox
    prop.test(52, 400, p=0.125, alternative="greater", correct=FALSE);

    ##
    ## 1-sample proportions test without continuity correction
    ##
    ## data:  52 out of 400, null probability 0.125
    ## X-squared = 0.091429, df = 1, p-value = 0.3812
    ## alternative hypothesis: true p is greater than 0.125
    ## 95 percent confidence interval:
    ##  0.1048085 1.0000000
    ## sample estimates:
    ##        p 
    ##    0.13 
:::
::::

## Test of Hypothesis for One Variance

In many practical situations, we are interested in testing whether the
variability in a population (i.e., its variance) has changed. This is
especially important in quality control, finance, and experimental
science. When we have data from a single normal population and want to
test a claim about the population variance, we use the chi-squared
($\chi^2$) test for one variance. This method assumes that the
underlying population is normally distributed and the sample
observations are independent.

**Hypothesis Tests for One Variance**

- Data from a single normal population; independent observations

- Variance unknown

- Large or small sample

**Hypothesis Test**\
$$\begin{aligned}
H_0 &: \sigma^2 = \sigma_0^2 \\
H_a &: \sigma^2 \ne \sigma_0^2 \quad \text{(or } \sigma^2 > \sigma_0^2 \text{ or } \sigma^2 < \sigma_0^2\text{)}
\end{aligned}$$ Assume $H_0$ is true, then:

$$\text{Test statistic:} \quad \chi^2_* = \frac{(n - 1)s^2}{\sigma_0^2} \sim \chi^2_{n - 1}$$

::: tcolorbox
**Decision rules:**

$H_a : \sigma^2 \ne \sigma_0^2$.\
Reject $H_0$ if $\chi^2_* > \chi^2_{n-1;\alpha/2}$ or if
$\chi^2_* < \chi^2_{n-1;1-\alpha/2}$.

$H_a : \sigma^2 > \sigma_0^2$.\
Reject $H_0$ if $\chi^2_* > \chi^2_{n-1;\alpha}$ or if
$P[\chi^2_{n-1} > \chi^2_*]$ is too small.

$H_a : \sigma^2 < \sigma_0^2$.\
Reject $H_0$ if $\chi^2_* < \chi^2_{n-1;1-\alpha}$ or if
$P[\chi^2_{n-1} < \chi^2_*]$ is too small.

**Note.** This is **NOT** robust to departures from Normality.
:::

:::: example
A company produces metal pipes of a standard length, and claims that the
standard deviation of the length is at most 1.2 cm. One of its clients
decides to test this claim by taking a sample of 25 pipes and checking
their lengths. They found that the standard deviation of the sample is
1.5 cm. Does this undermine the company's claim? Use $\alpha = 0.05$.\
*Note: Assume length is Normally distributed.*

**Solution**

$$\begin{aligned}
H_0 &: \sigma^2 \leq 1.2^2 \\
H_a &: \sigma^2 > 1.2^2
\end{aligned}$$

$$\chi^2_* = \frac{(n-1)s^2}{\sigma^2} = \frac{(25-1) \cdot 1.5^2}{1.2^2} = 37.5$$

$$\text{P-value} = P[\chi^2_{24} > 37.5] \approx 0.0389$$

**R Code**

::: tcolorbox
    1 - pchisq(37.5, df = 24);
    ## [1] 0.0389818
:::

**Conclusion**\
We reject $H_0 : \sigma^2 \leq 1.2^2$. We have evidence to indicate that
the variance of the length of metal pipes is more than $1.2^2$.
::::

::: tcolorbox
**Assumptions:** $Y_1, Y_2, \ldots, Y_n$ constitute a random sample from
a Normal distribution with $E(Y_i) = \mu$ and $V(Y_i) = \sigma^2$.

**Hypotheses:** $$\begin{aligned}
H_0 &: \sigma^2 = \sigma_0^2 \\
H_a &: \left\{
  \begin{array}{ll}
    \sigma^2 > \sigma_0^2 & \text{(upper-tailed alternative)} \\
    \sigma^2 < \sigma_0^2 & \text{(lower-tailed alternative)} \\
    \sigma^2 \ne \sigma_0^2 & \text{(two-tailed alternative)}
  \end{array}
\right.
\end{aligned}$$

  ----------------------- ------------------------------------------------------------------------------------------------
  **Test statistic:**     $\chi^2 = \dfrac{(n-1)S^2}{\sigma_0^2}$
  **Rejection Region:**   $\left\{
                          \begin{array}{ll}
                          \chi^2 > \chi^2_\alpha & \text{upper-tailed RR} \\
                          \chi^2 < \chi^2_{1 - \alpha} & \text{lower-tailed RR} \\
                          \chi^2 > \chi^2_{\alpha/2} \ \text{or} \ \chi^2 < \chi^2_{1 - \alpha/2} & \text{two-tailed RR}
                          \end{array}
                          \right.$
  ----------------------- ------------------------------------------------------------------------------------------------
:::

::: example
A manufacturer of car batteries claims that the life of his batteries is
approximately Normally distributed with a standard deviation equal to
0.9 year. If a random sample of 10 of these batteries has a standard
deviation of 1.2 years, do you think that $\sigma > 0.9$ year? Use a
0.05 level of significance.

**Step 1. State hypotheses.** $$\begin{aligned}
H_0 &: \sigma^2 = 0.81 \\
H_a &: \sigma^2 > 0.81
\end{aligned}$$

**Step 2. Compute test statistic.**\
$S^2 = 1.44$, $n = 10$, and $$\chi^2 = \frac{(9)(1.44)}{0.81} = 16$$

**Step 3. Find Rejection Region.**\
From the chi-squared table, the null hypothesis is rejected when
$\chi^2 > 16.919$, where $\nu = 9$ degrees of freedom.

![Right-tailed chi-squared distribution with critical value at
16.919.](section11/images/chisq_right_tail.pdf){width="70%"}

**Step 4. Conclusion.**\
The $\chi^2$ statistic is not significant at the 0.05 level. We conclude
that there is insufficient evidence to claim that $\sigma > 0.9$ year.
:::
